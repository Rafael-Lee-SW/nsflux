# core/RAG.py
"""
RAG(Retrieval-Augmented Generation) ì‹œìŠ¤í…œì˜ ë©”ì¸ ëª¨ë“ˆ

ì´ ëª¨ë“ˆì€ ë‹¤ìŒ í•µì‹¬ ê¸°ëŠ¥ë“¤ì„ ì œê³µí•©ë‹ˆë‹¤:
1. ì§ˆë¬¸ ë¶„ë¥˜ ë° êµ¬ì²´í™” (query_sort, specific_question)
2. RAG ì‹¤í–‰ ë° ì¡°ì • (execute_rag)
3. ì‘ë‹µ ìƒì„± (generate_answer, generate_answer_stream)

ê¸°íƒ€ ì„¸ë¶€ ê¸°ëŠ¥ë“¤ì€ í•˜ìœ„ ëª¨ë“ˆë¡œ ë¶„ë¦¬ë˜ì—ˆìŠµë‹ˆë‹¤:
- retrieval: ë¬¸ì„œ ê²€ìƒ‰ ê´€ë ¨ ê¸°ëŠ¥
- generation: í…ìŠ¤íŠ¸ ìƒì„± ê´€ë ¨ ê¸°ëŠ¥
- image_processing: ì´ë¯¸ì§€ ì²˜ë¦¬ ê´€ë ¨ ê¸°ëŠ¥
- query_processing: ì¿¼ë¦¬ ì²˜ë¦¬ ê´€ë ¨ ê¸°ëŠ¥
- sql_processing: SQL ì¿¼ë¦¬ ê´€ë ¨ ê¸°ëŠ¥
"""

import asyncio
import logging
import re
import uuid
from typing import Dict, List, Tuple, Any, Optional, Union, Generator
import httpx
from config import config
import textwrap
# ë‚´ë¶€ ëª¨ë“ˆ ì„í¬íŠ¸
from core.retrieval import retrieve, expand_time_range_if_needed # Ax100 APIë¡œ ëŒ€ì²´ë¨
from core.generation import generate, collect_vllm_text, collect_vllm_text_stream
from core.sql_processing import generate_sql

# SQL ê´€ë ¨ í•¨ìˆ˜ ì„í¬íŠ¸
from core.SQL_NS import run_sql_unno, run_sql_bl, get_metadata

# í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ë¶ˆëŸ¬ì˜¤ê¸°
from prompt import (
    QUERY_SORT_PROMPT,
    GENERATE_PROMPT_TEMPLATE,
    STREAM_PROMPT_TEMPLATE,
    IMAGE_DESCRIPTION_PROMPT,
    NON_RAG_PROMPT_TEMPLATE,
    IMAGE_PROMPT_TEMPLATE,
    TABLE_PROMPT_TEMPLATE,
)

# ìœ í‹¸ë¦¬í‹° ì„í¬íŠ¸
from utils.tracking import time_tracker

# vLLM ê´€ë ¨ ì„í¬íŠ¸
from vllm import SamplingParams
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.model_executor.models.interfaces import SupportsMultiModal

# ë¡œê¹… ì„¤ì •
logger = logging.getLogger("RAG")

# ê¸€ë¡œë²Œ êµ¬ë¶„ì
SECTION_SEPARATOR = "-" * 100

# RAG ì„œë¹„ìŠ¤ API í˜¸ì¶œ ì„¤ì •
RAG_API_TIMEOUT = 300.0  # íƒ€ì„ì•„ì›ƒ ì‹œê°„ (ì´ˆ)
# RAG_API_MAX_RETRIES = 3  # ìµœëŒ€ ì¬ì‹œë„ íšŸìˆ˜
# RAG_API_RETRY_DELAY = 1.0  # ì¬ì‹œë„ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)


@time_tracker
async def execute_rag(
    query: str, keywords: str, needs_table: str, time_range: str, **kwargs
) -> Tuple[str, List[Dict]]:
    """
    RAG(Retrieval-Augmented Generation) ì‹¤í–‰ì˜ ë©”ì¸ ì§„ì…ì 

    Args:
        query: êµ¬ì²´í™”ëœ ì‚¬ìš©ì ì§ˆë¬¸
        keywords: ê²€ìƒ‰ì— ì‚¬ìš©í•  í‚¤ì›Œë“œ
        needs_table: í…Œì´ë¸” ë°ì´í„° í•„ìš” ì—¬ë¶€ ("yes" ë˜ëŠ” "no")
        time_range: ê²€ìƒ‰ ì‹œê°„ ë²”ìœ„ ("all" ë˜ëŠ” "ì‹œì‘ì¼:ì¢…ë£Œì¼" í˜•ì‹)
        **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„° (model, tokenizer, embed_model, embed_tokenizer, data, config)

    Returns:
        Tuple[str, List[Dict]]: (ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©, ë¬¸ì„œ ë©”íƒ€ë°ì´í„° ë¦¬ìŠ¤íŠ¸)
    """
    logger.info("execute_rag ì§„ì…: query='%s', needs_table=%s", query, needs_table)

    # í•„ìˆ˜ íŒŒë¼ë¯¸í„° ì¶”ì¶œ
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    config = kwargs.get("config")

    # í…Œì´ë¸” í•„ìš” ì—¬ë¶€ í™•ì¸
    if needs_table == "yes":
        logger.info("í…Œì´ë¸” ë°ì´í„° í•„ìš”: SQL ìƒì„± ì‹œì‘")
        try:
            # SQL ìƒì„± ë° ì‹¤í–‰
            result = await generate_sql(query, model, tokenizer, config)

            # SQL ì‹¤í–‰ ê²°ê³¼ê°€ ì—†ëŠ” ê²½ìš°
            if result is None:
                logger.warning("SQL ì‹¤í–‰ ê²°ê³¼ ì—†ìŒ")
                docs = "í…Œì´ë¸” ì¡°íšŒ ê²°ê³¼ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ì¡°íšŒí•  ë°ì´í„°ê°€ ì—†ê±°ë‚˜ SQL ì˜¤ë¥˜ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤."
                docs_list = []
                return docs, docs_list

            # SQL ì‹¤í–‰ ê²°ê³¼ ì²˜ë¦¬
            final_sql_query, title, explain, table_json, chart_json, detailed_result = (
                result
            )

            # LLM ì…ë ¥ìš© í”„ë¡¬í”„íŠ¸ êµ¬ì„±
            prompt = (
                f"ì‹¤ì œ ì‚¬ìš©ëœ SQLë¬¸: {final_sql_query}\n\n"
                f"ì¶”ê°€ ì„¤ëª…: {explain}\n\n"
                f"ì‹¤ì œ SQL ì¶”ì¶œëœ ë°ì´í„°: {str(table_json)}\n\n"
                f"ì‹¤ì œ ì„ ì ëœ B/L ë°ì´í„°: {str(detailed_result)}\n\n"
            )

            # ê²°ê³¼ ë©”íƒ€ë°ì´í„° êµ¬ì„± - RAG ì„œë¹„ìŠ¤ API í˜•ì‹ì— ë§ê²Œ ìˆ˜ì •
            docs_list = [
                {
                    "file_name": "SQL_Result",
                    "title": title,
                    "data": table_json,                 # â† ë°”ë¡œ data ë¡œ!
                    "chunk_id": 0,
                },
                {
                    "file_name": "B/L_Detail",
                    "title": "DG B/L ìƒì„¸ ì •ë³´",
                    "data": detailed_result,            # â† ë°”ë¡œ data ë¡œ!
                    "chunk_id": 1,
                },
            ]

            logger.info("í…Œì´ë¸” ë°ì´í„° ì²˜ë¦¬ ì™„ë£Œ")
            return prompt, docs_list

        except Exception as e:
            logger.error("SQL ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", str(e), exc_info=True)
            docs = f"í…Œì´ë¸” ì¡°íšŒ ì‹œë„ ì¤‘ ì˜ˆì™¸ê°€ ë°œìƒí–ˆìŠµë‹ˆë‹¤. í•´ë‹¹ SQLì„ ì‹¤í–‰í•  ìˆ˜ ì—†ì–´ì„œ í…Œì´ë¸” ë°ì´í„°ë¥¼ ê°€ì ¸ì˜¤ì§€ ëª»í–ˆìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}"
            docs_list = []
            return docs, docs_list
    else:
        logger.info("í‘œì¤€ ê²€ìƒ‰ ì‹¤í–‰: í‚¤ì›Œë“œ='%s', ì‹œê°„ ë²”ìœ„='%s'", keywords, time_range)

        try:
            async with httpx.AsyncClient(timeout=RAG_API_TIMEOUT) as client:
                logger.info("RAG ì„œë¹„ìŠ¤ API ë‹¨ì¼ í˜¸ì¶œ")
                response = await client.post(
                    f"{config.rag_service_url}/api/retrieve",
                    json={
                        "query": keywords,
                        "top_n": config.N,
                        "time_bound": time_range,
                        "min_docs": 50,
                    },
                )

                if response.status_code != 200:
                    logger.error(
                        "RAG ì„œë¹„ìŠ¤ API ì˜¤ë¥˜: %s - %s",
                        response.status_code,
                        response.text,
                    )
                    return "ê²€ìƒ‰ ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.", []
                
                # Here is the place of new function that delete the not related docs things from docs list
                
                result = response.json()
                docs = result.get("documents", "")
                docs_list = result.get("documents_list", [])

                logger.info("RAG ì„œë¹„ìŠ¤ API ì‘ë‹µ: %dê°œ ë¬¸ì„œ ê²€ìƒ‰ë¨", len(docs_list))
                
                # logger.info(f"[RAG_SERVICE_API_RESPONSE - docs_list]\n{docs_list}")
                # logger.info(f"[RAG_SERVICE_API_RESPONSE - docs]\n{docs}")

                # ------------------------------------------------------------------
                # ğŸ”¥ NEW ğŸ”¥  docs_filter ë¡œ í•„í„°ë§
                # ------------------------------------------------------------------
                docs_list = await docs_filter(query, docs_list, model, tokenizer, config)
                logger.info("docs_sort ì´í›„: %dê°œ ë¬¸ì„œë¡œ ì¶•ì†Œ", len(docs_list))
                

                # docs ë¬¸ìì—´ ì¬ì¡°í•© (LLM ì…ë ¥ìš©)
                # (êµì²´)
                docs = "\n\n".join(
                    f"[{doc.get('title', 'ì œëª©ì—†ìŒ')}] {_get_doc_text(doc)}"
                    for doc in docs_list
                )
                
                return docs, docs_list

        except httpx.ReadTimeout as e:
            logger.error("RAG ì„œë¹„ìŠ¤ API íƒ€ì„ì•„ì›ƒ: %s", e)
            return "ê²€ìƒ‰ ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. (íƒ€ì„ì•„ì›ƒ)", []
        except Exception as e:
            logger.error("RAG ì„œë¹„ìŠ¤ API í˜¸ì¶œ ì¤‘ ì˜¤ë¥˜: %s", e, exc_info=True)
            return f"ê²€ìƒ‰ ì„œë¹„ìŠ¤ì— ì—°ê²°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. ì˜¤ë¥˜: {str(e)}", []

@time_tracker
async def generate_answer(query: str, docs: str, **kwargs) -> str:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë¥¼ ê¸°ë°˜ìœ¼ë¡œ ìµœì¢… ë‹µë³€ ìƒì„±

    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
        **kwargs: ì¶”ê°€ íŒŒë¼ë¯¸í„° (model, tokenizer, config)

    Returns:
        str: ìƒì„±ëœ ë‹µë³€
    """
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    config = kwargs.get("config")

    answer = await generate(docs, query, model, tokenizer, config)
    return answer


@time_tracker
async def query_sort(params: Dict[str, Any]) -> Tuple[str, str, str, str]:
    """
    ì‚¬ìš©ì ì§ˆë¬¸ì„ ë¶„ì„í•˜ì—¬ êµ¬ì²´í™”í•˜ê³  RAG íŒŒë¼ë¯¸í„°ë¥¼ ì¶”ì¶œ

    Args:
        params: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬ (user_input, model, tokenizer, embed_model, embed_tokenizer, data, config)

    Returns:
        Tuple[str, str, str, str]: (êµ¬ì²´í™”ëœ ì§ˆë¬¸, í‚¤ì›Œë“œ, í…Œì´ë¸” í•„ìš” ì—¬ë¶€, ì‹œê°„ ë²”ìœ„)
    """
    max_attempts = 3
    attempt = 0

    while attempt < max_attempts:
        # í•„ìš” íŒŒë¼ë¯¸í„° ì¶”ì¶œ
        query = params["user_input"]
        model = params["model"]
        tokenizer = params["tokenizer"]
        config = params["config"]
        request_id = params["request_id"]
        
        # í”„ë¡¬í”„íŠ¸ êµ¬ì„±
        prompt = QUERY_SORT_PROMPT.format(user_query=query)
        logger.info("query_sort ì‹œì‘ (ì‹œë„ %d)", attempt + 1)

        try:
            # LLMì—ì„œ ì‘ë‹µ ìƒì„±
            if config.use_vllm:
                sampling_params = SamplingParams(
                    max_tokens=config.model.max_new_tokens,
                    temperature=config.model.temperature,
                    top_k=config.model.top_k,
                    top_p=config.model.top_p,
                    repetition_penalty=config.model.repetition_penalty,
                )
                answer = await collect_vllm_text(prompt, model, sampling_params, request_id)
            else:
                input_ids = tokenizer(
                    prompt, return_tensors="pt", truncation=True, max_length=4024
                ).to("cuda")
                token_count = input_ids["input_ids"].shape[1]
                outputs = model.generate(
                    **input_ids,
                    max_new_tokens=config.model.max_new_tokens,
                    do_sample=config.model.do_sample,
                    temperature=config.model.temperature,
                    top_k=config.model.top_k,
                    top_p=config.model.top_p,
                    repetition_penalty=config.model.repetition_penalty,
                    eos_token_id=tokenizer.eos_token_id,
                    pad_token_id=tokenizer.eos_token_id,
                )
                answer = tokenizer.decode(
                    outputs[0][token_count:], skip_special_tokens=True
                )

            logger.debug("Generated answer: %s", answer)

            # ì‘ë‹µì—ì„œ íƒœê·¸ë¡œ ê°ì‹¸ì§„ ì •ë³´ ì¶”ì¶œ
            query_pattern = r"<query.*?>(.*?)<query.*?>"
            keyword_pattern = r"<keyword.*?>(.*?)<keyword.*?>"
            table_pattern = r"<table.*?>(.*?)<table.*?>"
            time_pattern = r"<time.*?>(.*?)<time.*?>"

            m_query = re.search(query_pattern, answer, re.DOTALL)
            m_keyword = re.search(keyword_pattern, answer, re.DOTALL)
            m_table = re.search(table_pattern, answer, re.DOTALL)
            m_time = re.search(time_pattern, answer, re.DOTALL)

            # ëª¨ë“  í•„ìˆ˜ íƒœê·¸ê°€ ì¡´ì¬í•˜ëŠ” ê²½ìš°
            if m_query and m_keyword and m_table and m_time:
                qu = m_query.group(1).strip()
                ke = m_keyword.group(1).strip()
                ta = m_table.group(1).strip()
                ti = m_time.group(1).strip()

                # 'all' ì‹œê°„ ë²”ìœ„ ì²˜ë¦¬
                if ti == "all":
                    ti = "1900-01-01:2099-01-01"

                logger.info(
                    "ì§ˆë¬¸ êµ¬ì²´í™” ê²°ê³¼: ì§ˆë¬¸='%s', í‚¤ì›Œë“œ='%s', í…Œì´ë¸”='%s', ì‹œê°„='%s'",
                    qu,
                    ke,
                    ta,
                    ti,
                )
                return qu, ke, ta, ti
            else:
                logger.error("í•„ìš”í•œ íƒœê·¸ê°€ ëˆ„ë½ë¨. ì¬ì‹œë„ %d", attempt + 1)
                attempt += 1

        except Exception as e:
            logger.error("ì§ˆë¬¸ êµ¬ì²´í™” ì¤‘ ì˜¤ë¥˜ ë°œìƒ: %s", str(e), exc_info=True)
            attempt += 1

    # ìµœëŒ€ ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•œ ê²½ìš° ê¸°ë³¸ê°’ ë°˜í™˜
    default_qu = params["user_input"]
    default_ke = params["user_input"]
    default_ta = "no"
    default_ti = "1901-01-01:2099-12-01"
    logger.warning(
        "ìµœëŒ€ ì‹œë„ íšŸìˆ˜ë¥¼ ì´ˆê³¼í•˜ì—¬ ê¸°ë³¸ê°’ ë°˜í™˜: ì§ˆë¬¸='%s', í‚¤ì›Œë“œ='%s', í…Œì´ë¸”='%s', ì‹œê°„='%s'",
        default_qu,
        default_ke,
        default_ta,
        default_ti,
    )
    return default_qu, default_ke, default_ta, default_ti

@time_tracker
async def specific_question(params: Dict[str, Any]) -> Tuple[str, str, str, str]:
    """
    ëŒ€í™” ì´ë ¥ì„ ê³ ë ¤í•œ êµ¬ì²´ì ì¸ ì§ˆë¬¸ ìƒì„±
    (query_sortì™€ ìœ ì‚¬í•˜ì§€ë§Œ ëŒ€í™” ì´ë ¥ ì²˜ë¦¬ì— ì°¨ì´ê°€ ìˆì„ ìˆ˜ ìˆìŒ)

    Args:
        params: íŒŒë¼ë¯¸í„° ë”•ì…”ë„ˆë¦¬

    Returns:
        Tuple[str, str, str, str]: (êµ¬ì²´í™”ëœ ì§ˆë¬¸, í‚¤ì›Œë“œ, í…Œì´ë¸” í•„ìš” ì—¬ë¶€, ì‹œê°„ ë²”ìœ„)
    """
    # query_sortì™€ ë™ì¼í•œ ë¡œì§ ì‚¬ìš©
    return await query_sort(params)

@time_tracker
async def docs_filter(
    query: str,
    docs_list: List[Dict],
    model,
    tokenizer,
    config,
    max_chars_per_doc: int = 4196,
) -> List[Dict]:
    """
    ê²€ìƒ‰ëœ ë¬¸ì„œë“¤(docs_list) ì¤‘ì—ì„œ **ì‚¬ìš©ì ì§ˆë¬¸ê³¼ ì§ì ‘ì ìœ¼ë¡œ ê´€ë ¨** ìˆëŠ”
    ë¬¸ì„œë§Œ ì¶”ë ¤ì„œ ë°˜í™˜í•©ë‹ˆë‹¤.

    1) ê° ë¬¸ì„œë¥¼ ê°„ëµíˆ ìš”ì•½(ì œëª©Â·ì¼ë¶€ ë‚´ìš©)í•´ í”„ë¡¬í”„íŠ¸ë¡œ êµ¬ì„±
    2) LLM(collect_vllm_text)ì—ê²Œ 'ê´€ë ¨ ìˆëŠ” chunk_idë§Œ ì½¤ë§ˆë¡œ ë‚˜ì—´'í•˜ë„ë¡ ì§€ì‹œ
    3) ì‘ë‹µì—ì„œ ìˆ«ì(id)ë§Œ íŒŒì‹± â†’ docs_list í•„í„°ë§
    4) LLM í˜¸ì¶œ ì‹¤íŒ¨Â·ë¹ˆ ê²°ê³¼ ì‹œì—ëŠ” ì›ë³¸ docs_list ê·¸ëŒ€ë¡œ ë°˜í™˜
    """
    if not docs_list:
        return docs_list

    # í•„í„°ë§ ì „ ë¬¸ì„œ ì œëª© ë¡œê¹…
    logger.info("í•„í„°ë§ ì „ ë¬¸ì„œ ì œëª©:")
    for doc in docs_list:
        logger.info(f"- {doc.get('title', 'ì œëª©ì—†ìŒ')} (chunk_id: {doc['chunk_id']})")

    # ----------------------------------------------------------------------
    # (1) ë¬¸ì„œ ìš”ì•½ â†’ í”„ë¡¬í”„íŠ¸ìš© ë¬¸ìì—´ ë§Œë“¤ê¸°
    # ----------------------------------------------------------------------
    formatted_docs = []
    for doc in docs_list:
        # contents[0]ì´ dictì¸ ê²½ìš°(ì˜ˆ: {"data": ...})ì™€ ë¬¸ìì—´ì¸ ê²½ìš° ëª¨ë‘ ì²˜ë¦¬
        raw = ""
        try:
            raw = _get_doc_text(doc)
        except Exception:
            pass

        formatted_docs.append(
            f"{doc['chunk_id']}. ì œëª©: {doc.get('title', 'ì œëª©ì—†ìŒ')}\n"
            f"   ë‚´ìš©: {textwrap.shorten(raw, width=max_chars_per_doc, placeholder='â€¦')}"
        )

    prompt = (
        "ë‹¤ìŒì€ ì‚¬ìš©ìì˜ ì§ˆë¬¸ê³¼ ê²€ìƒ‰ëœ ë¬¸ì„œ ëª©ë¡ì…ë‹ˆë‹¤. "
        "ê° ë¬¸ì„œëŠ” chunk_idë¡œ êµ¬ë¶„ë˜ì–´ ìˆìŠµë‹ˆë‹¤.\n\n"
        f"[ì‚¬ìš©ì ì§ˆë¬¸]\n{query}\n\n"
        "[ë¬¸ì„œ ëª©ë¡]\n"
        + "\n".join(formatted_docs)
        + "\n\n"
        "ìœ„ ë¬¸ì„œ ì¤‘ **ì‚¬ìš©ì ì§ˆë¬¸ì— ì§ì ‘ì ìœ¼ë¡œ ë‹µí•˜ëŠ” ë° ë„ì›€ì´ ë˜ëŠ”** ë¬¸ì„œì˜ chunk_idë§Œì„ "
        "ì½¤ë§ˆ(,)ë¡œ êµ¬ë¶„í•˜ì—¬ í•œ ì¤„ë¡œ ì¶œë ¥í•˜ì„¸ìš”.\n"
        "ë°˜ë“œì‹œ ìˆ«ìì™€ ì½¤ë§ˆë§Œ í¬í•¨í•˜ê³  ë‹¤ë¥¸ ì„¤ëª…ì€ ì‘ì„±í•˜ì§€ ë§ˆì„¸ìš”."
    )

    # ----------------------------------------------------------------------
    # (2) LLM í˜¸ì¶œ
    # ----------------------------------------------------------------------
    sampling_params = SamplingParams(
        max_tokens=32,        # id ëª©ë¡ë§Œ ë½‘ìœ¼ë©´ 32 í† í°ì´ë©´ ì¶©ë¶„
        temperature=0.0,      # deterministic
        top_k=1,
        top_p=1.0,
    )

    try:
        answer = await collect_vllm_text(
            prompt, model, sampling_params, str(uuid.uuid4())
        )
        # ------------------------------------------------------------------
        # (3) ìˆ«ì(id)ë§Œ íŒŒì‹± â†’ docs_list í•„í„°ë§
        # ------------------------------------------------------------------
        keep_ids = set(map(int, re.findall(r"\d+", answer)))
        filtered = [doc for doc in docs_list if doc["chunk_id"] in keep_ids]

        # í•„í„°ë§ í›„ ë¬¸ì„œ ì œëª© ë¡œê¹…
        logger.info("í•„í„°ë§ í›„ ë¬¸ì„œ ì œëª©:")
        for doc in filtered:
            logger.info(f"- {doc.get('title', 'ì œëª©ì—†ìŒ')} (chunk_id: {doc['chunk_id']})")

        # LLMì´ ì•„ë¬´ê²ƒë„ ê³ ë¥´ì§€ ì•Šì•˜ìœ¼ë©´ ì›ë³¸ ìœ ì§€
        return filtered or docs_list

    except Exception as e:
        logger.error("docs_sort ì‹¤íŒ¨, ì›ë³¸ ë¦¬ìŠ¤íŠ¸ ì‚¬ìš©: %s", str(e), exc_info=True)
        return docs_list


# core/RAG.py â”€ import ì•„ë˜ ì•„ë¬´ ê³³ (ê¸°ì¡´ _extract_plain_text ì œê±° í›„ â†“ ë¶™ì—¬ë„£ê¸°)
@time_tracker
def _get_doc_text(doc: Dict[str, Any]) -> str:
    """
    ê²€ìƒ‰ ì„œë²„ê°€ ë‚´ë ¤ì¤€ ë‹¨ì¼ ë¬¸ì„œ(dict)ì—ì„œ ì‚¬ëŒì´ ì½ì„ í…ìŠ¤íŠ¸ë¥¼ ì¶”ì¶œí•œë‹¤.
    â‘  doc["text_short"]ê°€ ìˆìœ¼ë©´ â†’ ê·¸ëŒ€ë¡œ ë°˜í™˜
    â‘¡ fallback: doc["contents"] êµ¬ì¡°ë¥¼ í‰íƒ„í™”
    â‘¢ ê·¸ë˜ë„ ì—†ìœ¼ë©´ ë¹ˆ ë¬¸ìì—´
    """
    # 1) text_short ìµœìš°ì„ 
    short = doc.get("text_short")
    if short:
        return str(short).strip()

    # 2) ê¸°ì¡´ contents í˜¸í™˜ (dict / list / str ì–´ë–¤ í˜•íƒœë“  OK)
    contents = doc.get("contents")
    if contents is None:
        return ""

    # 2â€‘1) str
    if isinstance(contents, str):
        return contents.strip()

    # 2â€‘2) dict
    if isinstance(contents, dict):
        for key in ("data", "text", "page_content", "content", "body"):
            if key in contents and contents[key]:
                return str(contents[key]).strip()
        return " ".join(str(v).strip() for v in contents.values())

    # 2â€‘3) list (ì¬ê·€)
    if isinstance(contents, list):
        return " ".join(_get_doc_text({"contents": c}) for c in contents).strip()

    # 2â€‘4) ê¸°íƒ€
    return str(contents).strip()


@time_tracker
async def generate_answer_stream(
    query: str, docs: str, model, tokenizer, config, http_query: Dict
) -> Generator[str, None, None]:
    """
    ìŠ¤íŠ¸ë¦¬ë° ë°©ì‹ìœ¼ë¡œ ë‹µë³€ ìƒì„±

    Args:
        query: ì‚¬ìš©ì ì§ˆë¬¸
        docs: ê²€ìƒ‰ëœ ë¬¸ì„œ ë‚´ìš©
        model: ì–¸ì–´ ëª¨ë¸
        tokenizer: í† í¬ë‚˜ì´ì €
        config: ì„¤ì •
        http_query: HTTP ìš”ì²­ ì •ë³´

    Yields:
        str: ìƒì„±ëœ ë¶€ë¶„ í…ìŠ¤íŠ¸
    """
    logger.info(
        "[GENERATE_ANSWER_STREAM] ---------- ìŠ¤íŠ¸ë¦¬ë° ë‹µë³€ ìƒì„± ì‹œì‘ ----------"
    )

    # -------------------------------------------------------------------------
    # 1) ë¶„ê¸° ì²˜ë¦¬ë¥¼ ìœ„í•œ íŒŒë¼ë¯¸í„° ì¤€ë¹„
    # -------------------------------------------------------------------------
    # ì´ë¯¸ì§€ê°€ ìˆëŠ”ì§€ ì—¬ë¶€
    image_data = http_query.get("image_data")
    is_image = bool(image_data)  # image_dataê°€ Noneì´ ì•„ë‹ˆë©´ True

    # RAG ì‚¬ìš© ì—¬ë¶€
    use_rag = http_query.get("use_rag", True)

    # -------------------------------------------------------------------------
    # 2) ë¶„ê¸° ë¡œì§ì— ë”°ë¥¸ í”„ë¡¬í”„íŠ¸ ì„ íƒ
    # -------------------------------------------------------------------------
    if is_image and not use_rag:
        # ì´ë¯¸ì§€ ìˆìŒ + RAG ë¯¸ì‚¬ìš©
        prompt = IMAGE_PROMPT_TEMPLATE.format(query=query)
    elif is_image and use_rag:
        # ì´ë¯¸ì§€ ìˆìŒ + RAG ì‚¬ìš©
        prompt = STREAM_PROMPT_TEMPLATE.format(docs=docs, query=query)
    elif (not is_image) and use_rag:
        # ì´ë¯¸ì§€ ì—†ìŒ + RAG ì‚¬ìš©
        prompt = STREAM_PROMPT_TEMPLATE.format(docs=docs, query=query)
    else:
        # ì´ë¯¸ì§€ ì—†ìŒ + RAG ë¯¸ì‚¬ìš©
        prompt = NON_RAG_PROMPT_TEMPLATE.format(query=query)

    logger.info(f"[PROMPT SELECTION] is_image={is_image}, use_rag={use_rag}")
    logger.debug(f"[PROMPT CONTENT]\n{prompt}")

    # -------------------------------------------------------------------------
    # 3) request_id ì„¤ì • (ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ë“± ìš©ë„)
    # -------------------------------------------------------------------------
    request_id = http_query.get("page_id") or http_query.get("qry_id")
    if not request_id:
        request_id = str(uuid.uuid4())

    # -------------------------------------------------------------------------
    # 4) ì‹¤ì œ ì´ë¯¸ì§€ ì²˜ë¦¬ê°€ í•„ìš”í•œ ê²½ìš°, ì´ë¯¸ì§€ ë¡œë“œ (core.image_processing ì‚¬ìš©)
    # -------------------------------------------------------------------------
    pil_image = None
    if image_data:
        try:
            # ì´ë¯¸ì§€ ì²˜ë¦¬ ë¡œì§ì€ image_processing ëª¨ë“ˆë¡œ ì´ë™
            from core.image_processing import prepare_image

            pil_image = await prepare_image(image_data)
            logger.info(
                "ì´ë¯¸ì§€ ë¡œë“œ ì„±ê³µ: %s", str(pil_image.size if pil_image else None)
            )
        except Exception as e:
            logger.error("ì´ë¯¸ì§€ ë¡œë“œ ì‹¤íŒ¨: %s", str(e))

    # -------------------------------------------------------------------------
    # 5) ìŠ¤íŠ¸ë¦¬ë° í…ìŠ¤íŠ¸ ìƒì„± (vLLM ë˜ëŠ” HF)
    # -------------------------------------------------------------------------
    if config.use_vllm:
        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )

        # ë©€í‹°ëª¨ë‹¬ ìš”ì²­ êµ¬ì„±
        if pil_image:
            # ë©€í‹°ëª¨ë‹¬ ì²˜ë¦¬ë¥¼ ìœ„í•œ ìš”ì²­ êµ¬ì„±
            from core.image_processing import prepare_multimodal_request

            generate_request = await prepare_multimodal_request(
                prompt, pil_image, config.model_id, tokenizer
            )
        else:
            generate_request = prompt

        # ìŠ¤íŠ¸ë¦¬ë° ìƒì„±
        async for partial_chunk in collect_vllm_text_stream(
            generate_request, model, sampling_params, request_id
        ):
            # ì„±ëŠ¥ ëª¨ë‹ˆí„°ë§ ì—…ë°ì´íŠ¸ëŠ” collect_vllm_text_stream ë‚´ë¶€ì—ì„œ ìˆ˜í–‰
            yield partial_chunk
    else:
        # HuggingFace ëª¨ë¸ ì‚¬ìš© (Not use vLLM)
        import torch
        from transformers import TextIteratorStreamer

        input_ids = tokenizer(
            prompt, return_tensors="pt", truncation=True, max_length=4024
        ).to("cuda")
        streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
        generation_kwargs = dict(
            **input_ids,
            streamer=streamer,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )

        import threading

        t = threading.Thread(target=model.generate, kwargs=generation_kwargs)
        t.start()

        for new_token in streamer:
            yield new_token


# ì´ RAG.py íŒŒì¼ì€ ì´ì œ ë‹¤ë¥¸ ëª¨ë“ˆì—ì„œ í•„ìš”í•œ í•¨ìˆ˜ë“¤ë§Œ ë…¸ì¶œí•˜ë©°,
# ì„¸ë¶€ êµ¬í˜„ì€ ê°ê°ì˜ íŠ¹í™”ëœ ëª¨ë“ˆë¡œ ì´ë™ë˜ì—ˆìŠµë‹ˆë‹¤.
