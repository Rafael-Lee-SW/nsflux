# Project Tree of RAG for company

```
├─ Dockerfile
├─ app.py
├─ config.yaml
├─ ray_setup.py
├─ ray_utils.py
├─ requirements.txt
└─ utils.py
```

--- config.yaml

```yaml

# config.yaml
# Server : 2x H100 (80 GB SXM5), 52 CPU cores, 483.2 GB RAM, 6 TB SSD
### Model
model_id : 'google/gemma-3-27b-it'
response_url : "http://202.20.84.16:8083/responseToUI"
# response_url : "http://192.222.54.254:6460/testResponse"

ray:
  actor_count: 1                  # 총 Actor 개수(same as num_replicas)
  num_gpus: 1                     # 각 Actor(Node)가 점유하고 있는 GPU 갯수
  num_cpus: 24                    # 각 Actor(Node)가 점유하고 있는 CPU 갯수 (1 actor 시에 gpu 48개, 2 actor 시에 gpu 24개 할당)
  max_batch_size: 10              # max_concurrency(actor 최대 동시 처리량, default 1000)로 대체해도 됨
  batch_wait_timeout: 0.05        
  max_ongoing_requests: 100        # ray.serve에서 deployment setting으로 동시 요청 처리 갯수를 의미함(Batch랑 다름)

use_vllm: True # vLLM 사용 여부
vllm:
  enable_prefix_caching: True
  scheduler_delay_factor: 0.1
  enable_chunked_prefill: True
  tensor_parallel_size: 1         # vLLM의 GPU 사용 갯수 (!!!! num_gpus 보다 작아야 함 !!!!)
  max_num_seqs: 192               # v1에 따른 상향
  max_num_batched_tokens: 16384   # v1에 따른 상향
  block_size: 128                 # 미적용
  gpu_memory_utilization: 0.99    # v0: 0.95 / v1: 0.99로 상향
  disable_custom_all_reduce: true
  enable_memory_defrag: True      # v1 신규 기능 활성화
  # 추가할 설정
  disable_sliding_window: True  # sliding window 비활성화 - cascade attention과 충돌이 나서 이를 비활성화
  max_model_len: 30000

model:
  quantization_4bit : False # Quantize 4-bit
  quantization_8bit : False # Quantize 8-bit
  max_new_tokens : 2048      # 생성할 최대 토큰 수

  do_sample : True # True 일때만 아래가 적용
  temperature : 1.0          # 텍스트 다양성 조정: 높을수록 창의력 향상 (1.0)
  top_k : 30                 # top-k 샘플링: 상위 k개의 후보 토큰 중 하나를 선택 (50)
  top_p : 1.0                # top-p 샘플링: 누적 확률을 기준으로 후보 토큰을 선택 (1.0 보다 낮을수록 창의력 증가)
  repetition_penalty : 1.0   # 같은 단어를 반복해서 출력하지 않도록 패널티를 부여 (1.0 보다 클수록 페널티 증가)
embed_model_id : 'BM-K/KoSimCSE-roberta-multitask'
# cache_dir : "D:/huggingface" # Windows Local
# cache_dir : "/media/user/7340afbb-e4ce-4a38-8210-c6362e85eae7/RAG/RAG_application/huggingface" # Local
cache_dir : "/workspace/huggingface"  # Docker

### Data
data_path : 'data/0228_DB_.json'     # VectorDB Path - New one (계약서 데이터 포함)
# data_path : 'data/1104_NS_DB_old.json' # VectorDB Path - Old one
metadata_path : 'data/Metadata.json' # Metadata.json Path
sql_data_path : 'data/poc.db'        # SQLite 데이터베이스 Path

### Retrieve
N : 5 # Retrieve top N chunks

### Others
beep : '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------'
seed : 4734                     # Radom Seed
k : 15                        # SQL Max Rows (None=MAX)

```


--- Dockerfile

```dockerfile

# 베이스 이미지 선택
FROM globeai/flux_ns:1.26

# 작업 디렉토리 설정
WORKDIR /workspace

# requirements.txt만 먼저 복사해서 종속성 설치 (캐시 활용)
COPY requirements.txt .

# pip 캐시 사용 안 함으로 설치 (임시 파일 최소화)
RUN pip install --no-cache-dir -r requirements.txt

# gemma3.py 패치 파일 복사 (vLLM gemma3 파일 덮어쓰기)
COPY patches/vllm/gemma3.py /opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/gemma3.py

# Solve the C compier
RUN apt-get update && apt-get install build-essential -y

# 현재 디렉토리의 모든 파일을 컨테이너의 /app 폴더로 복사
COPY . /workspace

# Flask 앱이 실행될 포트를 열어둠
EXPOSE 5000

# Ray Dashboard 포트 (8265)와 vLLM 관련 포트 필요 시 추가
EXPOSE 8265
# Expose port for the vLLM
EXPOSE 8000

# Flask 앱 실행 명령어
CMD ["python", "app.py"]

```


--- requirements.txt

```txt

accelerate==1.3.0
aiohappyeyeballs==2.4.4
aiohttp==3.11.12
aiohttp-cors==0.7.0
aiosignal==1.3.2
annotated-types==0.7.0
attrs==25.1.0
bitsandbytes==0.45.1
blinker==1.9.0
cachetools==5.5.1
certifi==2025.1.31
charset-normalizer==3.4.1
click==8.1.8
colorama==0.4.6
colorful==0.5.6
distlib==0.3.9
filelock==3.17.0
Flask==3.1.0
Flask[async]==3.1.0
fastapi[standard]==0.112.0
Jinja2==3.1.5
uvicorn==0.34.0
gunicorn==23.0.0
frozenlist==1.5.0
fsspec==2025.2.0
google-api-core==2.24.1
google-auth==2.38.0
googleapis-common-protos==1.66.0
grpcio==1.70.0
huggingface-hub==0.28.1
idna==3.10
itsdangerous==2.2.0
Jinja2==3.1.5
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
langchain==0.3.19
MarkupSafe==3.0.2
mpmath==1.3.0
msgpack==1.1.0
multidict==6.1.0
networkx==3.4.2
numpy==1.26.4
opencensus==0.11.4
opencensus-context==0.1.3
optree==0.14.0
packaging==24.2
pillow==11.1.0
platformdirs==4.3.6
prometheus_client==0.21.1
propcache==0.2.1
proto-plus==1.26.0
protobuf==5.29.3
psutil==6.1.1
py-spy==0.4.0
pyasn1==0.6.1
pyasn1_modules==0.4.1
pydantic==2.10.6
pydantic_core==2.27.2
python-box==7.3.2
python-dotenv==1.0.1
PyYAML==6.0.2
rank-bm25==0.2.2
ray[serve]==2.40.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
rpds-py==0.22.3
rsa==4.9
safetensors==0.5.2
setuptools==75.8.0
six==1.17.0
smart-open==7.1.0
sympy==1.13.1
the==0.1.5
tokenizers==0.21.0
torch==2.5.1
torchvision==0.20.1
tqdm==4.67.1
transformers==4.49.0
typing_extensions==4.12.2
urllib3==2.3.0
virtualenv==20.29.1
vllm==0.7.3
Werkzeug==3.1.3
wrapt==1.17.2
yarl==1.18.3
python-pptx==1.0.2
pandas==2.2.3
plotly==6.0.0
pypdf2==3.0.1
umap-learn==0.5.3

```


--- app.py

```python

# app.py
import os
# Setting environment variable
# os.environ["TRANSFORMERS_CACHE"] = "/workspace/huggingface"
os.environ["HF_HOME"] = "/workspace/huggingface"
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# For the Huggingface Token setting
os.environ["HF_TOKEN_PATH"] = "/root/.cache/huggingface/token"
# Change to GNU to using OpenMP. Because this is more friendly with CUDA(NVIDIA),
# and Some library(Pytorch, Numpy, vLLM etc) use the OpenMP so that set the GNU is better.
# OpenMP: Open-Multi-Processing API
os.environ["MKL_THREADING_LAYER"] = "GNU"
# Increase download timeout (in seconds)
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "60"
# Use the vLLM as v1 version
os.environ["VLLM_USE_V1"] = "1"
os.environ["VLLM_STANDBY_MEM"] = "0"
os.environ["VLLM_METRICS_LEVEL"] = "1"
os.environ["VLLM_PROFILE_MEMORY"]= "1"
# GPU 단독 사용(박상제 연구원님이랑 분기점 - 연구원님 0번 GPU, 수완 1번 GPU)
os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # GPU1 사용
# 토크나이저 병렬 처리 명시적 비활성화
os.environ["TOKENIZERS_PARALLELISM"] = "false"

from flask import (
    Flask,
    request,
    Response,
    render_template,
    jsonify,
    g,
    stream_with_context,
)
import json
import yaml
from box import Box
from utils import random_seed, error_format, send_data_to_server, process_format_to_response
from datetime import datetime

# Import the Ray modules
from ray_setup import init_ray
from ray import serve
from ray_utils import InferenceActor
from ray_utils import InferenceService, SSEQueueManager

# ------ checking process of the thread level
import logging
import threading

# 로깅 설정: 요청 처리 시간과 현재 스레드 이름을 기록
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s %(levelname)s [%(threadName)s] %(message)s'
)

import ray
import uuid
import asyncio
import time

# Configuration
with open("./config.yaml", "r") as f:
    config_yaml = yaml.load(f, Loader=yaml.FullLoader)
    config = Box(config_yaml)
random_seed(config.seed)

########## Ray Dashboard 8265 port ##########
init_ray()  # Initialize the Ray
sse_manager = SSEQueueManager.options(name="SSEQueueManager").remote()
serve.start(detached=True)

#### Ray-Actor 다중 ####
inference_service = InferenceService.options(num_replicas=config.ray.actor_count).bind(config)
serve.run(inference_service)
inference_handle = serve.get_deployment_handle("inference", app_name="default")

#### Ray-Actor 단독 ####
# inference_actor = InferenceActor.options(num_cpus=config.ray.num_cpus, num_gpus=config.ray.num_gpus).remote(config)

########## FLASK APP setting ##########
app = Flask(__name__)
content_type = "application/json; charset=utf-8"

# 기본 페이지를 불러오는 라우트
@app.route("/")
def index():
    return render_template("index.html")  # index.html을 렌더링

# Test 페이지를 불러오는 라우트
@app.route("/test")
def test_page():
    return render_template("index_test.html")

# chatroomPage 페이지를 불러오는 라우트
@app.route("/chat")
def chat_page():
    return render_template("chatroom.html")

# data 관리
from data_control import data_control_bp
app.register_blueprint(data_control_bp, url_prefix="/data")

# Query Endpoint (Non-streaming)
@app.route("/query", methods=["POST"])
async def query():
    try:
        # Log when the query is received
        receive_time = datetime.now().isoformat()
        print(f"[APP] Received /query request at {receive_time}")
        
        # Optionally, attach the client time if desired:
        http_query = request.json  # 클라이언트로부터 JSON 요청 수신
        
        http_query["server_receive_time"] = receive_time
        
        # Ray Serve 배포된 서비스를 통해 추론 요청 (자동으로 로드밸런싱됨)
        # result = await inference_actor.process_query.remote(http_query) # 단일
        result = await inference_handle.query.remote(http_query) # 다중
        if isinstance(result, dict):
            result = json.dumps(result, ensure_ascii=False)
        print("APP.py - 결과: ", result)
        return Response(result, content_type=content_type)
    except Exception as e:
        error_resp = error_format(f"서버 처리 중 오류 발생: {str(e)}", 500)
        return Response(error_resp, content_type=content_type)

# --------------------- Streaming part ----------------------------

# # Streaming Endpoint (POST 방식 SSE) → 동기식 뷰 함수로 변경
# @app.route("/query_stream", methods=["POST"])
# def query_stream():
#     """
#     POST 방식 SSE 스트리밍 엔드포인트.
#     클라이언트가 {"input": "..."} 형태의 JSON을 보내면, SSE 스타일의 청크를 반환합니다.
#     """
#     body = request.json or {}
#     user_input = body.get("input", "")
#     # request_id 파트 추가
#     client_request_id = body.get("request_id")
#     print(f"[DEBUG] /query_stream (POST) called with user_input='{user_input}', request_id='{client_request_id}'")
    
#     http_query = {"qry_contents": user_input}
#     # request_id 파트 추가
#     if client_request_id:
#         http_query["request_id"] = client_request_id
#     print(f"[DEBUG] Built http_query={http_query}")

#     response = inference_handle.process_query_stream.remote(http_query)
#     obj_ref = response._to_object_ref_sync()
#     request_id = ray.get(obj_ref)
    
#     print(f"[DEBUG] streaming request_id={request_id}")
    
#     def sse_generator():
#         try:
#             while True:
#                 # Retrieve token from SSEQueueManager
#                 token = ray.get(sse_manager.get_token.remote(request_id, 120))
#                 if token is None or token == "[[STREAM_DONE]]":
#                     break
#                 yield f"data: {token}\n\n"
#         except Exception as e:
#             error_token = json.dumps({"type": "error", "message": str(e)})
#             yield f"data: {error_token}\n\n"
#         finally:
#             # Cleanup: close the SSE queue after streaming is done
#             try:
#                 obj_ref = inference_handle.close_sse_queue.remote(request_id)._to_object_ref_sync()
#                 ray.get(obj_ref)
#             except Exception as ex:
#                 print(f"[DEBUG] Error closing SSE queue for {request_id}: {str(ex)}")
#             print("[DEBUG] SSE closed.")

#     return Response(sse_generator(), mimetype="text/event-stream")

# --------------------- Streaming part TEST for API format matching ----------------------------

@app.route("/query_stream", methods=["POST"])
def query_stream():
    """
    POST 방식 SSE 스트리밍 엔드포인트.
    클라이언트가 아래 필드들을 포함한 JSON을 보내면:
      - qry_id, user_id, page_id, auth_class, qry_contents, qry_time
    auth_class는 내부적으로 'admin'으로 통일합니다.
    """
    body = request.json or {}
    # 새로운 필드 추출
    qry_id = body.get("qry_id")
    user_id = body.get("user_id")
    page_id = body.get("page_id")
    auth_class = "admin"  # 어떤 값이 와도 'admin'으로 통일
    qry_contents = body.get("qry_contents", "")
    qry_time = body.get("qry_time")  # 클라이언트 측 타임스탬프

    print(f"[DEBUG] /query_stream called with qry_id='{qry_id}', user_id='{user_id}', page_id='{page_id}', qry_contents='{qry_contents}', qry_time='{qry_time}'")
    
    # 새로운 http_query 생성 – 내부 로직에서는 page_id를 채팅방 id로 사용
    http_query = {
        "qry_id": qry_id,
        "user_id": user_id,
        "page_id": page_id if page_id else str(uuid.uuid4()),
        "auth_class": auth_class,
        "qry_contents": qry_contents,
        "qry_time": qry_time
    }
    
    # 기존 request_id 대신 page_id를 SSE queue key로 사용
    print(f"[DEBUG] Built http_query: {http_query}")
    
    # Ray Serve를 통한 streaming 호출 (변경 없음, 내부 인자는 수정된 http_query)
    response = inference_handle.process_query_stream.remote(http_query)
    obj_ref = response._to_object_ref_sync()
    chat_id = ray.get(obj_ref)  # chat_id는 page_id
    print(f"[DEBUG] streaming chat_id={chat_id}")
    
    def sse_generator():
        try:
            while True:
                # SSEQueueManager에서 토큰을 가져옴 (chat_id 사용)
                token = ray.get(sse_manager.get_token.remote(chat_id, 120))
                if token is None or token == "[[STREAM_DONE]]":
                    break
                yield f"data: {token}\n\n"
        except Exception as e:
            error_token = json.dumps({"type": "error", "message": str(e)})
            yield f"data: {error_token}\n\n"
        finally:
            try:
                obj_ref = inference_handle.close_sse_queue.remote(chat_id)._to_object_ref_sync()
                ray.get(obj_ref)
            except Exception as ex:
                print(f"[DEBUG] Error closing SSE queue for {chat_id}: {str(ex)}")
            print("[DEBUG] SSE closed.")

    return Response(sse_generator(), mimetype="text/event-stream")

# --------------------- CLT Streaming part ----------------------------

# --------------------- CLT Streaming part ----------------------------
@app.route("/queryToSLLM", methods=["POST"])
def query_stream_to_clt():
    """
    POST 방식 SSE 스트리밍 엔드포인트.
    클라이언트가 {"qry_id": "...", "user_id": "...", "page_id": "...", "qry_contents": "...", "qry_time": "..." }
    형태의 JSON을 보내면, 내부 Ray Serve SSE 스트림을 통해 처리한 후 지정된 response_url로 SSE 청크를 전송합니다.
    """
    # POST 요청 파라미터 파싱
    body = request.json or {}
    qry_id = body.get("qry_id", "")
    user_id = body.get("user_id", "")
    page_id = body.get("page_id", "")
    auth_class = "admin"  # 모든 요청을 'admin'으로 처리
    user_input = body.get("qry_contents", "")
    qry_time = body.get("qry_time", "")
    
    response_url = config.response_url

    print(f"[DEBUG] /queryToSLLM called with qry_id='{qry_id}', user_id='{user_id}', "
          f"page_id='{page_id}', qry_contents='{user_input}', qry_time='{qry_time}', url={response_url}")
    
    # 내부 로직에서는 page_id를 채팅방 ID(또는 request_id)로 사용합니다.
    http_query = {
        "qry_id": qry_id,
        "user_id": user_id,
        "page_id": page_id if page_id else str(uuid.uuid4()),
        "auth_class": auth_class,
        "qry_contents": user_input,
        "qry_time": qry_time,
        "response_url": response_url
    }
    print(f"[DEBUG] Built http_query={http_query}")

    # Ray Serve에 SSE 스트리밍 요청 보내기
    response = inference_handle.process_query_stream.remote(http_query)
    obj_ref = response._to_object_ref_sync()
    request_id = ray.get(obj_ref)
    print(f"[DEBUG] streaming request_id={request_id}")
    
    def sse_generator(request_id, response_url):
        token_buffer = []  # To collect tokens (for answer tokens only)
        last_sent_time = time.time()  # To track the last time data was sent
        answer_counter = 1  # 답변 업데이트 순번
        try:
            while True:
                token = ray.get(sse_manager.get_token.remote(request_id, 120))
                if token is None:
                    print("[DEBUG] 토큰이 None 반환됨. 종료합니다.")
                    break

                if isinstance(token, str):
                    token = token.strip()
                if token == "[[STREAM_DONE]]":
                    print("[DEBUG] 종료 토큰([[STREAM_DONE]]) 수신됨. 스트림 종료.")
                    break

                try:
                    token_dict = json.loads(token) if isinstance(token, str) else token
                except Exception as e:
                    print(f"[ERROR] JSON 파싱 실패: {e}. 원시 토큰: '{token}'")
                    continue

                # If token is a reference token, send it immediately
                if token_dict.get("type") == "reference":
                    print(f"[DEBUG] Reference token details: {token_dict}")
                    ref_format = process_format_to_response([token_dict], qry_id, continue_="C", update_index=answer_counter)
                    print(f"[DEBUG] Sending reference data: {json.dumps(ref_format, ensure_ascii=False, indent=2)}")
                    send_data_to_server(ref_format, response_url)
                    continue

                # Otherwise, accumulate answer tokens
                token_buffer.append(token_dict)
                current_time = time.time()
                # If 1 second has passed, flush the accumulated answer tokens
                if current_time - last_sent_time >= 1:
                    if len(token_buffer) > 0:
                        # Check if any token in the buffer signals termination.
                        final_continue = "E" if any(t.get("continue") == "E" for t in token_buffer) else "C"
                        print(f"[DEBUG] Flushing {len(token_buffer)} tokens with continue flag: {final_continue}")
                        buffer_format = process_format_to_response(token_buffer, qry_id, continue_=final_continue, update_index=answer_counter)
                        send_data_to_server(buffer_format, response_url)
                        token_buffer = []  # Reset the buffer
                        last_sent_time = current_time  # Update the last sent time
                        answer_counter += 1
                if token_dict.get("continue") == "E":
                    # Immediately flush the buffer with termination flag if needed
                    if len(token_buffer) > 0:
                        print(f"[DEBUG] Immediate flush due to termination flag in buffer (size {len(token_buffer)}).")
                        buffer_format = process_format_to_response(token_buffer, qry_id, continue_="E", update_index=answer_counter)
                        send_data_to_server(buffer_format, response_url)
                        token_buffer = []
                    break
            # After loop: if tokens remain, flush them with termination flag
            if len(token_buffer) > 0:
                print(f"[DEBUG] Final flush of remaining {len(token_buffer)} tokens with end flag.")
                buffer_format = process_format_to_response(token_buffer, qry_id, continue_="E", update_index=answer_counter)
                send_data_to_server(buffer_format, response_url)
        except Exception as e:
            print(f"[ERROR] sse_generator encountered an error: {e}")
        finally:
            try:
                obj_ref = inference_handle.close_sse_queue.remote(request_id)._to_object_ref_sync()
                ray.get(obj_ref)
            except Exception as ex:
                print(f"[DEBUG] Error closing SSE queue for {request_id}: {str(ex)}")
            print("[DEBUG] SSE closed.")

    
    # 별도의 스레드에서 SSE generator 실행
    job = threading.Thread(target=sse_generator, args=(request_id, response_url), daemon=False)
    job.start()

    # 클라이언트에는 즉시 "수신양호" 메시지를 JSON 형식으로 응답
    return Response(error_format("수신양호", 200, qry_id), content_type="application/json")


# @app.route("/queryToSLLM", methods=["POST"])
# def query_stream_to_clt():
#     """
#     POST 방식 SSE 스트리밍 엔드포인트.
#     클라이언트가 {"qry_id": "...", "user_id": "...", "page_id": "...", "qry_contents": "...", "qry_time": "..." }
#     형태의 JSON을 보내면, 내부 Ray Serve SSE 스트림을 통해 처리한 후 지정된 response_url로 SSE 청크를 전송합니다.
#     """
#     # POST 요청 파라미터 파싱
#     body = request.json or {}
#     qry_id = body.get("qry_id", "")
#     user_id = body.get("user_id", "")
#     page_id = body.get("page_id", "")
#     auth_class = "admin"  # 모든 요청을 'admin'으로 처리
#     user_input = body.get("qry_contents", "")
#     qry_time = body.get("qry_time", "")
    
#     response_url = config.response_url

#     print(f"[DEBUG] /query_stream_to_clt called with qry_id='{qry_id}', user_id='{user_id}', "
#           f"page_id='{page_id}', qry_contents='{user_input}', qry_time='{qry_time}', url={response_url}")
    
#     # 내부 로직에서는 page_id를 채팅방 ID(또는 request_id)로 사용합니다.
#     http_query = {
#         "qry_id": qry_id,
#         "user_id": user_id,
#         "page_id": page_id if page_id else str(uuid.uuid4()),
#         "auth_class": auth_class,
#         "qry_contents": user_input,
#         "qry_time": qry_time,
#         "response_url": response_url
#     }
#     print(f"[DEBUG] Built http_query={http_query}")

#     # Ray Serve에 SSE 스트리밍 요청 보내기
#     response = inference_handle.process_query_stream.remote(http_query)
#     obj_ref = response._to_object_ref_sync()
#     request_id = ray.get(obj_ref)
#     print(f"[DEBUG] streaming request_id={request_id}")
    
#     def sse_generator(request_id, response_url):
#         token_buffer = []  # To collect tokens
#         last_sent_time = time.time()  # To track the last time data was sent
#         answer_counter = 1  # 답변 업데이트 순번
#         try:
#             while True:
#                 token = ray.get(sse_manager.get_token.remote(request_id, 120))
#                 if token is None:
#                     print("[DEBUG] 토큰이 None 반환됨. 종료합니다.")
#                     break

#                 if isinstance(token, str):
#                     token = token.strip()
#                 if token == "[[STREAM_DONE]]":
#                     print("[DEBUG] 종료 토큰([[STREAM_DONE]]) 수신됨. 스트림 종료.")
#                     break

#                 try:
#                     token_dict = json.loads(token) if isinstance(token, str) else token
#                 except Exception as e:
#                     print(f"[ERROR] JSON 파싱 실패: {e}. 원시 토큰: '{token}'")
#                     continue

#                 token_buffer.append(token_dict)
#                 current_time = time.time()
#                 # If 1 second has passed, send the accumulated tokens
#                 if current_time - last_sent_time >= 1:
#                     if len(token_buffer) > 0:
#                         # Check if any token in the buffer signals termination.
#                         final_continue = "E" if any(t.get("continue") == "E" for t in token_buffer) else "C"
#                         buffer_format = process_format_to_response(token_buffer, qry_id, continue_=final_continue, update_index=answer_counter)
#                         send_data_to_server(buffer_format, response_url)
#                         token_buffer = []  # Reset the buffer
#                         last_sent_time = current_time  # Update the last sent time
#                         answer_counter += 1
#                 if token_dict.get("continue") == "E":
#                     # Immediately send the buffer with end flag if not already sent
#                     if len(token_buffer) > 0:
#                         buffer_format = process_format_to_response(token_buffer, qry_id, continue_="E", update_index=answer_counter)
#                         send_data_to_server(buffer_format, response_url)
#                         token_buffer = []
#                     break
#             # After loop: if tokens remain, send them with continue_="E"
#             if len(token_buffer) > 0:
#                 buffer_format = process_format_to_response(token_buffer, qry_id, continue_="E", update_index=answer_counter)
#                 send_data_to_server(buffer_format, response_url)
#         except Exception as e:
#             print(f"[ERROR] sse_generator encountered an error: {e}")
#         finally:
#             try:
#                 obj_ref = inference_handle.close_sse_queue.remote(request_id)._to_object_ref_sync()
#                 ray.get(obj_ref)
#             except Exception as ex:
#                 print(f"[DEBUG] Error closing SSE queue for {request_id}: {str(ex)}")
#             print("[DEBUG] SSE closed.")
    
#     # 별도의 스레드에서 SSE generator 실행
#     job = threading.Thread(target=sse_generator, args=(request_id, response_url), daemon=False)
#     job.start()

#     # 클라이언트에는 즉시 "수신양호" 메시지를 JSON 형식으로 응답
#     return Response(error_format("수신양호", 200, qry_id), content_type="application/json")

# @app.route("/queryToSLLM", methods=["POST"])
# def query_stream_to_clt():
#     """
#     POST 방식 SSE 스트리밍 엔드포인트.
#     클라이언트가 {"input": "..."} 형태의 JSON을 보내면, SSE 스타일의 청크를 반환합니다.
#     """
#     # POST 요청 params
#     body = request.json or {}
    
#     # CLT 통신과 맞는 규격
#     qry_id = body.get("qry_id", "")
#     user_id = body.get("user_id", "")
#     page_id = body.get("page_id", "")
#     auth_class = "admin"  # 어떤 값이 와도 'admin'으로 통일
#     user_input = body.get("qry_contents", "")
#     qry_time = body.get("qry_time", "")
    
#     response_url = config.response_url

#     print(f"[DEBUG] /query_stream_to_clt called with qry_id='{qry_id}', user_id='{user_id}', page_id='{page_id}', qry_contents='{user_input}', qry_time='{qry_time}', url={response_url}")
#     # 내부 로직에서는 page_id를 채팅방 ID(또는 request_id)로 사용합니다.
#     http_query = {
#         "qry_id": qry_id,
#         "user_id": user_id,
#         "page_id": page_id if page_id else str(uuid.uuid4()),
#         "auth_class": auth_class,
#         "qry_contents": user_input,
#         "qry_time": qry_time,
#         "response_url": response_url
#     }
#     print(f"[DEBUG] Built http_query={http_query}")

#     # Obtain request_id from Ray
#     response = inference_handle.process_query_stream.remote(http_query)
#     obj_ref = response._to_object_ref_sync()
#     request_id = ray.get(obj_ref)

#     print(f"[DEBUG] streaming request_id={request_id}")
    
#     def sse_generator(request_id, response_url):
#         try:
#             token_buffer = []  # To collect tokens
#             last_sent_time = time.time()  # To track the last time data was sent

#             answer_counter = 1  # 답변 업데이트 순번 (답변1, 답변2, ...)
            
#             while True:
#                 # Retrieve token from SSEQueueManager
#                 token = ray.get(sse_manager.get_token.remote(request_id, 120))
#                 token_dict = json.loads(token) if isinstance(token, str) else token
                
#                 token_buffer.append(token_dict)  # Collect token
                
#                 current_time = time.time()

#                 # If 1 second has passed, send the accumulated tokens
#                 if current_time - last_sent_time >= 1:
#                     # Send the accumulated tokens
#                     buffer_format = process_format_to_response(token_buffer, qry_id, update_index=answer_counter)
#                     send_data_to_server(buffer_format, response_url)
#                     token_buffer = []  # Reset the buffer
#                     last_sent_time = current_time  # Update the last sent time
#                     answer_counter += 1  # 순번 증가
                
#                 # If "continue" is "E", send the accumulated tokens with END signal
#                 elif token_dict.get("continue") == "E":
#                     # Send the accumulated tokens --- EXCEPT LAST END TOKEN
#                     buffer_format = process_format_to_response(token_buffer[:-1], qry_id, continue_="E", update_index=answer_counter)
#                     send_data_to_server(buffer_format, response_url)
#                     token_buffer = []  # Reset the buffer
#                     last_sent_time = current_time  # Update the last sent time

#                 # If the "continue" key indicates to stop, break the loop
#                 if token_dict.get("continue") == "E":
#                     break

#         except Exception as e:
#             # error_token = json.dumps({"type": "error", "message": str(e)})
#             # yield f"data: {error_token}\n\n"
#             print(e)

#         finally:
#             # Cleanup: close the SSE queue after streaming is done
#             try:
#                 obj_ref = inference_handle.close_sse_queue.remote(request_id)._to_object_ref_sync()
#                 ray.get(obj_ref)
#             except Exception as ex:
#                 print(f"[DEBUG] Error closing SSE queue for {request_id}: {str(ex)}")
#             print("[DEBUG] SSE closed.")
    
#     job = threading.Thread(target=sse_generator, args=(request_id, response_url), daemon=False)
#     job.start()

#     return Response(error_format("수신양호", 200, qry_id), content_type="application/json")

# ------------------------------------------------

# 새로 추가1: request_id로 대화 기록을 조회하는 API 엔드포인트
@app.route("/history", methods=["GET"])
def conversation_history():
    request_id = request.args.get("request_id", "")
    last_index = request.args.get("last_index")
    if not request_id:
        error_resp = error_format("request_id 파라미터가 필요합니다.", 400)
        return Response(error_resp, content_type="application/json; charset=utf-8")
    
    try:
        last_index = int(last_index) if last_index is not None else None
        response = inference_handle.get_history.remote(request_id, last_index=last_index)
        # DeploymentResponse를 ObjectRef로 변환
        obj_ref = response._to_object_ref_sync()
        history_data = ray.get(obj_ref)
        return jsonify(history_data)
    except Exception as e:
        print(f"[ERROR /history] {e}")
        error_resp = error_format(f"대화 기록 조회 오류: {str(e)}", 500)
        return Response(error_resp, content_type="application/json; charset=utf-8")


# 새로 추가2: request_id로 해당 답변의 참고자료를 볼 수 있는 API
@app.route("/reference", methods=["GET"])
def get_reference():
    request_id = request.args.get("request_id", "")
    msg_index_str = request.args.get("msg_index", "")
    if not request_id or not msg_index_str:
        error_resp = error_format("request_id와 msg_index 파라미터가 필요합니다.", 400)
        return Response(error_resp, content_type="application/json; charset=utf-8")
    
    try:
        msg_index = int(msg_index_str)
        # 먼저 history를 가져옴
        response = inference_handle.get_history.remote(request_id)
        obj_ref = response._to_object_ref_sync()
        history_data = ray.get(obj_ref)
        
        history_list = history_data.get("history", [])
        if msg_index < 0 or msg_index >= len(history_list):
            return jsonify({"error": "유효하지 않은 메시지 인덱스"}), 400
        
        message = history_list[msg_index]
        if message.get("role") != "ai":
            return jsonify({"error": "해당 메시지는 AI 응답이 아닙니다."}), 400
        
        chunk_ids = message.get("references", [])
        if not chunk_ids:
            return jsonify({"references": []})
        
        # chunk_ids에 해당하는 실제 참조 데이터 조회
        ref_response = inference_handle.get_reference_data.remote(chunk_ids)
        ref_obj_ref = ref_response._to_object_ref_sync()
        references = ray.get(ref_obj_ref)
        return jsonify({"references": references})
    except Exception as e:
        print(f"[ERROR /reference] {e}")
        error_resp = error_format(f"참조 조회 오류: {str(e)}", 500)
        return Response(error_resp, content_type="application/json; charset=utf-8")


# Flask app 실행
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=False)

```


--- ray_setup.py

```python

# ray_setup.py
import ray
from ray import serve

########## Starting Banner ############
from colorama import init, Fore, Style
init(autoreset=True)

BANNER = Fore.GREEN + r"""
'########:'##:::::::'##::::'##:'##::::'##::::::::::'##::: ##::'######::
 ##.....:: ##::::::: ##:::: ##:. ##::'##::::::::::: ###:: ##:'##... ##:
 ##::::::: ##::::::: ##:::: ##::. ##'##:::::::::::: ####: ##: ##:::..::
 ######::: ##::::::: ##:::: ##:::. ###::::::::::::: ## ## ##:. ######::
 ##...:::: ##::::::: ##:::: ##::: ## ##:::::::::::: ##. ####::..... ##:
 ##::::::: ##::::::: ##:::: ##:: ##:. ##::::::::::: ##:. ###:'##::: ##:
 ##::::::: ########:. #######:: ##:::. ##:'#######: ##::. ##:. ######::
..::::::::........:::.......:::..:::::..::.......::..::::..:::......:::
"""

def init_ray():
    print(BANNER)
    # Ray-Dashboard - GPU 상태, 사용 통계 등을 제공하는 모니터링 툴, host 0.0.0.0로 외부 접속을 허용하고, Default 포트인 8265으로 설정
    ray.init(
        include_dashboard=True,
        dashboard_host="0.0.0.0" # External IP accessable
        # dashboard_port=8265
    )
    print("Ray initialized. DashBoard running at http://192.222.54.254:8265") # New Server(2xH100)

```


--- ray_utils.py

```python

# ray_utils.py
import ray  # Ray library
from ray import serve
import json
import asyncio  # async I/O process module
from concurrent.futures import ProcessPoolExecutor  # 스레드 컨트롤
import uuid  # --- NEW OR MODIFIED ---
import time
from typing import Dict, Optional  # --- NEW OR MODIFIED ---
import threading  # To find out the usage of thread
import datetime

from RAG import (
    query_sort,
    execute_rag,
    generate_answer,
    generate_answer_stream,
)  # hypothetically
from utils import (
    load_model,
    load_data,
    process_format_to_response,
    process_to_format,
    error_format,
)
# from summarizer import summarize_conversation
from summarizer import summarize_conversation
from debug_tracking import log_batch_info, log_system_info

# 랭체인 도입
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage

# =============================================================================
# Custom Conversation Memory to store extra metadata (e.g., chunk_ids)
# =============================================================================
class CustomConversationBufferMemory(ConversationBufferMemory):
    """대화 저장 시 추가 메타데이터를 함께 기록"""
    def save_context(self, inputs: dict, outputs: dict) -> None:
        """
        inputs, outputs 예시:
            inputs = {
                "qry_contents": "사용자 질문",
                "qry_id": "...",
                "user_id": "...",
                "auth_class": "...",
                "qry_time": "..."
            }
            outputs = {
                "output": "AI의 최종 답변",
                "chunk_ids": [...참조 chunk_id 리스트...]
            }
        """
        try:
            user_content = inputs.get("qry_contents", "")
            human_msg = HumanMessage(
                content=user_content,
                additional_kwargs={
                    "qry_id": inputs.get("qry_id"),
                    "user_id": inputs.get("user_id"),
                    "auth_class": inputs.get("auth_class"),
                    "qry_time": inputs.get("qry_time")
                }
            )
            ai_content = outputs.get("output", "")
            ai_msg = AIMessage(
                content=ai_content,
                additional_kwargs={
                    "chunk_ids": outputs.get("chunk_ids", []),
                    "qry_id": inputs.get("qry_id"),
                    "user_id": inputs.get("user_id"),
                    "auth_class": inputs.get("auth_class"),
                    "qry_time": inputs.get("qry_time")
                }
            )

            self.chat_memory.messages.append(human_msg)
            self.chat_memory.messages.append(ai_msg)
        except Exception as e:
            print(f"[ERROR in save_context] {e}")
        
    def load_memory_variables(self, inputs: dict) -> dict:
        """
        랭체인 규약에 의해 {"history": [메시지 리스트]} 형태 리턴
        """
        try:
            return {"history": self.chat_memory.messages}
        except Exception as e:
            print(f"[ERROR in load_memory_variables] {e}")
            return {"history": []}

# Serialization function for messages
def serialize_message(msg):
    """
    HumanMessage -> {"role": "human", "content": ...}
    AIMessage    -> {"role": "ai", "content": ..., "references": [...]}
    """
    try:
        if isinstance(msg, HumanMessage):
            return {"role": "human", "content": msg.content}
        elif isinstance(msg, AIMessage):
            refs = msg.additional_kwargs.get("chunk_ids", [])
            # 디버그 출력
            print(f"[DEBUG serialize_message] AI refs: {refs}")
            return {"role": "ai", "content": msg.content, "references": refs}
        else:
            return {
                "role": "unknown",
                "content": getattr(msg, "content", str(msg))
            }
    except Exception as e:
        print(f"[ERROR in serialize_message] {e}")
        return {"role": "error", "content": str(e)}
    
# =============================================================================
# =============================================================================

@ray.remote  # From Decorator, Each Actor is allocated 1 GPU
class InferenceActor:
    async def __init__(self, config):
        self.config = config
        # 액터 내부에서 모델 및 토크나이저를 새로 로드 (GPU에 한 번만 로드)
        self.model, self.tokenizer, self.embed_model, self.embed_tokenizer = load_model(
            config
        )
        # 데이터는 캐시 파일을 통해 로드
        self.data = load_data(config.data_path)
        # 비동기 큐와 배치 처리 설정 (마이크로 배칭)
        self.request_queue = asyncio.Queue()
        self.max_batch_size = config.ray.max_batch_size  # 최대 배치 수
        self.batch_wait_timeout = config.ray.batch_wait_timeout  # 배치당 처리 시간

        # Actor 내부에서 ProcessPoolExecutor 생성 (직렬화 문제 회피)
        max_workers = int(min(config.ray.num_cpus * 0.8, (26*config.ray.actor_count)-4))
        self.process_pool = ProcessPoolExecutor(max_workers)

        # --- SSE Queue Manager ---
        # A dictionary to store SSE queues for streaming requests
        # Key = request_id, Value = an asyncio.Queue of partial token strings
        self.queue_manager = ray.get_actor("SSEQueueManager")
        self.active_sse_queues: Dict[str, asyncio.Queue] = {}

        self.batch_counter = 0  # New counter to track batches

        
        self.memory_map = {}

        # Micro-batching로 바꾸기(아래 주석 해체)
        # asyncio.create_task(self._batch_processor())
        
        # In-flight batching까지 추가 적용(Micro 사용할 경우 주석)
        asyncio.create_task(self._in_flight_batch_processor())


    def get_memory_for_session(self, request_id: str) -> CustomConversationBufferMemory:
        """
        세션별 Memory를 안전하게 가져오는 헬퍼 메서드.
        만약 memory_map에 request_id가 없으면 새로 생성해 저장 후 반환.
        """
        if request_id not in self.memory_map:
            print(f"[DEBUG] Creating new CustomConversationBufferMemory for session={request_id}")
            self.memory_map[request_id] = CustomConversationBufferMemory(return_messages=True)
        return self.memory_map[request_id]

    # -------------------------------------------------------------------------
    # Micro_batch_processor - OLD METHOD
    # -------------------------------------------------------------------------
    async def _batch_processor(self):
        """
        Continuously processes queued requests in batches (micro-batching).
        We add new logic for streaming partial tokens if a request has an SSE queue.
        """
        while True:
            batch = []
            batch_start_time = time.time()
            # 1) get first request from the queue
            print("=== _batch_processor waiting for request_queue item... ===")
            item = await self.request_queue.get()
            print(
                f"[DEBUG] 첫 요청 도착: {time.strftime('%H:%M:%S')} (현재 배치 크기: 1)"
            )
            batch.append(item)

            print(f"[DEBUG] Received first request at {time.strftime('%H:%M:%S')}")

            # 2) try to fill the batch up to batch_size or until timeout
            try:
                while len(batch) < self.max_batch_size:
                    print("현재 배치 사이즈 : ", len(batch))
                    print("최대 배치 사이즈 : ", self.max_batch_size)
                    item = await asyncio.wait_for(
                        self.request_queue.get(), timeout=self.batch_wait_timeout
                    )

                    batch.append(item)
                    print(
                        f"[DEBUG] 추가 요청 도착: {time.strftime('%H:%M:%S')} (현재 배치 크기: {len(batch)})"
                    )
            except asyncio.TimeoutError:
                elapsed = time.time() - batch_start_time
                print(
                    f"[DEBUG] 타임아웃 도달: {elapsed:.2f}초 후 (최종 배치 크기: {len(batch)})"
                )
                pass

            print(
                f"=== _batch_processor: 배치 사이즈 {len(batch)} 처리 시작 ({time.strftime('%H:%M:%S')}) ==="
            )

            # 각 요청 처리 전후에 로그 추가
            start_proc = time.time()
            await asyncio.gather(
                *(
                    self._process_single_query(req, fut, sse_queue)
                    for (req, fut, sse_queue) in batch
                )
            )
            proc_time = time.time() - start_proc
            print(f"[DEBUG] 해당 배치 처리 완료 (처리시간: {proc_time:.2f}초)")
            
    # -------------------------------------------------------------------------
    # In-flight BATCH PROCESSOR
    # -------------------------------------------------------------------------
    async def _in_flight_batch_processor(self):
        while True:
            # Wait for the first item (blocking until at least one is available)
            print(
                "=== [In-Flight Batching] Waiting for first item in request_queue... ==="
            )
            first_item = await self.request_queue.get()
            batch = [first_item]
            batch_start_time = time.time()

            print(
                "[In-Flight Batching] Got the first request. Attempting to fill a batch..."
            )

            # Attempt to fill up the batch until we hit max_batch_size or batch_wait_timeout
            while len(batch) < self.max_batch_size:
                try:
                    remain_time = self.batch_wait_timeout - (
                        time.time() - batch_start_time
                    )
                    if remain_time <= 0:
                        print(
                            "[In-Flight Batching] Timed out waiting for more requests; proceeding with current batch."
                        )
                        break
                    item = await asyncio.wait_for(
                        self.request_queue.get(), timeout=remain_time
                    )
                    batch.append(item)
                    print(
                        f"[In-Flight Batching] +1 request => batch size now {len(batch)} <<< {self.max_batch_size}"
                    )
                except asyncio.TimeoutError:
                    print(
                        "[In-Flight Batching] Timeout reached => proceeding with the batch."
                    )
                    break
            self.batch_counter += 1
            
            # 현재 배치 정보 로깅
            log_batch_info(batch)
            log_system_info("배치 처리 전 상태")

            # We have a batch of items: each item is ( http_query_or_stream_dict, future, sse_queue )
            # We'll process them concurrently.
            tasks = []
            for request_tuple in batch:
                request_obj, fut, sse_queue = request_tuple
                tasks.append(self._process_single_query(request_obj, fut, sse_queue))

            # Actually run them all concurrently
            await asyncio.gather(*tasks)
            log_system_info("배치 처리 후 상태")

    async def _process_single_query(self, http_query_or_stream_dict, future, sse_queue):
        """
        Process a single query from the micro-batch. If 'sse_queue' is given,
        we do partial-token streaming. Otherwise, normal final result.
        """
        # 스트리밍 요청인 경우 request_id를 미리 초기화
        request_id = None
        print(
            f"[DEBUG] _process_single_query 시작: {time.strftime('%H:%M:%S')}, 요청 내용: {http_query_or_stream_dict}, 현재 스레드: {threading.current_thread().name}"
        )
        try:
            # 1) 스트리밍 구분
            if (isinstance(http_query_or_stream_dict, dict)
                and "request_id" in http_query_or_stream_dict):
                # 스트리밍
                request_id = http_query_or_stream_dict["request_id"]
                http_query = http_query_or_stream_dict["http_query"]
                is_streaming = True
                print(f"[STREAM] _process_single_query: request_id={request_id}")
            else:
                # Non-스트리밍
                request_id = None
                http_query = http_query_or_stream_dict
                is_streaming = False
                print("[NORMAL] _process_single_query started...")
                
            # 2) Memory 객체 가져오기 (없으면 새로 생성)
            page_id = http_query.get("page_id", request_id)
            memory = self.get_memory_for_session(page_id)

            # 3) 유저가 현재 입력한 쿼리 가져오기
            user_input = http_query.get("qry_contents", "")
            
            # 4) LangChain Memory에서 이전 대화 이력(history) 추출
            past_context = memory.load_memory_variables({}).get("history", [])
            # history가 리스트 형식인 경우 (각 메시지가 별도 항목으로 저장되어 있다면)
            if isinstance(past_context, list):
                recent_messages = [msg if isinstance(msg, str) else msg.content for msg in past_context[-5:]]
                past_context = "\n\n".join(recent_messages)
            else:
                # 문자열인 경우, 메시지 구분자를 "\n\n"으로 가정하여 분리
                messages = str(past_context).split("\n\n")
                recent_messages = messages[-5:]
                past_context = "\n\n".join(recent_messages)
            
            # # 2) 추가: 전체 토큰 수가 4000개를 초과하면 마지막 4000 토큰만 유지
            # past_tokens = self.tokenizer.tokenize(str(past_context))
            # if len(past_tokens) > 4000:
            #     past_tokens = past_tokens[-4000:]
            #     past_context = self.tokenizer.convert_tokens_to_string(past_tokens)
            
            # ★ 토큰 수 계산 코드 추가 ★
            # retrieval 자료는 dict나 리스트일 수 있으므로 문자열로 변환하여 토큰화합니다.
            # 각 입력값을 명시적으로 str()로 변환합니다.
            past_tokens = self.tokenizer.tokenize(str(past_context))
            query_tokens = self.tokenizer.tokenize(str(user_input))
            total_tokens = len(past_tokens) + len(query_tokens)
            print(f"[DEBUG] Token counts - 이전 대화: {len(past_tokens)}, 사용자 입력 질문: {len(query_tokens)}, 총합: {total_tokens}")
            
            # # To Calculate the token
            # tokens = self.tokenizer(user_input, add_special_tokens=True)["input_ids"]
            # print(f"[DEBUG] Processing query: '{user_input}' with {len(tokens)} tokens")

            # 5) 필요하다면 데이터를 다시 로드(1.16version 유지)
            self.data = load_data(
                self.config.data_path
            )  # if you want always-latest, else skip

            # 6) 현재 사용중인 Thread 확인
            print("   ... calling query_sort() ...")
            # print(
            #     f"[DEBUG] query_sort 시작 (offload) - 스레드: {threading.current_thread().name}"
            # )
            # 7) “대화 이력 + 현재 사용자 질문”을 Prompt에 합쳐서 RAG 수행
            #    방법 1) query_sort() 전에 past_context를 참조해 query를 확장
            #    방법 2) generate_answer()에서 Prompt 앞부분에 붙임
            # 여기서는 예시로 “query_sort”에 past_context를 넘겨
            # 호출부 수정
            params = {
                "user_input": f"{past_context}\n사용자 질문: {user_input}",
                "model": self.model,
                "tokenizer": self.tokenizer,
                "embed_model": self.embed_model,
                "embed_tokenizer": self.embed_tokenizer,
                "data": self.data,
                "config": self.config,
            }
            QU, KE, TA, TI = await query_sort(params)
            print(f"   ... query_sort => QU={QU}, KE={KE}, TA={TA}, TI={TI}")

            # 4) RAG
            if TA == "yes":
                try:
                    docs, docs_list = execute_rag(
                        QU,
                        KE,
                        TA,
                        TI,
                        model=self.model,
                        tokenizer=self.tokenizer,
                        embed_model=self.embed_model,
                        embed_tokenizer=self.embed_tokenizer,
                        data=self.data,
                        config=self.config,
                    )
                    try:
                        retrieval, chart = process_to_format(docs_list, type="SQL")
                    except Exception as e:
                        print("[ERROR] process_to_format (SQL) failed:", str(e))
                        retrieval, chart = [], None

                    # If streaming => partial tokens
                    if is_streaming:
                        print(
                            f"[STREAM] Starting partial generation for request_id={request_id}"
                        )
                        await self._stream_partial_answer(
                            QU, docs, retrieval, chart, request_id, future, user_input
                        )
                    else:
                        # normal final result
                        output = await generate_answer(
                            QU,
                            docs,
                            model=self.model,
                            tokenizer=self.tokenizer,
                            config=self.config,
                        )
                        answer = process_to_format([output, chart], type="Answer")
                        final_data = [retrieval, answer]
                        outputs = process_format_to_response(final_data, qry_id=None, continue_="C")
                        
                        # >>> Record used chunk IDs
                        # 변경 후: retrieval 결과에서 추출
                        chunk_ids_used = []
                        print("---------------- chunk_id 찾기 : ", retrieval.get("rsp_data", []))
                        for doc in retrieval.get("rsp_data", []):
                            if "chunk_id" in doc:
                                chunk_ids_used.append(doc["chunk_id"])
                                                        
                        # >>> CHANGED: summarize the conversation
                        # loop = asyncio.get_event_loop()
                        # prev_summary = memory.load_memory_variables({}).get("summary", "")
                        # new_entry = f"User: {user_input}\nAssistant: {output}\nUsed Chunks: {chunk_ids_used}\n"
                        # updated_conversation = prev_summary + "\n" + new_entry
                        # # Summarized CPU 사용
                        # # import concurrent.futures

                        # # Create a dedicated pool with more workers (e.g., 4)
                        # # summary_pool = concurrent.futures.ProcessPoolExecutor(max_workers=4)

                        # # Later, when calling the summarization function:
                        # summarized = loop.run_in_executor(None, summarize_conversation, updated_conversation)
                        # # # After obtaining 'summarized' in _process_single_query:
                        # # if not summarized:
                        # #     print("[ERROR] Summarization returned an empty string.")
                        # # else:
                        # #     print(f"[CHECK] Summarized conversation: {summarized}")
                        # memory.save_context({"input": user_input}, {"output": output, "chunk_ids": chunk_ids_used})
                                            # 메모리에 저장
                        try:
                            memory.save_context(
                                {
                                    "qry_contents": user_input,
                                    "qry_id": http_query.get("qry_id"),
                                    "user_id": http_query.get("user_id"),
                                    "auth_class": http_query.get("auth_class"),
                                    "qry_time": http_query.get("qry_time")
                                },
                                {
                                    "output": output,
                                    "chunk_ids": chunk_ids_used
                                }
                            )
                        except Exception as e:
                            print(f"[ERROR memory.save_context] {e}")
                        # >>> CHANGED -----------------------------------------------------
                        future.set_result(outputs)

                except Exception as e:
                    outputs = error_format("내부 Excel 에 해당 자료가 없습니다.", 551)
                    future.set_result(outputs)

            else:
                try:
                    print("[SOOWAN] TA is No, before make a retrieval")
                    docs, docs_list = execute_rag(
                        QU,
                        KE,
                        TA,
                        TI,
                        model=self.model,
                        tokenizer=self.tokenizer,
                        embed_model=self.embed_model,
                        embed_tokenizer=self.embed_tokenizer,
                        data=self.data,
                        config=self.config,
                    )
                    retrieval = process_to_format(docs_list, type="Retrieval")
                    print("[SOOWAN] TA is No, and make a retrieval is successed")
                    if is_streaming:
                        print(
                            f"[STREAM] Starting partial generation for request_id={request_id}"
                        )
                        await self._stream_partial_answer(
                            QU, docs, retrieval, None, request_id, future, user_input
                        )
                    else:
                        output = await generate_answer(
                            QU,
                            docs,
                            model=self.model,
                            tokenizer=self.tokenizer,
                            config=self.config,
                        )
                        print("process_to_format 이후에 OUTPUT 생성 완료")
                        answer = process_to_format([output], type="Answer")
                        print("process_to_format 이후에 ANSWER까지 생성 완료")
                        final_data = [retrieval, answer]
                        outputs = process_format_to_response(final_data, qry_id=None, continue_="C")
                        
                        # >>> CHANGED: Record used chunk ID
                        chunk_ids_used = []
                        print("---------------- chunk_id 찾기 : ", retrieval.get("rsp_data", []))
                        for doc in retrieval.get("rsp_data", []):
                            if "chunk_id" in doc:
                                chunk_ids_used.append(doc["chunk_id"])
                                
                        
                        # loop = asyncio.get_event_loop()
                        # prev_summary = memory.load_memory_variables({}).get("summary", "")
                        # new_entry = f"User: {user_input}\nAssistant: {output}\nUsed Chunks: {chunk_ids_used}\n"
                        # updated_conversation = prev_summary + "\n" + new_entry
                        # # # Summarized CPU 사용
                        # # import concurrent.futures
                        
                        # # # Create a dedicated pool with more workers (e.g., 4)
                        # # summary_pool = concurrent.futures.ProcessPoolExecutor(max_workers=4)
                        
                        # # Later, when calling the summarization function:
                        # summarized = loop.run_in_executor(None, summarize_conversation, updated_conversation)
                        
                        # memory.save_context({"input": user_input}, {"output": output, "chunk_ids": chunk_ids_used})
                                            # 메모리 저장
                        try:
                            memory.save_context(
                                {
                                    "qry_contents": user_input,
                                    "qry_id": http_query.get("qry_id"),
                                    "user_id": http_query.get("user_id"),
                                    "auth_class": http_query.get("auth_class"),
                                    "qry_time": http_query.get("qry_time")
                                },
                                {
                                    "output": output,
                                    "chunk_ids": chunk_ids_used
                                }
                            )
                        except Exception as e:
                            print(f"[ERROR memory.save_context] {e}")
                        # --------------------------------------------------------------------
                        
                        future.set_result(outputs)

                except Exception as e:
                    # ====== 이 부분에서 SSE를 즉시 닫고 스트리밍 종료 ======
                    err_msg = f"[ERROR] 처리 중 오류 발생: {str(e)}"
                    print(err_msg)

                    # SSE 전송 (error 이벤트)
                    if request_id:
                        try:
                            error_token = json.dumps({"type": "error", "message": err_msg}, ensure_ascii=False)
                            await self.queue_manager.put_token.remote(request_id, error_token)
                            # 스트리밍 종료
                            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
                        except Exception as e2:
                            print(f"[ERROR] SSE 전송 중 추가 예외 발생: {str(e2)}")
                        finally:
                            # SSEQueue 정리
                            await self.close_sse_queue(request_id)

                    # Future 응답도 에러로
                    future.set_result(error_format(str(e), 500))
                    return
                
        except Exception as e:
            err_msg = f"[ERROR] 처리 중 오류 발생: {str(e)}"
            print("[ERROR]", err_msg)
            # SSE 스트리밍인 경우 error 토큰과 종료 토큰 전송
            if request_id:
                try:
                    error_token = json.dumps({"type": "error", "message": err_msg}, ensure_ascii=False)
                    await self.queue_manager.put_token.remote(request_id, error_token)
                except Exception as e2:
                    print(f"[ERROR] SSE 전송 중 추가 예외 발생: {str(e2)}")
            future.set_result(error_format(err_msg, 500))
        finally:
            # 스트리밍 요청인 경우 반드시 SSE 큐에 종료 토큰을 넣고 큐를 정리한다.
            if request_id:
                try:
                    await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
                except Exception as ex:
                    print(f"[DEBUG] Error putting STREAM_DONE: {str(ex)}")
                await self.close_sse_queue(request_id)

    # ------------------------------------------------------------
    # HELPER FOR STREAMING PARTIAL ANSWERS (Modified to send reference)
    # ------------------------------------------------------------
    async def _stream_partial_answer(
        self, QU, docs, retrieval, chart, request_id, future, user_input
    ):
        """
        Instead of returning a final string, we generate partial tokens
        and push them to the SSE queue in real time.
        We'll do a "delta" approach so each chunk is only what's newly added.
        """
        print(
            f"[STREAM] _stream_partial_answer => request_id={request_id}, chart={chart}"
        )

        # 단일
        # queue = self.active_sse_queues.get(request_id)
        # if not queue:
        #     print(f"[STREAM] SSE queue not found => fallback to normal final (request_id={request_id})")
        #     # fallback...
        #     return

        # This will hold the entire text so far. We'll yield only new pieces.
        
        # 먼저, 참조 데이터 전송: type을 "reference"로 명시
        reference_json = json.dumps({
            "type": "reference",
            "status_code": 200,
            "result": "OK",
            "detail": "Reference data",
            "evt_time": datetime.datetime.now().isoformat(),
            "data_list": [retrieval]
        }, ensure_ascii=False)
        # Debug: print the reference JSON before sending
        print(f"[DEBUG] Prepared reference data: {reference_json}")
        await self.queue_manager.put_token.remote(request_id, reference_json)
        
        print(f"[STREAM] Sent reference data for request_id={request_id}")
             
        # 1) 메모리 가져오기 (없으면 생성)
        try:
            memory = self.get_memory_for_session(request_id)
        except Exception as e:
            msg = f"[STREAM] Error retrieving memory for {request_id}: {str(e)}"
            print(msg)
            # 에러 응답을 SSE로 전송하고 종료
            error_token = json.dumps({"type":"error","message":msg}, ensure_ascii=False)
            await self.queue_manager.put_token.remote(request_id, error_token)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            future.set_result(error_format(msg, 500))
            return
        
        # 2) 과거 대화 이력 로드
        try:
            past_context = memory.load_memory_variables({})["history"]
            # history가 리스트 형식인 경우 (각 메시지가 별도 항목으로 저장되어 있다면)
            if isinstance(past_context, list):
                recent_messages = [msg if isinstance(msg, str) else msg.content for msg in past_context[-5:]]
                past_context = "\n\n".join(recent_messages)
            else:
                # 문자열인 경우, 메시지 구분자를 "\n\n"으로 가정하여 분리
                messages = str(past_context).split("\n\n")
                recent_messages = messages[-5:]
                past_context = "\n\n".join(recent_messages)
            
            # # 2) 추가: 전체 토큰 수가 4000개를 초과하면 마지막 4000 토큰만 유지
            # past_tokens = self.tokenizer.tokenize(str(past_context))
            # if len(past_tokens) > 4000:
            #     past_tokens = past_tokens[-4000:]
            #     past_context = self.tokenizer.convert_tokens_to_string(past_tokens)
        except KeyError:
            # 만약 "history" 키가 없으면 빈 문자열로 처리
            print(f"[STREAM] No 'history' in memory for {request_id}, using empty.")
            past_context = ""
        except Exception as e:
            msg = f"[STREAM] load_memory_variables error for {request_id}: {str(e)}"
            print(msg)
            error_token = json.dumps({"type":"error","message":msg}, ensure_ascii=False)
            await self.queue_manager.put_token.remote(request_id, error_token)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            future.set_result(error_format(msg, 500))
            return

        # 3) 최종 프롬프트 구성
        final_query = f"{past_context}\n\n[사용자 질문]\n{QU}"
        print(f"[STREAM] final_query = \n{final_query}")
        
        # ★ 토큰 수 계산 코드 추가 ★
        # retrieval 자료는 dict나 리스트일 수 있으므로 문자열로 변환하여 토큰화합니다.
        retrieval_str = str(retrieval)
        # 각 입력값을 명시적으로 str()로 변환합니다.
        past_tokens = self.tokenizer.tokenize(str(past_context))
        query_tokens = self.tokenizer.tokenize(str(QU))
        retrieval_tokens = self.tokenizer.tokenize(retrieval_str)
        total_tokens = len(self.tokenizer.tokenize(str(final_query))) + len(retrieval_tokens)
        print(f"[DEBUG] Token counts - 이전 대화: {len(past_tokens)}, RAG 검색 자료: {len(retrieval_tokens)}, 사용자 구체화 질문: {len(query_tokens)}, 총합: {total_tokens}")
        
        partial_accumulator = ""

        try:
            print(
                f"[STREAM] SSE: calling generate_answer_stream for request_id={request_id}"
            )
            async for partial_text in generate_answer_stream(
                final_query, docs, self.model, self.tokenizer, self.config
            ):
                # print(f"[STREAM] Received partial_text: {partial_text}")
                new_text = partial_text[len(partial_accumulator) :]
                partial_accumulator = partial_text
                if not new_text.strip():
                    continue
                    # Wrap answer tokens in a JSON object with type "answer"
                answer_json = json.dumps({
                    "type": "answer",
                    "answer": new_text
                }, ensure_ascii=False)
                # Use the central SSEQueueManager to put tokens
                # print(f"[STREAM] Sending token: {answer_json}")
                await self.queue_manager.put_token.remote(request_id, answer_json)
            final_text = partial_accumulator
            # # 이제 memory에 저장 (이미 request_id를 알고 있다고 가정) # 랭체인
            # try:
            #     memory.save_context({"input": user_input}, {"output": final_text})
            # except Exception as e:
            #     msg = f"[STREAM] memory.save_context failed: {str(e)}"
            #     print(msg)
                
                
            # >>> CHANGED: Update conversation summary in streaming branch as well
            chunk_ids_used = []
            print("---------------- chunk_id 찾기 : ", retrieval.get("rsp_data", []))
            for doc in retrieval.get("rsp_data", []):
                if "chunk_id" in doc:
                    chunk_ids_used.append(doc["chunk_id"])
                    
            # memory.save_context({"input": user_input}, {"output": final_text, "chunk_ids": chunk_ids_used})
            
            # 메모리 저장
            try:
                memory.save_context(
                    {
                        "qry_contents": user_input,
                        "qry_id": "",  # 필요한 경우 http_query에 있는 값을 넣음
                    },
                    {
                        "output": final_text,
                        "chunk_ids": chunk_ids_used
                    }
                )
            except Exception as e:
                print(f"[ERROR memory.save_context in stream] {e}")
            
            print("메시지 저장 직후 chunk_id 확인 : ", memory)
            # >>> CHANGED: -------------------------------------------------------
            
            # 최종 응답 구조
            if chart is not None:
                ans = process_to_format([final_text, chart], type="Answer")
                final_res = process_format_to_response(retrieval, ans)
            else:
                ans = process_to_format([final_text], type="Answer")
                final_res = process_format_to_response(retrieval, ans)
                
            # 담아서 보내기
            future.set_result(final_res)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            print(
                f"[STREAM] done => placed [[STREAM_DONE]] for request_id={request_id}"
            )
        except Exception as e:
            msg = f"[STREAM] error in partial streaming => {str(e)}"
            print(msg)
            future.set_result(error_format(msg, 500))
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")

    # --------------------------------------------------------
    # EXISTING METHODS FOR NORMAL QUERIES (unchanged)
    # --------------------------------------------------------
    async def process_query(self, http_query):
        """
        Existing synchronous method. Returns final string/dict once done.
        """
        loop = asyncio.get_event_loop()
        future = loop.create_future()
        # There's no SSE queue for normal queries
        sse_queue = None
        await self.request_queue.put((http_query, future, sse_queue))
        # print("self.request_queue : ", self.request_queue)
        return await future
    # ----------------------
    # 1) Streaming Entrypoint
    # ----------------------
    async def process_query_stream(self, http_query: dict) -> str:
        """
        /query_stream 호출 시 page_id(채팅방 id)를 기반으로 SSE queue 생성하고,
        대화 저장에 활용할 수 있도록 합니다.
        """
        # page_id를 채팅방 id로 사용 (없으면 생성)
        chat_id = http_query.get("page_id")
        if not chat_id:
            chat_id = str(uuid.uuid4())
        http_query["page_id"] = chat_id  # 강제 할당
        await self.queue_manager.create_queue.remote(chat_id)
        print(f"[STREAM] process_query_stream => chat_id={chat_id}, http_query={http_query}")

        loop = asyncio.get_event_loop()
        final_future = loop.create_future()

        sse_queue = asyncio.Queue()
        self.active_sse_queues[chat_id] = sse_queue
        print(f"[STREAM] Created SSE queue for chat_id={chat_id}")

        # 기존과 동일하게 micro-batch queue에 푸시 (http_query에 새 필드들이 포함됨)
        queued_item = {
            "request_id": chat_id,   # 내부적으로 page_id를 request_id처럼 사용
            "http_query": http_query,
        }

        print(f"[STREAM] Putting item into request_queue for chat_id={chat_id}")
        await self.request_queue.put((queued_item, final_future, sse_queue))
        print(f"[STREAM] Done putting item in queue => chat_id={chat_id}")

        return chat_id


    # ----------------------
    # 2) SSE token popping
    # ----------------------
    async def pop_sse_token(self, request_id: str) -> Optional[str]:
        """
        The SSE route calls this repeatedly to get partial tokens.
        If no token is available, we block up to 120s, else return None.
        """
        if request_id not in self.active_sse_queues:
            print(
                f"[STREAM] pop_sse_token => no SSE queue found for request_id={request_id}"
            )
            return None

        queue = self.active_sse_queues[request_id]
        try:
            token = await asyncio.wait_for(queue.get(), timeout=120.0)
            # print(f"[STREAM] pop_sse_token => got token from queue: {token}")
            return token
        except asyncio.TimeoutError:
            print(
                f"[STREAM] pop_sse_token => timed out waiting for token, request_id={request_id}"
            )
            return None

    # ----------------------
    # 3) SSE queue cleanup
    # ----------------------
    async def close_sse_queue(self, request_id: str):
        """
        Called by the SSE route after finishing.
        Remove the queue from memory.
        """
        if request_id in self.active_sse_queues:
            print(
                f"[STREAM] close_sse_queue => removing SSE queue for request_id={request_id}"
            )
            del self.active_sse_queues[request_id]
        else:
            print(f"[STREAM] close_sse_queue => no SSE queue found for {request_id}")
    
    # ----------------------
    # /history | 대화 기록 가져오기
    # ----------------------
    async def get_conversation_history(self, request_id: str) -> dict:
        """
        Returns the conversation history for the given request_id.
        The messages are serialized into a JSON-friendly format.
        """
        try:
            if request_id in self.memory_map:
                memory = self.memory_map[request_id]
                history_obj = memory.load_memory_variables({})
                if "history" in history_obj and isinstance(history_obj["history"], list):
                    # 직렬화
                    serialized = [serialize_message(msg) for msg in history_obj["history"]]
                    print("[HISTORY] 대화 기록 반환(직렬화) : ", serialized)
                    return {"history": serialized}
                else:
                    print("[HISTORY] 대화 기록 반환(직렬화X) : ", history_obj)
                    return {"history": []}
            else:
                return {"history": []}
        except Exception as e:
            print(f"[ERROR get_conversation_history] {e}")
            return {"history": []}
        
    # ----------------------
    # /reference | 해당 답변의 출처 가져오기
    # ----------------------
    async def get_reference_data(self, chunk_ids: list):
        try:
            result = []
            data = self.data
            for cid in chunk_ids:
                if cid in data["chunk_ids"]:
                    idx = data["chunk_ids"].index(cid)
                    record = {
                        "file_name": data["file_names"][idx],
                        "title": data["titles"][idx],
                        "text": data["texts_vis"][idx],
                        "date": str(data["times"][idx])
                    }
                    result.append(record)
            return result
        except Exception as e:
            print(f"[ERROR get_reference_data] {e}")
            return []

# Ray Serve를 통한 배포
@serve.deployment(name="inference", max_ongoing_requests=100)
class InferenceService:
    def __init__(self, config):
        self.config = config
        self.actor = InferenceActor.options(
            num_gpus=config.ray.num_gpus, 
            num_cpus=config.ray.num_cpus
        ).remote(config)

    async def query(self, http_query: dict):
        result = await self.actor.process_query.remote(http_query)
        return result

    async def process_query_stream(self, http_query: dict) -> str:
        req_id = await self.actor.process_query_stream.remote(http_query)
        return req_id

    async def pop_sse_token(self, req_id: str) -> str:
        token = await self.actor.pop_sse_token.remote(req_id)
        return token

    async def close_sse_queue(self, req_id: str) -> str:
        await self.actor.close_sse_queue.remote(req_id)
        return "closed"
    
    # /history
    async def get_history(self, request_id: str, last_index: int = None):
        result = await self.actor.get_conversation_history.remote(request_id)
        if last_index is not None and isinstance(result.get("history"), list):
            result["history"] = result["history"][last_index+1:]
        return result

    # /reference
    async def get_reference_data(self, chunk_ids: list):
        result = await self.actor.get_reference_data.remote(chunk_ids)
        return result


# Ray의 요청을 비동기적으로 관리하기 위해 도입하는 큐-매니저
@ray.remote
class SSEQueueManager:
    def __init__(self):
        self.active_queues = {}
        self.lock = asyncio.Lock()

    async def create_queue(self, request_id):
        async with self.lock:
            self.active_queues[request_id] = asyncio.Queue()
            return True

    async def get_queue(self, request_id):
        return self.active_queues.get(request_id)

    async def get_token(self, request_id, timeout: float):
        queue = self.active_queues.get(request_id)
        if queue:
            try:
                token = await asyncio.wait_for(queue.get(), timeout=timeout)
                return token
            except asyncio.TimeoutError:
                return None
        return None

    async def put_token(self, request_id, token):
        async with self.lock:
            if request_id in self.active_queues:
                await self.active_queues[request_id].put(token)
                return True
            return False

    async def delete_queue(self, request_id):
        async with self.lock:
            if request_id in self.active_queues:
                del self.active_queues[request_id]
                return True
            return False

```


--- utils.py

```python

# utils.py
import json
import numpy as np
import torch
import random
import shutil
from datetime import datetime, timedelta
from transformers import (
    AutoModel,
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    AutoConfig,
)

import os
import requests

# 전역 캐시 변수 - 데이터의 변화를 감지하기 위한
_cached_data = None
_cached_data_mtime = 0

# Import vLLM utilities
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

# Define the minimum valid file size (e.g., 10MB)
MIN_WEIGHT_SIZE = 10 * 1024 * 1024

# For tracking execution time of functions
from tracking import time_tracker

# Logging
import logging

logging.basicConfig(level=logging.DEBUG)

# -------------------------------------------------
# Function: find_weight_directory - 허깅페이스 권한 문제 해결 후에 잘 사용되지 아니함
# -------------------------------------------------
# Recursively searches for weight files (safetensors or pytorch_model.bin) in a given base path.
# This method Find the files searching the whole directory
# Because, vLLM not automatically find out the model files.
# -------------------------------------------------
@time_tracker
def find_weight_directory(base_path):
    # ---- Recursively searches for weight files in a given base path ----
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if ".safetensors" in file or "pytorch_model.bin" in file:
                file_path = os.path.join(root, file)
                try:
                    if os.path.getsize(file_path) >= MIN_WEIGHT_SIZE:
                        return root, "safetensors" if ".safetensors" in file else "pt"
                    else:
                        logging.debug(
                            f"파일 {file_path}의 크기가 너무 작음: {os.path.getsize(file_path)} bytes"
                        )
                except Exception as ex:
                    logging.debug(f"파일 크기 확인 실패: {file_path} - {ex}")
    return None, None

# -------------------------------------------------
# Function: load_model
# -------------------------------------------------
@time_tracker
def load_model(config):
    # Loads the embedding model and the main LLM model (using vLLM if specified in the config).
    
    # Get the HF token from the environment variable.
    logging.info("Starting model loading...")
    token = os.getenv("HF_TOKEN_PATH")
    # Check if token is likely a file path.
    if token is not None and not token.startswith("hf_"):
        if os.path.exists(token) and os.path.isfile(token):
            try:
                with open(token, "r") as f:
                    token = f.read().strip()
            except Exception as e:
                print("DEBUG: Exception while reading token file:", e)
                logging.warning("Failed to read token from file: %s", e)
                token = None
        else:
            logging.warning("The HF_TOKEN path does not exist: %s", token)
            token = None
    else:
        print("DEBUG: HF_TOKEN appears to be a token string; using it directly:")

    if token is None or token == "":
        logging.warning("HF_TOKEN is not set. Access to gated models may fail.")
        token = None

    # -------------------------------
    # Load the embedding model and tokenizer.
    # -------------------------------
    print("Loading embedding model")
    try:
        embed_model = AutoModel.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,  # using 'token' parameter
        )
    except Exception as e:
        raise e
    try:
        embed_tokenizer = AutoTokenizer.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,
        )
    except Exception as e:
        raise e
    print(":Embedding tokenizer loaded successfully.")
    embed_model.eval()
    embed_tokenizer.model_max_length = 4096

    # -------------------------------
    # Load the main LLM model via vLLM.
    # -------------------------------
    if config.use_vllm:
        print("vLLM mode enabled. Starting to load main LLM model via vLLM.")
        if config.model.quantization_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
            print("Using 4-bit quantization.")
        elif config.model.quantization_8bit:
            bnb_config = BitsAndBytesConfig(load_in_8bit=True)
            print("Using 8-bit quantization.")
        else:
            bnb_config = None
            print("Using pure option of Model(No quantization)")

        local_model_path = os.path.join(
            config.cache_dir, "models--" + config.model_id.replace("/", "--")
        )
        local_model_path = os.path.abspath(local_model_path)

        config_file = os.path.join(local_model_path, "config.json")
        need_patch = False

        if not os.path.exists(config_file):
            os.makedirs(local_model_path, exist_ok=True)
            try:
                hf_config = AutoConfig.from_pretrained(
                    config.model_id,
                    cache_dir=config.cache_dir,
                    trust_remote_code=True,
                    token=token,
                )
                if not hasattr(hf_config, "vocab_size"):
                    # 토크나이저가 이미 로드되어 있다면 그 값을 사용하거나 기본값 지정
                    hf_config.vocab_size = getattr(embed_tokenizer, "vocab_size", 30522)
            except Exception as e:
                raise e
            config_dict = hf_config.to_dict()
            if not config_dict.get("architectures"):
                print("[MODEL-LOADING] Config file의 architectures 정보 없음, Default Gemma2 아키텍처 설정")
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config_dict, f)
        else:
            # 이미 config_file이 존재하는 경우
            with open(config_file, "r", encoding="utf-8") as f:
                config_dict = json.load(f)

            if "vocab_size" not in config_dict:
                # embed_tokenizer의 vocab_size가 존재하면 사용하고, 없으면 기본값 30522로 설정
                config_dict["vocab_size"] = getattr(embed_tokenizer, "vocab_size", 30522)
                print("[MODEL-LOADING] 'vocab_size' 속성이 없어서 기본값으로 추가합니다:", config_dict["vocab_size"])
                with open(config_file, "w", encoding="utf-8") as f:
                    json.dump(config_dict, f)
            if not config_dict.get("architectures"):
                print("[MODEL-LOADING] Config file의 architectures 정보 없음, Default Gemma2 아키텍처 설정")
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
                with open(config_file, "w", encoding="utf-8") as f:
                    json.dump(config_dict, f)

        weight_dir, weight_format = find_weight_directory(local_model_path)
        if weight_dir is None:
            print("DEBUG: No model weights found. Attempting to download model snapshot.")
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    print(f"DEBUG: Snapshot download attempt {attempt+1}...")
                    # Attempt to download the model snapshot using the Hugging Face hub function.
                    from huggingface_hub import snapshot_download
                    snapshot_download(config.model_id, cache_dir=config.cache_dir, token=token)
                    break  # If download succeeds, break out of the loop.
                except Exception as e:
                    print(f"DEBUG: Snapshot download attempt {attempt+1} failed:", e)
                    if attempt < max_retries - 1:
                        print("DEBUG: Retrying snapshot download...")
                    else:
                        raise RuntimeError(f"Snapshot download failed after {max_retries} attempts: {e}")
            # After download, try to find the weights again.
            weight_dir, weight_format = find_weight_directory(local_model_path)
            if weight_dir is None:
                raise RuntimeError(f"Unable to find model weights even after snapshot download in {local_model_path}.")

        snapshot_config = os.path.join(weight_dir, "config.json")
        if not os.path.exists(snapshot_config):
            shutil.copy(config_file, snapshot_config)
        engine_args = AsyncEngineArgs(
            model=weight_dir,
            tokenizer=config.model_id,
            download_dir=config.cache_dir,
            trust_remote_code=True,
            config_format="hf",
            load_format=weight_format,
        )
        
        vllm_conf = config.get("vllm", {})
        
        engine_args.enable_prefix_caching = False
        engine_args.scheduler_delay_factor = vllm_conf.get("scheduler_delay_factor", 0.1)
        engine_args.enable_chunked_prefill = True
        engine_args.tensor_parallel_size = vllm_conf.get("tensor_parallel_size", 1) # Using Multi-GPU at once.
        # engine_args.max_num_seqs = vllm_conf.get("max_num_seqs")
        engine_args.max_num_batched_tokens = vllm_conf.get("max_num_batched_tokens", 8192)
        # engine_args.block_size = vllm_conf.get("block_size", 128)
        engine_args.gpu_memory_utilization = vllm_conf.get("gpu_memory_utilization")
        
        if vllm_conf.get("disable_custom_all_reduce", False):
            engine_args.disable_custom_all_reduce = True # For Fixing the Multi GPU problem
            
        # # 새로 추가: disable_sliding_window 옵션 확인
        # if vllm_conf.get("disable_sliding_window", False):
        #     engine_args.sliding_window = (-1, -1)
        #     print("Sliding window disabled: engine_args.sliding_window set to (-1, -1)")
        
        # engine_args.enable_memory_defrag = True # v1 새로운 기능
        # engine_args.max_model_len = vllm_conf.get("max_model_len") # Context Length
        
        # # ★★ 추가: 슬라이딩 윈도우 비활성화 옵션 적용 ★★
        # if vllm_conf.get("disable_sliding_window", False):
        #     # cascade attention에서는 슬라이딩 윈도우가 (-1, -1)이어야 함
        #     engine_args.sliding_window = (-1, -1)
        #     print("Sliding window disabled: engine_args.sliding_window set to (-1, -1)")
        
        # print("Final EngineArgs:", engine_args)
        
        #         # ── 여기서 unified_attention 호출 추적을 위한 monkey-patch ──
        # try:
        #     if hasattr(torch.ops.vllm, "unified_attention_with_output"):
        #         orig_unified_attention = torch.ops.vllm.unified_attention_with_output
        #         def tracking_unified_attention(*args, **kwargs):
        #             logging.info("Called unified_attention_with_output with args: %s, kwargs: %s", args, kwargs)
        #             return orig_unified_attention(*args, **kwargs)
        #         torch.ops.vllm.unified_attention_with_output = tracking_unified_attention
        #         logging.info("Monkey-patched unified_attention_with_output for tracking.")
        # except Exception as e:
        #     logging.warning("Failed to monkey-patch unified_attention_with_output: %s", e)
        
        # # ── 끝 ──

        print("EngineArgs setting be finished")
        
        try:
            # --- v1 구동 해결책: 현재 스레드가 메인 스레드가 아니면 signal 함수를 임시 패치 ---
            import threading, signal
            if threading.current_thread() is not threading.main_thread():
                original_signal = signal.signal
                signal.signal = lambda s, h: None  # signal 설정 무시
                print("비메인 스레드에서 signal.signal을 monkey-patch 하였습니다.")
            # --- v1 구동 해결책: ------------------------------------------------------ ---
            engine = AsyncLLMEngine.from_engine_args(engine_args) # Original
            # v1 구동 해결책: 엔진 생성 후 원래 signal.signal으로 복원 (필요 시) ----------------- ---
            if threading.current_thread() is not threading.main_thread():
                signal.signal = original_signal
            # --- v1 구동 해결책: ------------------------------------------------------ ---
            print("DEBUG: vLLM engine successfully created.") # Original
            
        except Exception as e:
            print("DEBUG: Exception during engine creation:", e)
            if "HeaderTooSmall" in str(e):
                print("DEBUG: Falling back to PyTorch weights.")
                fallback_dir = None
                for root, dirs, files in os.walk(local_model_path):
                    for file in files:
                        if (
                            "pytorch_model.bin" in file
                            and os.path.getsize(os.path.join(root, file))
                            >= MIN_WEIGHT_SIZE
                        ):
                            fallback_dir = root
                            break
                    if fallback_dir:
                        break
                if fallback_dir is None:
                    logging.error(
                        "DEBUG: No PyTorch weight file found in", local_model_path
                    )
                    raise e
                engine_args.load_format = "pt"
                engine_args.model = fallback_dir
                print("DEBUG: New EngineArgs for fallback:", engine_args)
                engine = AsyncLLMEngine.from_engine_args(engine_args)
                print("DEBUG: vLLM engine created with PyTorch fallback.")
            else:
                logging.error("DEBUG: Engine creation failed:", e)
                raise e

        engine.is_vllm = True

        print("DEBUG: Loading main LLM tokenizer with token authentication.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
                local_files_only=True  # Force loading from local cache to avoid hub requests
            )
        except Exception as e:
            print("DEBUG: Exception loading main tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        return engine, tokenizer, embed_model, embed_tokenizer

    else:
        print("DEBUG: vLLM is not used. Loading model via standard HF method.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        try:
            model = AutoModelForCausalLM.from_pretrained(
                config.model_id,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                cache_dir=config.cache_dir,
                quantization_config=bnb_config,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading model:", e)
            raise e
        model.eval()
        return model, tokenizer, embed_model, embed_tokenizer

# -------------------------------------------------
# Function: load_data
# -------------------------------------------------
@time_tracker
def load_data(data_path):
    global _cached_data, _cached_data_mtime
    try:
        current_mtime = os.path.getmtime(data_path)
    except Exception as e:
        print("파일 수정 시간 확인 실패:", e)
        return None

    # 캐시가 비어있거나 파일 수정 시간이 변경된 경우 데이터 재로드
    if _cached_data is None or current_mtime != _cached_data_mtime:
        with open(data_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # --- 디버그 함수: 벡터 포맷 검사 ---
        debug_vector_format(data)

        # 데이터 전처리 (예: 리스트 변환 및 numpy, torch 변환)
        file_names = []
        chunk_ids = []  # >>> CHANGED: Added to record each chunk's ID
        titles = []
        times = []
        vectors = []
        texts = []
        texts_short = []
        texts_vis = []
        missing_time = 0

        for file_obj in data:
            for chunk in file_obj["chunks"]:
                file_names.append(file_obj["file_name"])
                chunk_ids.append(chunk.get("chunk_id", 0))  # >>> CHANGED: Record chunk_id
                try:
                    arr = np.array(chunk["vector"])
                    vectors.append(arr)
                except Exception as e:
                    logging.warning(f"[load_data] 벡터 변환 오류: {e} → 빈 벡터로 대체")
                    vectors.append(np.zeros((1, 768), dtype=np.float32))  # 임의로 1x768 형식
                
                titles.append(chunk["title"])
                
                # 날짜 파싱
                if chunk["date"]:
                    try:
                        times.append(datetime.strptime(chunk["date"], "%Y-%m-%d"))
                    except ValueError:
                        logging.warning(f"잘못된 날짜 형식: {chunk['date']} → 기본 날짜로 대체")
                        times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))
                        missing_time += 1
                else:
                    missing_time += 1
                    times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))

                texts.append(chunk["text"])
                texts_short.append(chunk["text_short"])
                texts_vis.append(chunk["text_vis"])

        # 실제 텐서로 변환
        try:
            vectors = np.array(vectors)
            vectors = torch.from_numpy(vectors).to(torch.float32)
        except Exception as e:
            logging.error(f"[load_data] 최종 벡터 텐서 변환 오류: {str(e)}")
            # 필요 시 추가 처리

        _cached_data = {
            "file_names": file_names,
            "chunk_ids": chunk_ids,  # >>> CHANGED: Saved chunk IDs here
            "titles": titles,
            "times": times,
            "vectors": vectors,
            "texts": texts,
            "texts_short": texts_short,
            "texts_vis": texts_vis,
        }
        _cached_data_mtime = current_mtime
        print(f"Data loaded! Length: {len(titles)}, Missing times: {missing_time}")
    else:
        print("Using cached data")

    return _cached_data

# -------------------------------------------------
# Function: debug_vector_format
# -------------------------------------------------
def debug_vector_format(data):
    """
    data(List[Dict]): load_data에서 JSON으로 로드된 객체.
    각 file_obj에 대해 chunks 리스트를 순회하며 vector 형식을 디버깅 출력.
    """
    print("\n[DEBUG] ===== 벡터 형식 검사 시작 =====")
    for f_i, file_obj in enumerate(data):
        file_name = file_obj.get("file_name", f"Unknown_{f_i}")
        chunks = file_obj.get("chunks", [])
        for c_i, chunk in enumerate(chunks):
            vector_data = chunk.get("vector", None)
            if vector_data is None:
                # print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector 없음(None)")
                continue
            # 자료형, 길이, shape 등 확인
            vector_type = type(vector_data)
            # shape을 안전하게 얻기 위해 np.array 변환 시도
            try:
                arr = np.array(vector_data)
                shape = arr.shape
                # print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector_type={vector_type}, shape={shape}")
            except Exception as e:
                print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector 변환 실패: {str(e)}")
    print("[DEBUG] ===== 벡터 형식 검사 종료 =====\n")

# -------------------------------------------------
# Function: random_seed
# -------------------------------------------------
@time_tracker
def random_seed(seed):
    # Set random seed for Python's built-in random module
    random.seed(seed)

    # Set random seed for NumPy
    np.random.seed(seed)

    # Set random seed for PyTorch
    torch.manual_seed(seed)

    # Ensure the same behavior on different devices (CPU vs GPU)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # If using multi-GPU.

    # Enable deterministic algorithms
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

# -------------------------------------------------
# Function: process_to_format
# -------------------------------------------------
@time_tracker
def process_to_format(qry_contents, type):
    # 여기서 RAG 시스템을 호출하거나 답변을 생성하도록 구현하세요.
    # 예제 응답 형식
    ### rsp_type : RA(Retrieval All), RT(Retrieval Text), RB(Retrieval taBle), AT(Answer Text), AB(Answer taBle) ###
    print("[SOOWAN] process_to_format 진입")
    if type == "Retrieval":
        print("[SOOWAN] 타입 : 리트리버")
        tmp_format = {"rsp_type": "R", "rsp_tit": "남성 내부 데이터", "rsp_data": []}
        for i, form in enumerate(qry_contents):
            tmp_format_ = {
                "rsp_tit": f"{i+1}번째 검색데이터: {form['title']} (출처:{form['file_name']})",
                "rsp_data": form["contents"],
                "chunk_id": form.get("chunk_id"),
            }
            tmp_format["rsp_data"].append(tmp_format_)
        return tmp_format

    elif type == "SQL":
        print("[SOOWAN] 타입 : SQL")
        tmp_format = {
            "rsp_type": "R",
            "rsp_tit": "남성 내부 데이터",
            "rsp_data": [{"rsp_tit": "SQL Query 결과표", "rsp_data": []}],
        }
        tmp_format_sql = {
            "rsp_type": "TB",
            "rsp_tit": qry_contents[0]["title"],
            "rsp_data": qry_contents[0]["data"],
        }
        tmp_format_chart = {
            "rsp_type": "CT",
            "rsp_tit": qry_contents[1]["title"],
            "rsp_data": {"chart_tp": "BAR", "chart_data": qry_contents[1]["data"]},
        }
        tmp_format["rsp_data"][0]["rsp_data"].append(tmp_format_sql)
        # tmp_format['rsp_data'].append(tmp_format_chart)
        return tmp_format, tmp_format_chart

    elif type == "Answer":
        print("[SOOWAN] 타입 : 대답")
        tmp_format = {"rsp_type": "A", "rsp_tit": "답변", "rsp_data": []}
        # for i, form in enumerate(qry_contents):
            # if i == 0:
        tmp_format_ = {"rsp_type": "TT", "rsp_data": qry_contents}
        tmp_format["rsp_data"].append(tmp_format_)
            # elif i == 1:
            #     tmp_format["rsp_data"].append(form)
            # else:
            #     None

        return tmp_format

    else:
        print("Error! Type Not supported!")
        return None

# @time_tracker
# def process_format_to_response(formats, qry_id, continue_="C", update_index=1):
#     # Get multiple formats to tuple

#     ans_format = {
#         "status_code": 200,
#         "result": "OK",
#         "detail": "",
#         "continue":continue_,
#         "qry_id": qry_id,
#         "rsp_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
#         "data_list": [],
#     }

#     # 누적된 토큰을 하나의 문자열로 결합합니다.
#     aggregated_answer = "".join(token.get("answer", "") for token in formats)
#     ans_format["data_list"].append({
#         "rsp_type": "A",
#         "rsp_tit": f"답변{update_index}",
#         "rsp_data": [
#             {
#                 "rsp_type": "TT",
#                 "rsp_data": aggregated_answer
#             }
#         ]
#     })
    
#     # Validate JSON before returning
#     try:
#         json.dumps(ans_format, ensure_ascii=False)  # Test JSON validity
#     except Exception as e:
#         print(f"[ERROR] Invalid JSON structure: {str(e)}")
#         ans_format["status_code"] = 500
#         ans_format["result"] = "ERROR"
#         ans_format["detail"] = f"JSON Error: {str(e)}"

#     # for format in formats:
#     #     ans_format["data_list"].append(format)

#     # return json.dumps(ans_format, ensure_ascii=False)
#     return ans_format

@time_tracker
def process_format_to_response(formats, qry_id, continue_="C", update_index=1):
    # If there are any reference tokens, return only them.
    reference_tokens = [token for token in formats if token.get("type") == "reference"]
    if reference_tokens:
        # For this example, we'll use the first reference token.
        ref = reference_tokens[0]
        # Add the extra keys.
        ref["qry_id"] = qry_id
        ref["continue"] = continue_
        ref["rsp_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")
        # Ensure that a "rsp_tit" key exists to satisfy downstream requirements.
        if "rsp_tit" not in ref:
            ref["rsp_tit"] = "Reference"
        return ref

    # Otherwise, aggregate the normal answer tokens.
    normal_tokens = [token.get("answer", "") for token in formats if token.get("type") != "reference"]
    aggregated_answer = "".join(normal_tokens)
    
    ans_format = {
        "status_code": 200,
        "result": "OK",
        "detail": "",
        "continue": continue_,
        "qry_id": qry_id,
        "rsp_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
        "data_list": [{
            "rsp_type": "A",
            "rsp_tit": f"답변{update_index}",
            "rsp_data": [{
                "rsp_type": "TT",
                "rsp_data": aggregated_answer
            }]
        }]
    }
    
    # Validate JSON structure before returning.
    try:
        json.dumps(ans_format, ensure_ascii=False)
    except Exception as e:
        print(f"[ERROR] Invalid JSON structure: {str(e)}")
        ans_format["status_code"] = 500
        ans_format["result"] = "ERROR"
        ans_format["detail"] = f"JSON Error: {str(e)}"
    
    return ans_format



# @time_tracker
# def process_format_to_response(formats, qry_id, continue_="C", update_index=1):
#     # 누적된 토큰들을 하나의 문자열로 결합합니다.
#     aggregated_answer = "".join(token.get("answer", "") for token in formats)
    
#     # retrieval과 동일한 구조를 위해, 답변 데이터는 내부 data_list가 딕셔너리 형태로 구성됩니다.
#     answer = {
#         "rsp_type": "A",                # Answer
#         "rsp_tit": f"답변{update_index}",
#         "rsp_data": [                    # 바로 텍스트 응답 리스트를 구성
#             {
#                 "rsp_tit": f"답변{update_index}",
#                 "rsp_data": [
#                     {
#                         'rsp_type': 'TT',
#                         'rsp_tit': '',
#                         'rsp_data': aggregated_answer,
#                     }
#                 ]
                
#             }
#         ]
#     }
    
#     # 최종 응답 구조: 최상위에 data_list는 리스트이고, 내부에 딕셔너리로 답변 데이터를 포함합니다.
#     ans_format = {
#         "status_code": 200,
#         "result": "OK",
#         "detail": "Answer",
#         "continue": continue_,
#         "qry_id": qry_id,
#         "rsp_time": datetime.now().isoformat(),
#         "data_list": [
#             {
#                 "type": "answer",               # 응답 타입 answer
#                 "status_code": 200,
#                 "result": "OK",
#                 "detail": "Answer",
#                 "evt_time": datetime.now().isoformat(),
#                 "data_list": answer              # retrieval의 data_list와 동일하게 딕셔너리 형태
#             }
#         ]
#     }
#     return ans_format

@time_tracker
def error_format(message, status, qry_id=""):
    ans_format = {
        "status_code": status,
        "result": message,
        "qry_id": qry_id,  # 추가: qry_id 포함
        "detail": "",
        "evt_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
    }
    return json.dumps(ans_format)

# @time_tracker
# def send_data_to_server(data, url):
#     headers = {
#         "Content-Type": "application/json; charset=utf-8"
#     }
#     try:
#         # 다른 서버로 데이터를 전송 (POST 요청)
#         response = requests.post(url, json=data, headers=headers)
#         if response.status_code == 200:
#             print(f"Data sent successfully: {data}")
#         else:
#             print(f"Failed to send data: {response.status_code}")
#             print(f"Failed data: {data}")
#     except requests.exceptions.RequestException as e:
#         print(f"Error sending data: {e}")
@time_tracker     
def send_data_to_server(data, url):
    try:
        if not data or "data_list" not in data:
            print("[ERROR] Empty or Invalid data structure")
            return
        # Log reference data if present
        for item in data["data_list"]:
            if item.get("rsp_type") == "A" and "references" in str(item):
                print(f"[DEBUG] Sending reference data: {json.dumps(data, ensure_ascii=False, indent=2)}")
        response = requests.post(url, json=data, timeout=10)
        
        if response.status_code != 200:
            print(f"[ERROR] Failed to send data: {response.status_code}, {response.text}")
        
        return response

    except Exception as e:
        print(f"[ERROR] send_data_to_server encountered an error: {str(e)}")


# ---------------------- 벡터화 -----------------------

import yaml
from box import Box
# Configuration
with open("./config.yaml", "r") as f:
    config_yaml = yaml.load(f, Loader=yaml.FullLoader)
    config = Box(config_yaml)

# 임베딩 모델 및 토크나이저 (청크 벡터화를 위해 별도 로드)
embedding_model = AutoModel.from_pretrained(config.embed_model_id, cache_dir=config.cache_dir)
embedding_tokenizer = AutoTokenizer.from_pretrained(config.embed_model_id, cache_dir=config.cache_dir)
embedding_model.eval()

# -------------------- 벡터화 함수 --------------------
@time_tracker
def vectorize_content(content):
    try:
        inputs = embedding_tokenizer(content, padding=True, truncation=True, return_tensors="pt")
        with torch.no_grad():
            outputs = embedding_model(**inputs, return_dict=False)
        # 첫 토큰의 임베딩을 사용 (1D 벡터)
        vector = outputs[0][:, 0, :].squeeze(0).tolist()
        
        # 벡터 일관성 확인
        expected_dim = 768  # 임베딩 모델 차원에 맞게 조정
        
        # 리스트가 아닌 경우 변환 시도
        if not isinstance(vector, list):
            print(f"경고: 벡터가 리스트가 아님, 타입: {type(vector)}")
            try:
                vector = list(vector)
            except Exception as e:
                print("오류: 벡터를 리스트로 변환 실패:", e)
                vector = [0.0] * expected_dim  # 기본 벡터 제공
        
        # 벡터 차원 확인 및 조정
        if len(vector) != expected_dim:
            print(f"경고: 벡터 차원 불일치. 예상: {expected_dim}, 실제: {len(vector)}")
            if len(vector) < expected_dim:
                # 부족한 차원은 0으로 패딩
                vector.extend([0.0] * (expected_dim - len(vector)))
            else:
                # 초과 차원은 자르기
                vector = vector[:expected_dim]
        
        # 기존 파일 형식과 일치하도록 항상 2차원 배열 형식으로 반환 ([[...] 형태])
        if vector and not isinstance(vector[0], list):
            return [vector]
        return vector
    except Exception as e:
        print(f"vectorize_content 함수 오류: {str(e)}")
        # 오류 시 기본 벡터 반환 (2차원 형식)
        return [[0.0] * 768]

# -------------------- 텍스트 출력 필드 정규화 함수 --------------------
def normalize_text_vis(text_vis):
    """
    text_vis가 이미 올바른 리스트-딕셔너리 구조이면 그대로 반환하고,
    그렇지 않은 경우 기본 구조로 감싸서 반환합니다.
    """
    if isinstance(text_vis, list) and len(text_vis) > 0 and isinstance(text_vis[0], dict):
        # 필요한 키가 존재하는지 확인
        if all(k in text_vis[0] for k in ("rsp_type", "rsp_tit", "rsp_data")):
            return text_vis
    if isinstance(text_vis, str):
        return [{
            "rsp_type": "TT",
            "rsp_tit": "",
            "rsp_data": text_vis
        }]
    return [{
        "rsp_type": "TT",
        "rsp_tit": "",
        "rsp_data": str(text_vis)
    }]

# -------------------- 데이터셋 진단 및 수정 도구 --------------------
# 데이터셋 진단 및 복구 함수 (utils.py 또는 별도 파일에 추가)
def diagnose_and_fix_dataset(data_path, output_path=None):
    """
    데이터셋의 벡터 차원 문제를 진단하고 수정합니다.
    """
    try:
        print(f"데이터셋 진단 중: {data_path}")
        with open(data_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        print(f"데이터셋 내 파일 수: {len(data)}")
        dimensions = {}
        fixed_count = 0
        problem_count = 0
        
        # 1단계: 가장 흔한 차원 찾기
        for file_idx, file in enumerate(data):
            file_name = file.get("file_name", f"Unknown-{file_idx}")
            for chunk_idx, chunk in enumerate(file.get("chunks", [])):
                if "vector" in chunk and chunk["vector"]:
                    vector = chunk["vector"]
                    try:
                        if isinstance(vector, list):
                            dim = len(vector)
                            dimensions[dim] = dimensions.get(dim, 0) + 1
                        else:
                            print(f"벡터가 리스트가 아님: {file_name}, 청크 {chunk_idx}")
                            problem_count += 1
                    except Exception as e:
                        print(f"벡터 길이 확인 실패: {file_name}, 청크 {chunk_idx} - {str(e)}")
                        problem_count += 1
        
        if dimensions:
            # 가장 흔한 차원 찾기
            expected_dim = max(dimensions.items(), key=lambda x: x[1])[0]
            print(f"가장 흔한 벡터 차원: {expected_dim} (총 {dimensions[expected_dim]}개 발견)")
            print(f"발견된 모든 차원: {dimensions}")
        else:
            print("데이터셋에서 유효한 벡터를 찾을 수 없습니다!")
            return False
        
        # 2단계: 잘못된 차원의 벡터 수정
        for file_idx, file in enumerate(data):
            file_name = file.get("file_name", f"Unknown-{file_idx}")
            for chunk_idx, chunk in enumerate(file.get("chunks", [])):
                if "vector" in chunk and chunk["vector"]:
                    vector = chunk["vector"]
                    try:
                        if not isinstance(vector, list):
                            print(f"리스트가 아닌 벡터 수정 시도: {file_name}, 청크 {chunk_idx}")
                            try:
                                vector = list(vector)
                                chunk["vector"] = vector
                                fixed_count += 1
                            except:
                                # 변환 실패 시 빈 벡터 생성
                                chunk["vector"] = [0.0] * expected_dim
                                fixed_count += 1
                                print(f"리스트 변환 실패, 기본 벡터 사용")
                        
                        dim = len(vector)
                        if dim != expected_dim:
                            print(f"벡터 차원 수정: {file_name}, 청크 {chunk_idx} (차원: {dim})")
                            if dim < expected_dim:
                                # 0으로 패딩
                                chunk["vector"] = vector + [0.0] * (expected_dim - dim)
                            else:
                                # 자르기
                                chunk["vector"] = vector[:expected_dim]
                            fixed_count += 1
                    except Exception as e:
                        print(f"벡터 처리 중 오류: {file_name}, 청크 {chunk_idx} - {str(e)}")
                        problem_count += 1
        
        print(f"고정된 벡터 수: {fixed_count}, 문제 벡터 수: {problem_count}")
        
        # 수정된 데이터셋 저장
        if output_path is None:
            output_path = data_path
        
        # 덮어쓰기 전 백업 생성
        if output_path == data_path:
            backup_path = f"{data_path}.bak"
            print(f"백업 생성: {backup_path}")
            with open(backup_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"수정된 데이터셋 저장: {output_path}")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return True
    
    except Exception as e:
        print(f"데이터셋 진단 중 오류: {str(e)}")
        return False

```

# Error log
```log

(InferenceActor pid=563) 2025-03-13 07:35:19,107 INFO Entering load_model()
(InferenceActor pid=563) 2025-03-13 07:35:19,107 INFO Starting model loading...
(InferenceActor pid=563) Loading embedding model
(InferenceActor pid=563) 2025-03-13 07:35:19,465 INFO Entering find_weight_directory()
(InferenceActor pid=563) 2025-03-13 07:35:19,466 INFO Exiting find_weight_directory() -- Elapsed: 0.00s
(InferenceActor pid=563) :Embedding tokenizer loaded successfully.
(InferenceActor pid=563) vLLM mode enabled. Starting to load main LLM model via vLLM.
(InferenceActor pid=563) Using pure option of Model(No quantization)
(InferenceActor pid=563) [MODEL-LOADING] 'vocab_size' 속성이 없어서 기본값으로 추가합니다: 32000
(InferenceActor pid=563) EngineArgs setting be finished
(InferenceActor pid=563) 비메인 스레드에서 signal.signal을 monkey-patch 하였습니다.
(InferenceActor pid=563) INFO 03-13 07:35:19 config.py:2444] Downcasting torch.float32 to torch.float16.
(InferenceActor pid=563) INFO 03-13 07:35:24 config.py:549] This model supports multiple tasks: {'generate', 'classify', 'embed', 'reward', 'score'}. Defaulting to 'generate'.
(InferenceActor pid=563) INFO 03-13 07:35:24 config.py:1555] Chunked prefill is enabled with max_num_batched_tokens=16384.
(InferenceActor pid=563) INFO 03-13 07:35:25 core.py:50] Initializing a V1 LLM engine (v0.7.3) with config: model='/workspace/huggingface/models--google--gemma-3-27b-it/snapshots/dfb98f29ff907e391ceed2be3834ca071ea260f1', speculative_config=None, tokenizer='google/gemma-3-27b-it', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=True, dtype=torch.float16, max_seq_len=1048576, download_dir='/workspace/huggingface', load_format=LoadFormat.SAFETENSORS, tensor_parallel_size=1, pipeline_parallel_size=1, disable_custom_all_reduce=True, quantization=None, enforce_eager=False, kv_cache_dtype=auto,  device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='xgrammar'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=/workspace/huggingface/models--google--gemma-3-27b-it/snapshots/dfb98f29ff907e391ceed2be3834ca071ea260f1, num_scheduler_steps=1, multi_step_stream_outputs=True, enable_prefix_caching=False, chunked_prefill_enabled=True, use_async_output_proc=True, disable_mm_preprocessor_cache=False, mm_processor_kwargs=None, pooler_config=None, compilation_config={"level":3,"custom_ops":["none"],"splitting_ops":["vllm.unified_attention","vllm.unified_attention_with_output"],"use_inductor":true,"compile_sizes":[],"use_cudagraph":true,"cudagraph_num_of_warmups":1,"cudagraph_capture_sizes":[512,504,496,488,480,472,464,456,448,440,432,424,416,408,400,392,384,376,368,360,352,344,336,328,320,312,304,296,288,280,272,264,256,248,240,232,224,216,208,200,192,184,176,168,160,152,144,136,128,120,112,104,96,88,80,72,64,56,48,40,32,24,16,8,4,2,1],"max_capture_size":512}
(InferenceActor pid=563) WARNING 03-13 07:35:25 utils.py:2262] Methods determine_num_available_blocks,device_config,get_cache_block_size_bytes,list_loras,load_config,pin_lora,remove_lora,scheduler_config not implemented in <vllm.v1.worker.gpu_worker.Worker object at 0x7018fbd285d0>
(InferenceActor pid=563) INFO 03-13 07:35:26 gpu_model_runner.py:1049] Starting to load model /workspace/huggingface/models--google--gemma-3-27b-it/snapshots/dfb98f29ff907e391ceed2be3834ca071ea260f1...
(InferenceActor pid=563) WARNING 03-13 07:35:26 utils.py:78] Gemma3ForConditionalGeneration has no vLLM implementation, falling back to Transformers implementation. Some features may not be supported and performance may not be optimal.
(InferenceActor pid=563) INFO 03-13 07:35:26 transformers.py:129] Using Transformers backend.
(InferenceActor pid=563) WARNING 03-13 07:35:26 config.py:3473] `torch.compile` is turned on, but the model /workspace/huggingface/models--google--gemma-3-27b-it/snapshots/dfb98f29ff907e391ceed2be3834ca071ea260f1 does not support it. Please open an issue on GitHubif you want it to be supported.
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291] EngineCore hit an exception: Traceback (most recent call last):
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 283, in run_engine_core
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     engine_core = EngineCoreProc(*args, **kwargs)
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 238, in __init__
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     super().__init__(vllm_config, executor_class, log_stats)
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/v1/engine/core.py", line 56, in __init__
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     self.model_executor = executor_class(vllm_config)
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]                           ^^^^^^^^^^^^^^^^^^^^^^^^^^^
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/executor/executor_base.py", line 52, in __init__
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     self._init_executor()
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py", line 47, in _init_executor
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     self.collective_rpc("load_model")
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/executor/uniproc_executor.py", line 56, in collective_rpc
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     answer = run_method(self.driver_worker, method, args, kwargs)
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]              ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/utils.py", line 2196, in run_method
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     return func(*args, **kwargs)
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]            ^^^^^^^^^^^^^^^^^^^^^
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/v1/worker/gpu_worker.py", line 133, in load_model
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     self.model_runner.load_model()
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/v1/worker/gpu_model_runner.py", line 1051, in load_model
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     self.model = get_model(vllm_config=self.vllm_config)
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]                  ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/model_executor/model_loader/__init__.py", line 14, in get_model
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     return loader.load_model(vllm_config=vllm_config)
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 406, in load_model
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     model = _initialize_model(vllm_config=vllm_config)
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/model_executor/model_loader/loader.py", line 125, in _initialize_model
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     return model_class(vllm_config=vllm_config, prefix=prefix)
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/vllm/model_executor/models/transformers.py", line 135, in __init__
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     self.vocab_size = config.vocab_size
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]                       ^^^^^^^^^^^^^^^^^
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]   File "/opt/conda/lib/python3.11/site-packages/transformers/configuration_utils.py", line 214, in __getattribute__
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]     return super().__getattribute__(key)
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291] AttributeError: 'Gemma3Config' object has no attribute 'vocab_size'
(InferenceActor pid=563) ERROR 03-13 07:35:26 core.py:291]
(raylet) A worker died or was killed while executing a task by an unexpected system error. To troubleshoot the problem, check the logs for the dead worker. RayTask ID: ffffffffffffffffb613e7356796f8beb37b1d1401000000 Worker ID: 7fd72ec033883704502de90cb459aecb840009fa4790d14e8a19025c Node ID: 9e022cea4b6634e84c679fa9b6033cebfdf9b25e733c384ffdcdbba7 Worker IP address: 172.17.0.4 Worker port: 42145 Worker PID: 563 Worker exit type: SYSTEM_ERROR Worker exit detail: Worker unexpectedly exits with a connection error code 2. End of file. There are some potential root causes. (1) The process is killed by SIGKILL by OOM killer due to high memory usage. (2) ray stop --force is called. (3) The worker is crashed unexpectedly due to SIGSEGV or other unexpected errors.


```

-----------------

# Requirements


Base-Knowledge:
 - 위 파일들은 LLM 모델을 활용한 사내 RAG 서비스의 소스 코드입니다.
 - 파일 트리와 각 파일의 내용이 코드 블록 내에 포함되어, 프로젝트의 현재 구조와 상태를 한눈에 파악할 수 있습니다.
 - vLLM과 ray를 활용하여 사용성 및 추론 성능을 개선하였습니다.
 - Langchain을 활용하여 reqeust_id별로 대화를 저장하고 활용할 수 있습니다.
 - 에러 발생 시 로깅을 통해 문제를 추적할 수 있도록 설계되었습니다.

Answer-Rule:
 1. 추후 소스 코드 개선, 구조 변경, 에러 로그 추가 등 다양한 요구사항을 반영할 수 있는 확장성을 고려합니다.
 2. 전체 코드는 한국어로 주석 및 설명이 포함되어, 이해와 유지보수가 용이하도록 작성됩니다.

My-Requirements:
 1. Gemma3로 모델을 업데이트하고자 하는데 다음과 같은 에러가 발생하고 있어. 이에 대한 원인을 분석하고 이를 해결해줘.
