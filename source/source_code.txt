# Project Tree of RAG for company

```
├─ app.py
├─ config.yaml
├─ core/RAG.py
├─ core/generation.py
├─ core/image_processing.py
├─ core/retrieval.py
├─ ray_deploy/langchain.py
├─ ray_deploy/ray_setup.py
├─ ray_deploy/ray_utils.py
├─ utils/debug_tracking.py
├─ utils/utils_format.py
└─ utils/utils_load.py
```

--- config.yaml

```yaml

# config.yaml
# Server : 2x H100 (80 GB SXM5), 52 CPU cores, 483.2 GB RAM, 6 TB SSD
### Model
model_id : 'google/gemma-3-27b-it'
response_url : "http://202.20.84.16:8083/responseToUI"
# response_url : "https://eo5smhcazmp1bqe.m.pipedream.net"

### ray
ray:
  actor_count: 1                  # 총 Actor 개수(same as num_replicas)
  num_gpus: 1                     # 각 Actor(Node)가 점유하고 있는 GPU 갯수
  num_cpus: 24                    # 각 Actor(Node)가 점유하고 있는 CPU 갯수 (1 actor 시에 gpu 48개, 2 actor 시에 gpu 24개 할당)
  max_batch_size: 5               # max_concurrency(actor 최대 동시 처리량, default 1000)로 대체해도 됨
  batch_wait_timeout: 0.05        
  max_ongoing_requests: 100       # ray.serve에서 deployment setting으로 동시 요청 처리 갯수를 의미함(Batch랑 다름)

### vllm
use_vllm: True                    # vLLM 사용 여부
vllm:
  enable_prefix_caching: True
  scheduler_delay_factor: 0.1
  enable_chunked_prefill: True
  tensor_parallel_size: 1         # vLLM의 GPU 사용 갯수 (!!!! num_gpus 보다 작아야 함 !!!!)
  max_num_seqs: 128               # v1에 따른 상향
  max_num_batched_tokens: 34000   # v1에 따른 상향
  block_size: 128                 # 미적용
  gpu_memory_utilization: 0.99    # v0: 0.95 / v1: 0.99로 상향
  ### 모델 변경에 따른 추가된 설정
  max_model_len: 20000            # For the new model (Gemma2 : 8192) / Gemma3는 1xH100(SXM5)일 경우 최대 22000~23000, so that 20000으로 세팅
  ### v1에 따른 새로운 인자값
  disable_custom_all_reduce: true
  # enable_memory_defrag: True      # Gemma3 이식 작업에서 도입, 현재 미사용
  # disable_sliding_window: True    # Gemma3 이식 작업에서 도입, 현재 미사용, sliding window 비활성화 - cascade attention과 충돌이 나서 이를 비활성화
  ### Gemma3 - Multi Modal 이미지 기능에 따른 새로운 인자값
  mm_processor_kwargs:            # 이미지 처리 시 사용되는 추가 인자 정의
    do_pan_and_scan: True         # 이미지 내 객체 감지 및 영역 스캔 기능을 활성화
  disable_mm_preprocessor_cache: False # 이미지 전처리 캐시를 비활성화 할지 여부
  limit_mm_per_prompt:            # 프롬프트 당 허용되는 멀티모달(예, 이미지) 입력의 최대 개수
    image: 2                      # 최대 image 처리 개수

### model huggingface setting and sampling params
model:
  quantization_4bit : False       # Quantize 4-bit
  quantization_8bit : False       # Quantize 8-bit
  max_new_tokens : 2048           # 생성할 최대 토큰 수
  do_sample : False               # True 일때만 아래가 적용
  temperature : 1.0               # 텍스트 다양성 조정: 높을수록 창의력 향상 (1.0)
  top_k : 30                      # top-k 샘플링: 상위 k개의 후보 토큰 중 하나를 선택 (50)
  top_p : 1.0                     # top-p 샘플링: 누적 확률을 기준으로 후보 토큰을 선택 (1.0 보다 낮을수록 창의력 증가)
  repetition_penalty : 1.0        # 같은 단어를 반복해서 출력하지 않도록 패널티를 부여 (1.0 보다 클수록 페널티 증가)
embed_model_id : 'BM-K/KoSimCSE-roberta-multitask'
# cache_dir : "D:/huggingface" # Windows Local
# cache_dir : "/media/user/7340afbb-e4ce-4a38-8210-c6362e85eae7/RAG/RAG_application/huggingface" # Local
cache_dir : "/workspace/huggingface"  # Docker

### Data
data_path : '/workspace/data/0228_DB_.json'     # VectorDB Path - New one (계약서 데이터 포함)
# data_path : 'data/1104_NS_DB_old.json' # VectorDB Path - Old one
metadata_path : '/workspace/data/Metadata.json' # Metadata.json Path
metadata_unno : '/workspace/data/METADATA_OPRAIMDG.json'
sql_data_path : '/workspace/data/poc.db'        # SQLite 데이터베이스 Path

### Retrieve
N : 5 # Retrieve top N chunks

### Others
beep : '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------'
seed : 4734                     # Radom Seed
k : 15                          # SQL Max Rows (None=MAX)

```


--- app.py

```python

# app.py

########## Setting the Thread to main ##########
import multiprocessing
multiprocessing.set_start_method("spawn", force=True)

# Setting environment variable
import os
# --------- Huggingface setting ---------
# For the Huggingface Token setting
os.environ["HF_HOME"] = "/workspace/huggingface"
os.environ["HF_TOKEN_PATH"] = "/root/.cache/huggingface/token"
# Increase download timeout (in seconds)
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "60"
# --------- OpenMP: Open-Multi-Processing API setting ---------
# Change to GNU to using OpenMP. Because this is more friendly with CUDA(NVIDIA), and Some library(Pytorch, Numpy, vLLM etc) use the OpenMP so that set the GNU is better.
os.environ["MKL_THREADING_LAYER"] = "GNU"
# --------- vLLM ---------
# Use the vLLM as v1 version
os.environ["VLLM_USE_V1"] = "1"
os.environ["VLLM_STANDBY_MEM"] = "0"

# Metrics logging configuration 
os.environ["VLLM_METRICS_LEVEL"] = "2"  # Increase to max level
os.environ["VLLM_LOG_STATS_INTERVAL"] = "5"  # Report every 5 seconds
os.environ["VLLM_SHOW_PROGRESS"] = "1"  # Show progress bars
os.environ["VLLM_LOG_LEVEL"] = "DEBUG"  # Set log level to DEBUG

os.environ["VLLM_PROFILE_MEMORY"]= "1"
# GPU 단독 사용(박상제 연구원님이랑 분기점 - 연구원님 0번 GPU, 수완 1번 GPU - 기본 안정화 세팅은 0번 GPU)
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

# 토크나이저 병렬 처리 명시적 비활성화
os.environ["TOKENIZERS_PARALLELISM"] = "false"

# VLLM FLASH ATTENTION SETTING
os.environ["VLLM_ATTENTION_BACKEND"] = "FLASH_ATTN"
os.environ["VLLM_WORKER_MULTIPROC_METHOD"] = "spawn"

# --------- Ray ---------
os.environ["RAY_DEDUP_LOGS"] = "0"

from flask import (
    Flask,
    request,
    Response,
    render_template,
    jsonify,
    g,
    stream_with_context,
)
import json
from datetime import datetime

# Import the Ray modules
from ray_deploy import init_ray, InferenceService, SSEQueueManager
# Import utils
from utils import random_seed, error_format, send_data_to_server, process_format_to_response

# ------ checking process of the thread level
import logging
import threading

# Metrics logging configuration - 여기에 모든 로깅 설정 통합
import logging
from logging.handlers import RotatingFileHandler
import sys

# 글로벌 로깅 설정
# app.py 로깅 설정 수정
def setup_logging():
    """
    통합 로깅 설정 - 중복 로그 방지
    """
    # 로그 포맷 설정
    log_format = '%(asctime)s - %(name)s - %(levelname)s - [%(threadName)s] %(message)s'
    
    # 루트 로거 설정
    root_logger = logging.getLogger()
    root_logger.setLevel(logging.INFO)
    
    # 기존 핸들러 제거 (중복 방지)
    for handler in root_logger.handlers[:]:
        root_logger.removeHandler(handler)
    
    # 콘솔 핸들러 추가
    console_handler = logging.StreamHandler(sys.stdout)
    console_handler.setFormatter(logging.Formatter(log_format))
    root_logger.addHandler(console_handler)
    
    # 성능 전용 로거 설정 - 중복 로그 방지를 위한 추가 확인
    perf_logger = logging.getLogger("performance")
    # 기존 핸들러 모두 제거
    for handler in perf_logger.handlers[:]:
        perf_logger.removeHandler(handler)
    
    perf_logger.propagate = False  # 루트 로거로 전파하지 않음
    perf_logger.setLevel(logging.INFO)
    
    # 성능 로거 핸들러 추가
    perf_handler = logging.StreamHandler(sys.stdout)
    perf_handler.setFormatter(logging.Formatter(log_format))
    perf_logger.addHandler(perf_handler)
    
    # 파일 로깅 설정 (폴더 생성 후)
    try:
        os.makedirs("logs", exist_ok=True)
        
        # 일반 로그 파일 핸들러
        file_handler = RotatingFileHandler(
            "logs/server.log", 
            maxBytes=10*1024*1024,  # 10MB
            backupCount=5
        )
        file_handler.setFormatter(logging.Formatter(log_format))
        root_logger.addHandler(file_handler)
        
        # 성능 전용 로그 파일
        perf_file_handler = RotatingFileHandler(
            "logs/performance.log", 
            maxBytes=10*1024*1024,  # 10MB
            backupCount=5
        )
        perf_file_handler.setFormatter(logging.Formatter(log_format))
        perf_logger.addHandler(perf_file_handler)
        
    except (FileNotFoundError, PermissionError) as e:
        logging.warning(f"로그 파일을 생성할 수 없습니다: {e}. 콘솔 로깅만 활성화됩니다.")
    
    # Ray 로거와 vLLM 로거 제어
    ray_logger = logging.getLogger("ray")
    ray_logger.setLevel(logging.WARNING)  # Ray 로그 축소
    
    vllm_logger = logging.getLogger("vllm")
    vllm_logger.setLevel(logging.INFO)   # vLLM 로그는 INFO 유지
    
    # 스레드 이름으로 로깅되는 것 방지
    thread_logger = logging.getLogger("Thread")
    thread_logger.setLevel(logging.WARNING)  
    
    # 로깅 설정 확인
    logging.info("로깅 시스템 초기화 완료 - 중복 로그 방지")

# 로깅 설정 적용
setup_logging()

import ray
from ray import serve
import uuid
import asyncio
import time

# Configuration
import yaml
from box import Box
with open("./config.yaml", "r") as f:
    config_yaml = yaml.load(f, Loader=yaml.FullLoader)
    config = Box(config_yaml)
random_seed(config.seed)

########## Ray Dashboard 8265 port ##########
init_ray()  # Initialize the Ray
sse_manager = SSEQueueManager.options(name="SSEQueueManager").remote()
serve.start(detached=True)

#### Ray-Actor 다중 ####
inference_service = InferenceService.options(num_replicas=config.ray.actor_count).bind(config)
serve.run(inference_service)
inference_handle = serve.get_deployment_handle("inference", app_name="default")

#### Ray-Actor 단독 ####
# inference_actor = InferenceActor.options(num_cpus=config.ray.num_cpus, num_gpus=config.ray.num_gpus).remote(config)

########## FLASK APP setting ##########
app = Flask(__name__)
content_type = "application/json; charset=utf-8"

# 기본 페이지를 불러오는 라우트
@app.route("/")
def index():
    return render_template("index.html")  # index.html을 렌더링

# Test 페이지를 불러오는 라우트
@app.route("/test")
def test_page():
    return render_template("index_test.html")

# chatroomPage 페이지를 불러오는 라우트
@app.route("/chat")
def chat_page():
    return render_template("chatroom.html")

# data 관리
from data_control.data_control import data_control_bp
app.register_blueprint(data_control_bp, url_prefix="/data")

# --------------------- Non Streaming part ----------------------------
@app.route("/query", methods=["POST"])
async def query():
    try:
        # Log when the query is received
        receive_time = datetime.now().isoformat()
        print(f"[APP] Received /query request at {receive_time}")
        
        # Optionally, attach the client time if desired:
        http_query = request.json  # 클라이언트로부터 JSON 요청 수신
        
        http_query["server_receive_time"] = receive_time
        
        # Ray Serve 배포된 서비스를 통해 추론 요청 (자동으로 로드밸런싱됨)
        # result = await inference_actor.process_query.remote(http_query) # 단일
        result = await inference_handle.query.remote(http_query) # 다중
        if isinstance(result, dict):
            result = json.dumps(result, ensure_ascii=False)
        print("APP.py - 결과: ", result)
        return Response(result, content_type=content_type)
    except Exception as e:
        error_resp = error_format(f"서버 처리 중 오류 발생: {str(e)}", 500)
        return Response(error_resp, content_type=content_type)

# --------------------- Streaming part ----------------------------
@app.route("/query_stream", methods=["POST"])
def query_stream():
    """
    POST 방식 SSE 스트리밍 엔드포인트.
    클라이언트가 아래 필드들을 포함한 JSON을 보내면:
      - qry_id, user_id, page_id, auth_class, qry_contents, qry_time
    auth_class는 내부적으로 'admin'으로 통일합니다.
    """
    body = request.json or {}
    # 새로운 필드 추출
    qry_id = body.get("qry_id")
    user_id = body.get("user_id")
    page_id = body.get("page_id")
    auth_class = "admin"  # 어떤 값이 와도 'admin'으로 통일
    qry_contents = body.get("qry_contents", "")
    qry_time = body.get("qry_time")  # 클라이언트 측 타임스탬프
    image_data = body.get("image_data") # Image Data
    # RAG 여부
    use_rag = body.get("rag")
    print(f"[DEBUG] RAG using = ", use_rag)
    
    print(f"[DEBUG] /query_stream called with qry_id='{qry_id}', user_id='{user_id}', page_id='{page_id}', qry_contents='{qry_contents}', qry_time='{qry_time}'")
    
    
    # 새로운 http_query 생성 – 내부 로직에서는 page_id를 채팅방 id로 사용
    http_query = {
        "qry_id": qry_id,
        "user_id": user_id,
        "page_id": page_id if page_id else str(uuid.uuid4()),
        "auth_class": auth_class,
        "qry_contents": qry_contents,
        "qry_time": qry_time,
        "use_rag" : use_rag,
    }
    
    # image_data가 존재하면 http_query에 추가하고, 길이(또는 타입)만 간략하게 출력
    if image_data is not None:
        http_query["image_data"] = image_data
        # image_data가 문자열이나 시퀀스 타입이면 길이를, 아니면 타입을 출력합니다.
        if hasattr(image_data, "__len__"):
            print(f"[DEBUG] image_data received: length={len(image_data)}")
        else:
            print(f"[DEBUG] image_data received: type={type(image_data)}")

    # http_query 전체를 출력할 때 image_data 내용은 생략(요약 정보만 출력)
    http_query_print = http_query.copy()
    if "image_data" in http_query_print:
        http_query_print["image_data"] = "<omitted>"
    print(f"[DEBUG] Built http_query: {http_query_print}")
    
    # Ray Serve를 통한 streaming 호출 (변경 없음, 내부 인자는 수정된 http_query)
    response = inference_handle.process_query_stream.remote(http_query)
    obj_ref = response._to_object_ref_sync()
    chat_id = ray.get(obj_ref)  # chat_id는 page_id
    print(f"[DEBUG] streaming chat_id={chat_id}")
    
    def sse_generator():
        try:
            while True:
                # SSEQueueManager에서 토큰을 가져옴 (chat_id 사용)
                token = ray.get(sse_manager.get_token.remote(chat_id, 120))
                if token is None or token == "[[STREAM_DONE]]":
                    break
                yield f"data: {token}\n\n"
        except Exception as e:
            error_token = json.dumps({"type": "error", "message": str(e)})
            yield f"data: {error_token}\n\n"
        finally:
            try:
                obj_ref = inference_handle.close_sse_queue.remote(chat_id)._to_object_ref_sync()
                ray.get(obj_ref)
            except Exception as ex:
                print(f"[DEBUG] Error closing SSE queue for {chat_id}: {str(ex)}")
            print("[DEBUG] SSE closed.")

    return Response(sse_generator(), mimetype="text/event-stream")

# --------------------- CLT Streaming part ----------------------------
@app.route("/queryToSLLM", methods=["POST"])
def query_stream_to_clt():
    """
    POST 방식 SSE 스트리밍 엔드포인트.
    클라이언트가 {"qry_id": "...", "user_id": "...", "page_id": "...", "qry_contents": "...", "qry_time": "..." }
    형태의 JSON을 보내면, 내부 Ray Serve SSE 스트림을 통해 처리한 후 지정된 response_url로 SSE 청크를 전송합니다.
    """
    # POST 요청 파라미터 파싱
    body = request.json or {}
    qry_id = body.get("qry_id", "")
    user_id = body.get("user_id", "")
    page_id = body.get("page_id", "")
    auth_class = "admin"  # 모든 요청을 'admin'으로 처리
    user_input = body.get("qry_contents", "")
    qry_time = body.get("qry_time", "")
    
    response_url = config.response_url

    print(f"[DEBUG] /queryToSLLM called with qry_id='{qry_id}', user_id='{user_id}', "
          f"page_id='{page_id}', qry_contents='{user_input}', qry_time='{qry_time}', url={response_url}")
    
    # 내부 로직에서는 page_id를 채팅방 ID(또는 request_id)로 사용합니다.
    http_query = {
        "qry_id": qry_id,
        "user_id": user_id,
        "page_id": page_id if page_id else str(uuid.uuid4()),
        "auth_class": auth_class,
        "qry_contents": user_input,
        "qry_time": qry_time,
        "response_url": response_url
    }
    print(f"[DEBUG] Built http_query={http_query}")

    # Ray Serve에 SSE 스트리밍 요청 보내기
    response = inference_handle.process_query_stream.remote(http_query)
    obj_ref = response._to_object_ref_sync()
    request_id = ray.get(obj_ref)
    print(f"[DEBUG] streaming request_id={request_id}")
    
    def sse_generator(request_id, response_url):
        token_buffer = []  # To collect tokens (for answer tokens only)
        last_sent_time = time.time()  # To track the last time data was sent
        answer_counter = 1  # 답변 업데이트 순번
        try:
            while True:
                token = ray.get(sse_manager.get_token.remote(request_id, 120))
                if token is None:
                    print("[DEBUG] 토큰이 None 반환됨. 종료합니다.")
                    break

                if isinstance(token, str):
                    token = token.strip()
                if token == "[[STREAM_DONE]]":
                    print("[DEBUG] 종료 토큰([[STREAM_DONE]]) 수신됨. 스트림 종료.")
                    break

                try:
                    token_dict = json.loads(token) if isinstance(token, str) else token
                except Exception as e:
                    print(f"[ERROR] JSON 파싱 실패: {e}. 원시 토큰: '{token}'")
                    continue

                # If token is a reference token, send it immediately
                if token_dict.get("type") == "reference":
                    print(f"[DEBUG] Reference token details: {token_dict}")
                    ref_format = process_format_to_response([token_dict], qry_id, continue_="C", update_index=answer_counter)
                    print(f"[DEBUG] Sending reference data: {json.dumps(ref_format, ensure_ascii=False, indent=2)}")
                    send_data_to_server(ref_format, response_url)
                    continue

                # Otherwise, accumulate answer tokens
                token_buffer.append(token_dict)
                current_time = time.time()
                # If 1 second has passed, flush the accumulated answer tokens
                if current_time - last_sent_time >= 1:
                    if len(token_buffer) > 0:
                        # Check if any token in the buffer signals termination.
                        final_continue = "E" if any(t.get("continue") == "E" for t in token_buffer) else "C"
                        print(f"[DEBUG] Flushing {len(token_buffer)} tokens with continue flag: {final_continue}")
                        buffer_format = process_format_to_response(token_buffer, qry_id, continue_=final_continue, update_index=answer_counter)
                        send_data_to_server(buffer_format, response_url)
                        token_buffer = []  # Reset the buffer
                        last_sent_time = current_time  # Update the last sent time
                        answer_counter += 1
                if token_dict.get("continue") == "E":
                    # Immediately flush the buffer with termination flag if needed
                    if len(token_buffer) > 0:
                        print(f"[DEBUG] Immediate flush due to termination flag in buffer (size {len(token_buffer)}).")
                        buffer_format = process_format_to_response(token_buffer, qry_id, continue_="E", update_index=answer_counter)
                        send_data_to_server(buffer_format, response_url)
                        token_buffer = []
                    break
            # After loop: if tokens remain, flush them with termination flag
            if len(token_buffer) > 0:
                print(f"[DEBUG] Final flush of remaining {len(token_buffer)} tokens with end flag.")
                buffer_format = process_format_to_response(token_buffer, qry_id, continue_="E", update_index=answer_counter)
                send_data_to_server(buffer_format, response_url)
        except Exception as e:
            print(f"[ERROR] sse_generator encountered an error: {e}")
        finally:
            try:
                obj_ref = inference_handle.close_sse_queue.remote(request_id)._to_object_ref_sync()
                ray.get(obj_ref)
            except Exception as ex:
                print(f"[DEBUG] Error closing SSE queue for {request_id}: {str(ex)}")
            print("[DEBUG] SSE closed.")

    
    # 별도의 스레드에서 SSE generator 실행
    job = threading.Thread(target=sse_generator, args=(request_id, response_url), daemon=False)
    job.start()

    # 클라이언트에는 즉시 "수신양호" 메시지를 JSON 형식으로 응답
    return Response(error_format("수신양호", 200, qry_id), content_type="application/json")


# --------------------- History & Reference part ----------------------------

# 새로 추가1: request_id로 대화 기록을 조회하는 API 엔드포인트
@app.route("/history", methods=["GET"])
def conversation_history():
    request_id = request.args.get("request_id", "")
    last_index = request.args.get("last_index")
    if not request_id:
        error_resp = error_format("request_id 파라미터가 필요합니다.", 400)
        return Response(error_resp, content_type="application/json; charset=utf-8")
    
    try:
        last_index = int(last_index) if last_index is not None else None
        response = inference_handle.get_history.remote(request_id, last_index=last_index)
        # DeploymentResponse를 ObjectRef로 변환
        obj_ref = response._to_object_ref_sync()
        history_data = ray.get(obj_ref)
        return jsonify(history_data)
    except Exception as e:
        print(f"[ERROR /history] {e}")
        error_resp = error_format(f"대화 기록 조회 오류: {str(e)}", 500)
        return Response(error_resp, content_type="application/json; charset=utf-8")

# 새로 추가2: request_id로 해당 답변의 참고자료를 볼 수 있는 API
@app.route("/reference", methods=["GET"])
def get_reference():
    request_id = request.args.get("request_id", "")
    msg_index_str = request.args.get("msg_index", "")
    if not request_id or not msg_index_str:
        error_resp = error_format("request_id와 msg_index 파라미터가 필요합니다.", 400)
        return Response(error_resp, content_type="application/json; charset=utf-8")
    
    try:
        msg_index = int(msg_index_str)
        # 먼저 history를 가져옴
        response = inference_handle.get_history.remote(request_id)
        obj_ref = response._to_object_ref_sync()
        history_data = ray.get(obj_ref)
        
        history_list = history_data.get("history", [])
        if msg_index < 0 or msg_index >= len(history_list):
            return jsonify({"error": "유효하지 않은 메시지 인덱스"}), 400
        
        message = history_list[msg_index]
        if message.get("role") != "ai":
            return jsonify({"error": "해당 메시지는 AI 응답이 아닙니다."}), 400
        
        chunk_ids = message.get("references", [])
        if not chunk_ids:
            return jsonify({"references": []})
        
        # chunk_ids에 해당하는 실제 참조 데이터 조회
        ref_response = inference_handle.get_reference_data.remote(chunk_ids)
        ref_obj_ref = ref_response._to_object_ref_sync()
        references = ray.get(ref_obj_ref)
        return jsonify({"references": references})
    except Exception as e:
        print(f"[ERROR /reference] {e}")
        error_resp = error_format(f"참조 조회 오류: {str(e)}", 500)
        return Response(error_resp, content_type="application/json; charset=utf-8")

# Flask app 실행
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=False)

```


--- ray_deploy/ray_setup.py

```python

# ray_deploy/ray_setup.py
import ray
from ray import serve

########## Starting Banner ############
from colorama import init, Fore, Style
init(autoreset=True)

BANNER = Fore.GREEN + r"""
'########:'##:::::::'##::::'##:'##::::'##::::::::::'##::: ##::'######::
 ##.....:: ##::::::: ##:::: ##:. ##::'##::::::::::: ###:: ##:'##... ##:
 ##::::::: ##::::::: ##:::: ##::. ##'##:::::::::::: ####: ##: ##:::..::
 ######::: ##::::::: ##:::: ##:::. ###::::::::::::: ## ## ##:. ######::
 ##...:::: ##::::::: ##:::: ##::: ## ##:::::::::::: ##. ####::..... ##:
 ##::::::: ##::::::: ##:::: ##:: ##:. ##::::::::::: ##:. ###:'##::: ##:
 ##::::::: ########:. #######:: ##:::. ##:'#######: ##::. ##:. ######::
..::::::::........:::.......:::..:::::..::.......::..::::..:::......:::
"""

def init_ray():
    print(BANNER)
    # Ray-Dashboard - GPU 상태, 사용 통계 등을 제공하는 모니터링 툴, host 0.0.0.0로 외부 접속을 허용하고, Default 포트인 8265으로 설정
    ray.init(
        include_dashboard=True,
        dashboard_host="0.0.0.0" # External IP accessable
        # dashboard_port=8265
    )
    print("Ray initialized. DashBoard running at http://192.222.54.254:8265") # New Server(2xH100)

```


--- ray_deploy/ray_utils.py

```python

# ray_deploy/ray_utils.py
import ray  # Ray library
from ray import serve
import json
import asyncio  # async I/O process module
from concurrent.futures import ProcessPoolExecutor  # 스레드 컨트롤
import uuid
import time
from typing import Dict, Optional, List, Any, Tuple, Union
import threading  # To find out the usage of thread
import datetime
import logging

from core import (
    query_sort,
    specific_question,
    execute_rag,
    generate_answer,
    generate_answer_stream,
    image_query,
)
from utils import (
    load_model,
    load_data,
    process_format_to_response,
    process_to_format,
    error_format,
)
from utils.summarizer import summarize_conversation
# Langchain Memory system
from ray_deploy.langchain import CustomConversationBufferMemory, serialize_message

# 로깅 설정
logging.basicConfig(level=logging.INFO, 
                    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s')
logger = logging.getLogger('RAY_UTILS')

# Configuration
import yaml
from box import Box
with open("./config.yaml", "r") as f:
    config_yaml = yaml.load(f, Loader=yaml.FullLoader)
    config = Box(config_yaml)

@ray.remote  # From Decorator, Each Actor is allocated 1 GPU
class InferenceActor:
    def __init__(self, config):
        """
        InferenceActor 초기화: 모델, 토크나이저, 데이터 로드 및 배치 처리 설정
        """
        self.config = config
        
        # 성능 모니터 초기화 (Ray 액터 내부에서)
        from utils.debug_tracking import get_performance_monitor, print_memory_usage
        self.performance_monitor = get_performance_monitor()
        
        # 초기 메모리 사용량 로깅
        print_memory_usage("InferenceActor 초기화 시작")
        
        # 액터 내부에서 모델 및 토크나이저를 새로 로드 (GPU에 한 번만 로드)
        self.model, self.tokenizer, self.embed_model, self.embed_tokenizer = load_model(
            config
        )
        # 데이터는 캐시 파일을 통해 로드
        self.data = load_data(config.data_path)
        
        # 비동기 큐와 배치 처리 설정
        self.request_queue = asyncio.Queue()
        self.max_batch_size = config.ray.max_batch_size  # 최대 배치 수
        self.batch_wait_timeout = config.ray.batch_wait_timeout  # 배치당 처리 시간 - 이제 사용하지 아니함(연속 배치이기에 배치당 처리 시간이 무의미)

        # Actor 내부에서 ProcessPoolExecutor 생성 (직렬화 문제 회피)
        max_workers = int(min(config.ray.num_cpus * 0.8, (26*config.ray.actor_count)-4))
        self.process_pool = ProcessPoolExecutor(max_workers)

        # --- SSE Queue Manager --- 
        # Key = request_id, Value = an asyncio.Queue of partial token strings
        # A dictionary to store SSE queues for streaming requests
        self.queue_manager = ray.get_actor("SSEQueueManager")
        self.active_sse_queues: Dict[str, asyncio.Queue] = {}

        self.batch_counter = 0  # New counter to track batches

        self.memory_map = {}
        
        # 활성 작업 추적을 위한 변수 추가
        self.active_tasks = set()
        self.max_concurrent_tasks = config.ray.max_batch_size
        
        # 연속 배치 처리기 시작 (Continuous batch)
        asyncio.create_task(self.continuous_batch_processor())
        
    # -------------------------------------------------------------------------
    # GET MEMORY FOR SESSION - 세션별 메모리 관리
    # -------------------------------------------------------------------------
    def get_memory_for_session(self, request_id: str) -> CustomConversationBufferMemory:
        """
        세션별 Memory를 안전하게 가져오는 헬퍼 메서드.
        만약 memory_map에 request_id가 없으면 새로 생성해 저장 후 반환.
        
        Args:
            request_id: 세션(대화) ID
            
        Returns:
            CustomConversationBufferMemory: 대화 기록을 저장하는 메모리 객체
        """
        if request_id not in self.memory_map:
            logger.info(f"Creating new ConversationBufferMemory for session={request_id}")
            self.memory_map[request_id] = CustomConversationBufferMemory(return_messages=True)
        return self.memory_map[request_id]

    # -------------------------------------------------------------------------
    # CONTINUOUOS BATCH PROCESSOR - 연속 배치 처리
    # -------------------------------------------------------------------------
    async def continuous_batch_processor(self):
        """
        연속적인 배치 처리 - 최대 max_batch_size개의 요청이 항상 동시에 처리되도록 함
        """
        while True:
            # 사용 가능한 슬롯 확인
            available_slots = self.max_concurrent_tasks - len(self.active_tasks)
            
            if available_slots > 0:
                try:
                    # 새 요청 받기 (짧은 타임아웃으로 non-blocking 유지)
                    request_tuple = await asyncio.wait_for(
                        self.request_queue.get(), 
                        timeout=0.01
                    )
                    
                    # 비동기 처리 작업 생성
                    request_obj, fut, sse_queue = request_tuple
                    task = asyncio.create_task(
                        self._process_single_query(request_obj, fut, sse_queue)
                    )
                    
                    # 작업 완료 시 활성 목록에서 제거하는 콜백 설정
                    task.add_done_callback(
                        lambda t, task_ref=task: self.active_tasks.discard(task_ref)
                    )
                    
                    # 활성 작업 목록에 추가
                    self.active_tasks.add(task)
                    
                    logger.info(f"[Continuous Batching] +1 request => active tasks now {len(self.active_tasks)}/{self.max_concurrent_tasks}")
                
                except asyncio.TimeoutError:
                    # 새 요청이 없으면 잠시 대기
                    await asyncio.sleep(0.01)
            else:
                # 모든 슬롯이 사용 중이면 작업 완료될 때까지 대기
                if self.active_tasks:
                    # 작업 중 하나라도 완료될 때까지 대기
                    done, pending = await asyncio.wait(
                        self.active_tasks, 
                        return_when=asyncio.FIRST_COMPLETED
                    )
                    
                    # 완료된 작업 확인
                    for task in done:
                        try:
                            await task
                        except Exception as e:
                            logger.error(f"Task failed: {e}")
                    
                    logger.info(f"[Continuous Batching] Tasks completed: {len(done)} => active tasks now {len(self.active_tasks)}/{self.max_concurrent_tasks}")
                else:
                    await asyncio.sleep(0.01)

    # -------------------------------------------------------------------------
    # 유틸리티 함수들: 코드 중복 제거 및 재사용성 향상
    # -------------------------------------------------------------------------
    def _get_request_info(self, http_query_or_stream_dict: dict) -> Tuple[dict, str, bool]:
        """
        요청 정보를 추출하는 유틸리티 함수
        
        Args:
            http_query_or_stream_dict: 원본 요청 데이터
            
        Returns:
            Tuple[dict, str, bool]: (http_query, request_id, is_streaming) 형태의 튜플
        """
        # 스트리밍 여부 및 request_id 확인
        if isinstance(http_query_or_stream_dict, dict) and "request_id" in http_query_or_stream_dict:
            request_id = http_query_or_stream_dict["request_id"]
            http_query = http_query_or_stream_dict["http_query"]
            is_streaming = True
            logger.info(f"[STREAM] request_id={request_id}")
        else:
            request_id = None
            http_query = http_query_or_stream_dict
            is_streaming = False
            logger.info("[NORMAL] Non-streaming request")
            
        return http_query, request_id, is_streaming
    
    async def _load_conversation_history(self, page_id: str) -> str:
        """
        대화 기록 로드하는 유틸리티 함수
        
        Args:
            page_id: 세션(대화) ID
            
        Returns:
            str: 포맷팅된 대화 이력 텍스트
        """
        memory = self.get_memory_for_session(page_id)
        
        try:
            past_context = memory.load_memory_variables({}).get("history", [])
            # history가 리스트 형식인 경우 (각 메시지가 별도 항목으로 저장되어 있다면)
            if isinstance(past_context, list):
                recent_messages = [msg if isinstance(msg, str) else msg.content for msg in past_context[-5:]]
                return "\n\n".join(recent_messages)
            else:
                # 문자열인 경우, 메시지 구분자를 "\n\n"으로 가정하여 분리
                messages = str(past_context).split("\n\n")
                recent_messages = messages[-5:]
                return "\n\n".join(recent_messages)
        except Exception as e:
            logger.error(f"대화 기록 로드 중 오류: {e}")
            return ""
        
    async def _process_image_if_exists(self, http_query: dict) -> dict:
        """
        이미지 데이터가 있는 경우 이미지 처리 수행
        
        Args:
            http_query: 요청 데이터
            
        Returns:
            dict: 이미지 설명 정보
        """
        image_data = http_query.get("image_data")
        if image_data is not None:
            logger.info("이미지 데이터 감지, 이미지 처리 시작")
            return await image_query(http_query, self.model, self.config)
        return {"is_structured": False, "description": "이미지는 입력되지 않았습니다."}
    
    async def _store_conversation(self, memory: CustomConversationBufferMemory, 
                                user_input: str, output: str, 
                                chunk_ids: List[str], http_query: dict) -> None:
        """
        대화 내용을 메모리에 저장하는 유틸리티 함수
        
        Args:
            memory: 대화 메모리 객체
            user_input: 사용자 입력
            output: AI 응답
            chunk_ids: 참조된 청크 ID 목록
            http_query: 원본 요청 데이터
        """
        try:
            memory.save_context(
                {
                    "qry_contents": user_input,
                    "qry_id": http_query.get("qry_id", ""),
                    "user_id": http_query.get("user_id", ""),
                    "auth_class": http_query.get("auth_class", ""),
                    "qry_time": http_query.get("qry_time", "")
                },
                {
                    "output": output,
                    "chunk_ids": chunk_ids
                }
            )
            logger.debug(f"대화 저장 완료: 청크 ID {chunk_ids}")
        except Exception as e:
            logger.error(f"대화 저장 중 오류: {e}")
    
    def _extract_chunk_ids(self, retrieval: dict) -> List[str]:
        """
        검색 결과에서 청크 ID 추출하는 유틸리티 함수
        
        Args:
            retrieval: 검색 결과 데이터
            
        Returns:
            List[str]: 청크 ID 목록
        """
        chunk_ids = []
        for doc in retrieval.get("rsp_data", []):
            if "chunk_id" in doc:
                chunk_ids.append(doc["chunk_id"])
        return chunk_ids
    
    async def _handle_error(self, e: Exception, request_id: str, future: asyncio.Future) -> None:
        """
        오류 처리를 위한 유틸리티 함수
        
        Args:
            e: 발생한 예외
            request_id: 요청 ID
            future: 결과를 설정할 Future 객체
        """
        err_msg = f"처리 중 오류 발생: {str(e)}"
        logger.error(err_msg)
        
        # 스트리밍인 경우 에러 토큰 전송
        if request_id:
            try:
                error_token = json.dumps({"type": "error", "message": err_msg}, ensure_ascii=False)
                await self.queue_manager.put_token.remote(request_id, error_token)
                await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
                await self.close_sse_queue(request_id)
            except Exception as e2:
                logger.error(f"SSE 에러 전송 중 추가 오류: {str(e2)}")
                
        # Future에 에러 결과 설정
        future.set_result(error_format(err_msg, 500))
    
    # -------------------------------------------------------------------------
    # 메인 처리 함수: RAG 활용 쿼리 처리
    # -------------------------------------------------------------------------
    async def _process_rag_query(self, http_query: dict, user_input: str, 
                               image_description: dict, request_id: str, 
                               is_streaming: bool, future: asyncio.Future) -> None:
        """
        RAG를 사용하는 쿼리 처리 로직
        
        Args:
            http_query: 요청 데이터
            user_input: 사용자 입력
            image_description: 이미지 설명 데이터
            request_id: 요청 ID
            is_streaming: 스트리밍 여부
            future: 결과를 설정할 Future 객체
        """
        try:
            # 데이터 다시 로드 (항상 최신 데이터 사용)
            self.data = load_data(self.config.data_path)
            
            # 질문 분류 및 RAG 파라미터 준비
            params = {
                "user_input": f"사용자 질문: {user_input} [이미지 설명: {image_description.get('description')}]",
                "model": self.model,
                "tokenizer": self.tokenizer,
                "embed_model": self.embed_model,
                "embed_tokenizer": self.embed_tokenizer,
                "data": self.data,
                "config": self.config,
            }
            
            # 질문 구체화
            QU, KE, TA, TI = await query_sort(params)
            logger.info(f"질문 구체화 결과: QU={QU}, KE={KE}, TA={TA}, TI={TI}")
            
            # 테이블 데이터가 필요한 경우 (SQL 기반 RAG)
            if TA == "yes":
                await self._process_table_query(QU, KE, TA, TI, user_input, request_id, is_streaming, future, http_query)
            else:
                # 일반 RAG 처리
                await self._process_standard_rag(QU, KE, TA, TI, user_input, request_id, is_streaming, future, http_query)
                
        except Exception as e:
            await self._handle_error(e, request_id, future)
    
    # -------------------------------------------------------------------------
    # SQL 테이블 기반 처리 함수
    # -------------------------------------------------------------------------
    async def _process_table_query(self, QU: str, KE: str, TA: str, TI: str,
                                 user_input: str, request_id: str,
                                 is_streaming: bool, future: asyncio.Future, 
                                 http_query: dict) -> None:
        """
        테이블 데이터가 필요한 쿼리 처리 (SQL 기반)
        
        Args:
            QU, KE, TA, TI: 질문 분류 결과
            user_input: 사용자 입력
            request_id: 요청 ID
            is_streaming: 스트리밍 여부
            future: 결과를 설정할 Future 객체
            http_query: 원본 요청 데이터
        """
        try:
            # SQL 실행 및 데이터 추출
            docs, docs_list = await execute_rag(
                QU, KE, TA, TI,
                model=self.model,
                tokenizer=self.tokenizer,
                embed_model=self.embed_model,
                embed_tokenizer=self.embed_tokenizer,
                data=self.data,
                config=self.config,
            )
            
            # 포맷 변환
            try:
                retrieval, chart = process_to_format(docs_list, type="SQL")
            except Exception as e:
                logger.error(f"SQL 결과 포맷 변환 실패: {str(e)}")
                retrieval, chart = [], None
                
            # 스트리밍 또는 일반 응답 처리
            if is_streaming:
                await self._stream_partial_answer(QU, docs, retrieval, chart, request_id, future, user_input, http_query)
            else:
                # 일반 최종 결과 생성
                output = await generate_answer(QU, docs, model=self.model, tokenizer=self.tokenizer, config=self.config)
                answer = process_to_format([output, chart], type="Answer")
                final_data = [retrieval, answer]
                outputs = process_format_to_response(final_data, qry_id=None, continue_="C")
                
                # 청크 ID 추출 및 대화 저장
                chunk_ids = self._extract_chunk_ids(retrieval)
                memory = self.get_memory_for_session(http_query.get("page_id", request_id))
                await self._store_conversation(memory, user_input, output, chunk_ids, http_query)
                
                # 최종 결과 반환
                future.set_result(outputs)
                
        except Exception as e:
            await self._handle_error(e, request_id, future)
            
    # -------------------------------------------------------------------------
    # 표준 RAG 처리 함수
    # -------------------------------------------------------------------------
    async def _process_standard_rag(self, QU: str, KE: str, TA: str, TI: str,
                                  user_input: str, request_id: str,
                                  is_streaming: bool, future: asyncio.Future, 
                                  http_query: dict) -> None:
        """
        일반 RAG 처리 로직
        
        Args:
            QU, KE, TA, TI: 질문 분류 결과
            user_input: 사용자 입력
            request_id: 요청 ID
            is_streaming: 스트리밍 여부
            future: 결과를 설정할 Future 객체
            http_query: 원본 요청 데이터
        """
        try:
            # 대화 이력 기반 질문 구체화
            QU, KE, TA, TI = await specific_question({
                "user_input": user_input,
                "model": self.model,
                "tokenizer": self.tokenizer,
                "embed_model": self.embed_model,
                "embed_tokenizer": self.embed_tokenizer,
                "data": self.data,
                "config": self.config,
            })
            
            # RAG 실행
            docs, docs_list = await execute_rag(
                QU, KE, TA, TI,
                model=self.model,
                tokenizer=self.tokenizer,
                embed_model=self.embed_model,
                embed_tokenizer=self.embed_tokenizer,
                data=self.data,
                config=self.config,
            )
            
            # 검색 결과 포맷 변환
            retrieval = process_to_format(docs_list, type="Retrieval")
            
            # 스트리밍 또는 일반 응답 처리
            if is_streaming:
                await self._stream_partial_answer(QU, docs, retrieval, None, request_id, future, user_input, http_query)
            else:
                # 최종 답변 생성
                output = await generate_answer(QU, docs, model=self.model, tokenizer=self.tokenizer, config=self.config)
                answer = process_to_format([output], type="Answer")
                final_data = [retrieval, answer]
                outputs = process_format_to_response(final_data, qry_id=None, continue_="C")
                
                # 청크 ID 추출 및 대화 저장
                chunk_ids = self._extract_chunk_ids(retrieval)
                memory = self.get_memory_for_session(http_query.get("page_id", request_id))
                await self._store_conversation(memory, user_input, output, chunk_ids, http_query)
                
                # 최종 결과 반환
                future.set_result(outputs)
                
        except Exception as e:
            await self._handle_error(e, request_id, future)
            
    # -------------------------------------------------------------------------
    # RAG 없이 직접 응답 생성 함수
    # -------------------------------------------------------------------------
    async def _process_direct_query(self, user_input: str, request_id: str, 
                                  future: asyncio.Future, http_query: dict) -> None:
        """
        RAG를 사용하지 않고 직접 응답을 생성하는 로직
        
        Args:
            user_input: 사용자 입력
            request_id: 요청 ID
            future: 결과를 설정할 Future 객체
            http_query: 원본 요청 데이터
        """
        try:
            # 이 함수는 RAG를 사용하지 않고 직접 응답을 생성합니다
            docs = None
            retrieval = None
            chart = None
            
            # 스트리밍 방식으로 응답 생성
            await self._stream_partial_answer(user_input, docs, retrieval, chart, request_id, future, user_input, http_query)
        except Exception as e:
            await self._handle_error(e, request_id, future)
    
    # -------------------------------------------------------------------------
    # PROCESS SINGLE QUERY (분기 처리 단순화)
    # -------------------------------------------------------------------------
    async def _process_single_query(self, http_query_or_stream_dict: dict, future: asyncio.Future, sse_queue) -> None:
        """
        단일 쿼리 처리의 메인 함수 (개선된 버전)
        
        Args:
            http_query_or_stream_dict: 원본 요청 데이터
            future: 결과를 설정할 Future 객체
            sse_queue: 스트리밍을 위한 SSE 큐
        """
        # 요청 정보 추출
        http_query, request_id, is_streaming = self._get_request_info(http_query_or_stream_dict)
        
        # 요청 ID가 없으면 생성 (성능 모니터링용)
        if not request_id:
            request_id = str(uuid.uuid4())
        
        # 사용자 입력 추출
        user_input = http_query.get("qry_contents", "")
        logger.info(f"사용자 입력: {user_input}")
        
        # 성능 모니터링 시작 - 요청별 트래킹 시작
        self.performance_monitor.start_request(request_id, input_text=user_input, tokenizer=self.tokenizer)
        self.performance_monitor.update_request(request_id, 0, checkpoint="query_received")
        
        try:
            # RAG 사용 여부 확인
            use_rag = http_query.get("use_rag", True)
            
            if use_rag is False:
                logger.info("RAG 비활성화 모드로 실행")
                self.performance_monitor.update_request(request_id, 0, checkpoint="rag_disabled")
                await self._process_direct_query(user_input, request_id, future, http_query)
            else:
                # 이미지 처리 (있는 경우)
                image_description = await self._process_image_if_exists(http_query)
                self.performance_monitor.update_request(request_id, 0, checkpoint="image_processed")
                
                # RAG 활용 응답 생성
                await self._process_rag_query(http_query, user_input, image_description, 
                                        request_id, is_streaming, future)
        except Exception as e:
            err_msg = f"처리 중 오류 발생: {str(e)}"
            logger.error(err_msg)
            
            # 성능 모니터에 오류 기록
            self.performance_monitor.update_request(request_id, 0, checkpoint=f"error: {str(e)[:50]}")
            self.performance_monitor.complete_request(request_id, success=False)
            
            # 스트리밍인 경우 에러 토큰 전송
            if is_streaming:
                try:
                    error_token = json.dumps({"type": "error", "message": err_msg}, ensure_ascii=False)
                    await self.queue_manager.put_token.remote(request_id, error_token)
                    await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
                    await self.close_sse_queue(request_id)
                except Exception as e2:
                    logger.error(f"SSE 에러 전송 중 추가 오류: {str(e2)}")
                    
            # Future에 에러 결과 설정
            future.set_result(error_format(err_msg, 500))
            
        finally:
            # 스트리밍 요청인 경우 정리 작업
            if is_streaming:
                try:
                    await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
                except Exception as ex:
                    logger.error(f"STREAM_DONE 토큰 전송 중 오류: {str(ex)}")
                await self.close_sse_queue(request_id)

    # -------------------------------------------------------------------------
    # STREAMING PARTIAL ANSWER - 스트리밍 응답 처리
    # -------------------------------------------------------------------------
    async def _stream_partial_answer(self, QU: str, docs, retrieval, chart, 
                                request_id: str, future: asyncio.Future, 
                                user_input: str, http_query: dict) -> None:
        """
        스트리밍 방식의 부분 응답 생성 및 전송
        
        Args:
            QU: 구체화된 질문
            docs: RAG 검색 문서 내용
            retrieval: 검색 결과 포맷
            chart: 차트 데이터 (있는 경우)
            request_id: 요청 ID
            future: 결과를 설정할 Future 객체
            user_input: 원본 사용자 입력
            http_query: 원본 요청 데이터
        """
        logger.info(f"스트리밍 응답 시작: request_id={request_id}")
        
        # 성능 측정 초기화 - 시작 시간 기록
        stream_start_time = time.time()
        token_count = 0
        first_token_received = False
        self.performance_monitor.update_request(request_id, 0, checkpoint="stream_start")

        try:
            # 1. 참조 데이터 전송
            if retrieval:
                reference_json = json.dumps({
                    "type": "reference",
                    "status_code": 200,
                    "result": "OK",
                    "detail": "Reference data",
                    "evt_time": datetime.datetime.now().isoformat(),
                    "data_list": [retrieval]
                }, ensure_ascii=False)
                await self.queue_manager.put_token.remote(request_id, reference_json)
                logger.info(f"참조 데이터 전송 완료: request_id={request_id}")
                self.performance_monitor.update_request(request_id, 0, checkpoint="reference_sent")

            
            # 2. 메모리 가져오기
            memory = self.get_memory_for_session(request_id)
            
            # 3. 대화 이력 불러오기
            past_context = await self._load_conversation_history(request_id)
            
            # 4. 최종 프롬프트 구성
            final_query = f"{past_context}\n\n[사용자 질문]\n{QU}"
            
            # 토큰 수 계산
            retrieval_str = str(retrieval) if retrieval else ""
            past_tokens = self.tokenizer.tokenize(str(past_context))
            query_tokens = self.tokenizer.tokenize(str(QU))
            retrieval_tokens = self.tokenizer.tokenize(retrieval_str)
            total_tokens = len(self.tokenizer.tokenize(str(final_query))) + len(retrieval_tokens)
            logger.info(f"토큰 수: 이전 대화={len(past_tokens)}, 검색 자료={len(retrieval_tokens)}, 질문={len(query_tokens)}, 총={total_tokens}")
            
            self.performance_monitor.update_request(
                request_id, 0, 
                checkpoint=f"prompt_ready (total_tokens={total_tokens})"
            )
                
            # 5. 스트리밍 응답 생성 및 전송
            partial_accumulator = ""
            
            # HTTP 쿼리에 request_id 추가하여 전달 - vLLM 토큰 추적용
            http_query.update({"page_id": request_id})
            
            async for partial_text in generate_answer_stream(
                final_query, docs, self.model, self.tokenizer, self.config, http_query
            ):
                new_text = partial_text[len(partial_accumulator):]
                partial_accumulator = partial_text
                
                if new_text == "":
                    continue
                
                # 토큰 수는 이제 StreamingTokenCounter에서 직접 추적되므로 여기서는 간단히 갱신
                current_tokens = len(partial_accumulator.split())
                token_count = current_tokens
                
                # 첫 토큰 도착 확인 (StreamingTokenCounter에서 성능 모니터에 이미 기록했을 수 있음)
                current_time = time.time()
                if not first_token_received and token_count > 0:
                    first_token_latency = current_time - stream_start_time
                    first_token_received = True
                    logger.info(f"첫 토큰 생성: {request_id} (latency: {first_token_latency:.3f}s)")
                
                # 응답 토큰 JSON 형식으로 포장하여 전송
                answer_json = json.dumps({
                    "type": "answer",
                    "answer": new_text
                }, ensure_ascii=False)
                await self.queue_manager.put_token.remote(request_id, answer_json)
            
            # 생성 완료 시간 계산
            generation_time = time.time() - stream_start_time
            tokens_per_second = token_count / generation_time if generation_time > 0 else 0
            
            # 최종 성능 지표 업데이트
            self.performance_monitor.update_request(
                request_id, token_count,
                current_output=partial_accumulator,
                checkpoint=f"generation_complete ({generation_time:.2f}s, {tokens_per_second:.2f} tokens/s, {token_count} tokens)"
            )
            
            # 6. 최종 축적된 텍스트
            final_text = partial_accumulator
            
            # 7. 대화 저장
            chunk_ids = self._extract_chunk_ids(retrieval) if retrieval else []
            await self._store_conversation(memory, user_input, final_text, chunk_ids, http_query)
            
            # 8. 최종 응답 구조 생성
            if chart is not None:
                ans = process_to_format([final_text, chart], type="Answer")
            else:
                ans = process_to_format([final_text], type="Answer")
                
            # 9. 결과 설정 및 스트리밍 종료 신호 전송
            final_res = process_format_to_response([retrieval, ans] if retrieval else [ans], qry_id=http_query.get("qry_id", ""), continue_="E")
            future.set_result(final_res)
            
            # 요청 완료 처리 - 성공
            self.performance_monitor.complete_request(request_id, success=True)
            
            logger.info(f"스트리밍 응답 완료: request_id={request_id}")
        
        except Exception as e:
            err_msg = f"스트리밍 응답 중 오류: {str(e)}"
            logger.error(err_msg)
            future.set_result(error_format(err_msg, 500))
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")

    # -------------------------------------------------------------------------
    # API 인터페이스 메서드들
    # -------------------------------------------------------------------------
    async def process_query(self, http_query: dict):
        """
        일반 쿼리 처리 메서드 - 최종 결과를 한 번에 반환
        
        Args:
            http_query: 요청 데이터
            
        Returns:
            dict: 처리 결과
        """
        loop = asyncio.get_event_loop()
        future = loop.create_future()
        # 스트리밍 아님 - sse_queue는 None
        await self.request_queue.put((http_query, future, None))
        return await future
    
    async def process_query_stream(self, http_query: dict) -> str:
        """
        스트리밍 쿼리 처리 메서드 - SSE 큐를 생성하고 request_id 반환
        
        Args:
            http_query: 요청 데이터
            
        Returns:
            str: 생성된 채팅/요청 ID
        """
        # page_id를 채팅방 id로 사용 (없으면 생성)
        chat_id = http_query.get("page_id")
        if not chat_id:
            chat_id = str(uuid.uuid4())
        http_query["page_id"] = chat_id  # 강제 할당
        
        # SSE 큐 생성
        await self.queue_manager.create_queue.remote(chat_id)
        logger.info(f"스트리밍 처리 시작: chat_id={chat_id}")
        
        # 이미지 데이터 로그 출력 시 내용 생략
        http_query_print = http_query.copy()
        if "image_data" in http_query_print:
            http_query_print["image_data"] = "<omitted>"
        logger.debug(f"요청 데이터: {http_query_print}")

        # 비동기 처리를 위한 Future 생성
        loop = asyncio.get_event_loop()
        final_future = loop.create_future()

        # 처리 요청 큐에 추가
        sse_queue = asyncio.Queue()
        self.active_sse_queues[chat_id] = sse_queue
        
        queued_item = {
            "request_id": chat_id,
            "http_query": http_query,
        }

        await self.request_queue.put((queued_item, final_future, sse_queue))
        logger.info(f"처리 큐에 추가됨: chat_id={chat_id}")

        return chat_id
    
    async def pop_sse_token(self, request_id: str) -> Optional[str]:
        """
        SSE 토큰 꺼내기 - 클라이언트에 전송할 부분 토큰 추출
        
        Args:
            request_id: 요청 ID
            
        Returns:
            Optional[str]: 다음 토큰 (없으면 None)
        """
        if request_id not in self.active_sse_queues:
            logger.warning(f"SSE 큐 없음: request_id={request_id}")
            return None

        queue = self.active_sse_queues[request_id]
        try:
            token = await asyncio.wait_for(queue.get(), timeout=120.0)
            return token
        except asyncio.TimeoutError:
            logger.warning(f"토큰 대기 시간 초과: request_id={request_id}")
            return None
    
    async def close_sse_queue(self, request_id: str):
        """
        SSE 큐 종료 및 정리
        
        Args:
            request_id: 요청 ID
        """
        if request_id in self.active_sse_queues:
            logger.info(f"SSE 큐 종료: request_id={request_id}")
            del self.active_sse_queues[request_id]
        else:
            logger.warning(f"종료할 SSE 큐 없음: request_id={request_id}")
    
    # -------------------------------------------------------------------------
    # 대화 기록 및 참조 관련 메서드들
    # -------------------------------------------------------------------------
    async def get_conversation_history(self, request_id: str, last_index: int = None) -> dict:
        """
        요청 ID에 해당하는 대화 기록 조회
        
        Args:
            request_id: 요청 ID
            last_index: 마지막으로 받은 메시지 인덱스 (이후 메시지만 반환)
            
        Returns:
            dict: 직렬화된 대화 기록
        """
        try:
            if request_id in self.memory_map:
                memory = self.memory_map[request_id]
                history_obj = memory.load_memory_variables({})
                if "history" in history_obj and isinstance(history_obj["history"], list):
                    # 직렬화
                    serialized = [serialize_message(msg) for msg in history_obj["history"]]
                    logger.info(f"대화 기록 반환: 메시지 {len(serialized)}개")
                    
                    # last_index 이후의 메시지만 반환
                    if last_index is not None and isinstance(last_index, int):
                        serialized = serialized[last_index+1:]
                        
                    return {"history": serialized}
                else:
                    logger.warning("대화 기록이 리스트 형식이 아님")
                    return {"history": []}
            else:
                logger.info(f"해당 ID의 대화 기록 없음: {request_id}")
                return {"history": []}
        except Exception as e:
            logger.error(f"대화 기록 조회 오류: {e}")
            return {"history": []}
        
    async def get_reference_data(self, chunk_ids: list) -> list:
        """
        청크 ID 목록에 해당하는 참조 데이터 조회
        
        Args:
            chunk_ids: 청크 ID 목록
            
        Returns:
            list: 참조 데이터 목록
        """
        try:
            result = []
            data = self.data
            
            # 주어진 청크 ID에 해당하는 데이터 찾기
            for cid in chunk_ids:
                if cid in data["chunk_ids"]:
                    idx = data["chunk_ids"].index(cid)
                    record = {
                        "file_name": data["file_names"][idx],
                        "title": data["titles"][idx],
                        "text": data["texts_vis"][idx],
                        "date": str(data["times"][idx])
                    }
                    result.append(record)
            
            logger.info(f"참조 데이터 조회: {len(result)}/{len(chunk_ids)} 항목 찾음")
            return result
        except Exception as e:
            logger.error(f"참조 데이터 조회 오류: {e}")
            return []


# -------------------------------------------------------------------------
# Ray Serve 배포를 위한 서비스 클래스
# -------------------------------------------------------------------------
@serve.deployment(name="inference", max_ongoing_requests=100)
class InferenceService:
    """
    Ray Serve를 통한 배포를 위한 서비스 클래스
    각 요청을 InferenceActor로 전달하고 결과를 반환함
    """
    def __init__(self, config):
        """
        InferenceService 초기화
        
        Args:
            config: 설정 객체
        """
        self.config = config
        self.actor = InferenceActor.options(
            num_gpus=config.ray.num_gpus, 
            num_cpus=config.ray.num_cpus
        ).remote(config)

    async def query(self, http_query: dict):
        """
        일반 쿼리 처리 API
        
        Args:
            http_query: 요청 데이터
            
        Returns:
            dict: 처리 결과
        """
        result = await self.actor.process_query.remote(http_query)
        return result
    
    async def process_query_stream(self, http_query: dict) -> str:
        """
        스트리밍 쿼리 처리 API
        
        Args:
            http_query: 요청 데이터
            
        Returns:
            str: 생성된 채팅/요청 ID
        """
        req_id = await self.actor.process_query_stream.remote(http_query)
        return req_id
    
    async def pop_sse_token(self, req_id: str) -> str:
        """
        SSE 토큰 조회 API
        
        Args:
            req_id: 요청 ID
            
        Returns:
            str: 다음 토큰
        """
        token = await self.actor.pop_sse_token.remote(req_id)
        return token

    async def close_sse_queue(self, req_id: str) -> str:
        """
        SSE 큐 종료 API
        
        Args:
            req_id: 요청 ID
            
        Returns:
            str: 종료 상태
        """
        await self.actor.close_sse_queue.remote(req_id)
        return "closed"
    
    async def get_history(self, request_id: str, last_index: int = None):
        """
        대화 기록 조회 API
        
        Args:
            request_id: 요청 ID
            last_index: 마지막으로 받은 메시지 인덱스
            
        Returns:
            dict: 대화 기록
        """
        result = await self.actor.get_conversation_history.remote(request_id, last_index)
        return result

    async def get_reference_data(self, chunk_ids: list):
        """
        참조 데이터 조회 API
        
        Args:
            chunk_ids: 청크 ID 목록
            
        Returns:
            list: 참조 데이터 목록
        """
        result = await self.actor.get_reference_data.remote(chunk_ids)
        return result

```


--- ray_deploy/langchain.py

```python

# ray_deploy/langchain.py

# 랭체인 도입
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage

# =============================================================================
# Custom Conversation Memory to store extra metadata (e.g., chunk_ids)
# =============================================================================
class CustomConversationBufferMemory(ConversationBufferMemory):
    """대화 저장 시 추가 메타데이터를 함께 기록"""
    def save_context(self, inputs: dict, outputs: dict) -> None:
        """
        inputs, outputs 예시:
            inputs = {
                "qry_contents": "사용자 질문",
                "qry_id": "...",
                "user_id": "...",
                "auth_class": "...",
                "qry_time": "..."
            }
            outputs = {
                "output": "AI의 최종 답변",
                "chunk_ids": [...참조 chunk_id 리스트...]
            }
        """
        try:
            user_content = inputs.get("qry_contents", "")
            human_msg = HumanMessage(
                content=user_content,
                additional_kwargs={
                    "qry_id": inputs.get("qry_id"),
                    "user_id": inputs.get("user_id"),
                    "auth_class": inputs.get("auth_class"),
                    "qry_time": inputs.get("qry_time")
                }
            )
            ai_content = outputs.get("output", "")
            ai_msg = AIMessage(
                content=ai_content,
                additional_kwargs={
                    "chunk_ids": outputs.get("chunk_ids", []),
                    "qry_id": inputs.get("qry_id"),
                    "user_id": inputs.get("user_id"),
                    "auth_class": inputs.get("auth_class"),
                    "qry_time": inputs.get("qry_time")
                }
            )

            self.chat_memory.messages.append(human_msg)
            self.chat_memory.messages.append(ai_msg)
        except Exception as e:
            print(f"[ERROR in save_context] {e}")
        
    def load_memory_variables(self, inputs: dict) -> dict:
        """
        랭체인 규약에 의해 {"history": [메시지 리스트]} 형태 리턴
        """
        try:
            return {"history": self.chat_memory.messages}
        except Exception as e:
            print(f"[ERROR in load_memory_variables] {e}")
            return {"history": []}

# Serialization function for messages
def serialize_message(msg):
    """
    HumanMessage -> {"role": "human", "content": ...}
    AIMessage    -> {"role": "ai", "content": ..., "references": [...]}
    """
    try:
        if isinstance(msg, HumanMessage):
            return {"role": "human", "content": msg.content}
        elif isinstance(msg, AIMessage):
            refs = msg.additional_kwargs.get("chunk_ids", [])
            # 디버그 출력
            print(f"[DEBUG serialize_message] AI refs: {refs}")
            return {"role": "ai", "content": msg.content, "references": refs}
        else:
            return {
                "role": "unknown",
                "content": getattr(msg, "content", str(msg))
            }
    except Exception as e:
        print(f"[ERROR in serialize_message] {e}")
        return {"role": "error", "content": str(e)}
    
# =============================================================================
# =============================================================================

```


--- utils/utils_format.py

```python

# utils/utils_format.py
import json
from datetime import datetime, timedelta

import requests

# Define the minimum valid file size (e.g., 10MB)
MIN_WEIGHT_SIZE = 10 * 1024 * 1024

# For tracking execution time of functions
from utils.tracking import time_tracker

# -------------------------------------------------
# Function: process_to_format
# -------------------------------------------------
@time_tracker
def process_to_format(qry_contents, type):
    # 여기서 RAG 시스템을 호출하거나 답변을 생성하도록 구현하세요.
    # 예제 응답 형식
    ### rsp_type : RA(Retrieval All), RT(Retrieval Text), RB(Retrieval taBle), AT(Answer Text), AB(Answer taBle) ###
    print("[SOOWAN] process_to_format 진입")
    if type == "Retrieval":
        print("[SOOWAN] 타입 : 리트리버")
        tmp_format = {"rsp_type": "R", "rsp_tit": "남성 내부 데이터", "rsp_data": []}
        for i, form in enumerate(qry_contents):
            tmp_format_ = {
                "rsp_tit": f"{i+1}번째 검색데이터: {form['title']} (출처:{form['file_name']})",
                "rsp_data": form["contents"],
                "chunk_id": form.get("chunk_id"),
            }
            tmp_format["rsp_data"].append(tmp_format_)
        return tmp_format

    elif type == "SQL":
        print("[SOOWAN] 타입 : SQL")
        tmp_format = {
            "rsp_type": "R",
            "rsp_tit": "남성 내부 데이터",
            "rsp_data": [{"rsp_tit": "SQL Query 결과표", "rsp_data": []}],
        }
        tmp_format_sql = {
            "rsp_type": "TB",
            "rsp_tit": qry_contents[0]["title"],
            "rsp_data": qry_contents[0]["data"],
        }
        tmp_format_chart = {
            "rsp_type": "CT",
            "rsp_tit": qry_contents[1]["title"],
            "rsp_data": {"chart_tp": "BAR", "chart_data": qry_contents[1]["data"]},
        }
        tmp_format["rsp_data"][0]["rsp_data"].append(tmp_format_sql)
        # tmp_format['rsp_data'].append(tmp_format_chart)
        return tmp_format, tmp_format_chart

    elif type == "Answer":
        print("[SOOWAN] 타입 : 대답")
        tmp_format = {"rsp_type": "A", "rsp_tit": "답변", "rsp_data": []}
        # for i, form in enumerate(qry_contents):
            # if i == 0:
        tmp_format_ = {"rsp_type": "TT", "rsp_data": qry_contents}
        tmp_format["rsp_data"].append(tmp_format_)
            # elif i == 1:
            #     tmp_format["rsp_data"].append(form)
            # else:
            #     None

        return tmp_format

    else:
        print("Error! Type Not supported!")
        return None

# @time_tracker
# def process_format_to_response(formats, qry_id, continue_="C", update_index=1):
#     # Get multiple formats to tuple

#     ans_format = {
#         "status_code": 200,
#         "result": "OK",
#         "detail": "",
#         "continue":continue_,
#         "qry_id": qry_id,
#         "rsp_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
#         "data_list": [],
#     }

#     # 누적된 토큰을 하나의 문자열로 결합합니다.
#     aggregated_answer = "".join(token.get("answer", "") for token in formats)
#     ans_format["data_list"].append({
#         "rsp_type": "A",
#         "rsp_tit": f"답변{update_index}",
#         "rsp_data": [
#             {
#                 "rsp_type": "TT",
#                 "rsp_data": aggregated_answer
#             }
#         ]
#     })
    
#     # Validate JSON before returning
#     try:
#         json.dumps(ans_format, ensure_ascii=False)  # Test JSON validity
#     except Exception as e:
#         print(f"[ERROR] Invalid JSON structure: {str(e)}")
#         ans_format["status_code"] = 500
#         ans_format["result"] = "ERROR"
#         ans_format["detail"] = f"JSON Error: {str(e)}"

#     # for format in formats:
#     #     ans_format["data_list"].append(format)

#     # return json.dumps(ans_format, ensure_ascii=False)
#     return ans_format

@time_tracker
def process_format_to_response(formats, qry_id, continue_="C", update_index=1):
    # If there are any reference tokens, return only them.
    reference_tokens = [token for token in formats if token.get("type") == "reference"]
    if reference_tokens:
        # For this example, we'll use the first reference token.
        ref = reference_tokens[0]
        # Add the extra keys.
        ref["qry_id"] = qry_id
        ref["continue"] = continue_
        ref["rsp_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")
        # Ensure that a "rsp_tit" key exists to satisfy downstream requirements.
        if "rsp_tit" not in ref:
            ref["rsp_tit"] = "Reference"
        return ref

    # Otherwise, aggregate the normal answer tokens.
    normal_tokens = [token.get("answer", "") for token in formats if token.get("type") != "reference"]
    aggregated_answer = "".join(normal_tokens)
    
    ans_format = {
        "status_code": 200,
        "result": "OK",
        "detail": "",
        "continue": continue_,
        "qry_id": qry_id,
        "rsp_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
        "data_list": [{
            "rsp_type": "A",
            "rsp_tit": f"답변{update_index}",
            "rsp_data": [{
                "rsp_type": "TT",
                "rsp_data": aggregated_answer
            }]
        }]
    }
    
    # Validate JSON structure before returning.
    try:
        json.dumps(ans_format, ensure_ascii=False)
    except Exception as e:
        print(f"[ERROR] Invalid JSON structure: {str(e)}")
        ans_format["status_code"] = 500
        ans_format["result"] = "ERROR"
        ans_format["detail"] = f"JSON Error: {str(e)}"
    
    return ans_format



# @time_tracker
# def process_format_to_response(formats, qry_id, continue_="C", update_index=1):
#     # 누적된 토큰들을 하나의 문자열로 결합합니다.
#     aggregated_answer = "".join(token.get("answer", "") for token in formats)
    
#     # retrieval과 동일한 구조를 위해, 답변 데이터는 내부 data_list가 딕셔너리 형태로 구성됩니다.
#     answer = {
#         "rsp_type": "A",                # Answer
#         "rsp_tit": f"답변{update_index}",
#         "rsp_data": [                    # 바로 텍스트 응답 리스트를 구성
#             {
#                 "rsp_tit": f"답변{update_index}",
#                 "rsp_data": [
#                     {
#                         'rsp_type': 'TT',
#                         'rsp_tit': '',
#                         'rsp_data': aggregated_answer,
#                     }
#                 ]
                
#             }
#         ]
#     }
    
#     # 최종 응답 구조: 최상위에 data_list는 리스트이고, 내부에 딕셔너리로 답변 데이터를 포함합니다.
#     ans_format = {
#         "status_code": 200,
#         "result": "OK",
#         "detail": "Answer",
#         "continue": continue_,
#         "qry_id": qry_id,
#         "rsp_time": datetime.now().isoformat(),
#         "data_list": [
#             {
#                 "type": "answer",               # 응답 타입 answer
#                 "status_code": 200,
#                 "result": "OK",
#                 "detail": "Answer",
#                 "evt_time": datetime.now().isoformat(),
#                 "data_list": answer              # retrieval의 data_list와 동일하게 딕셔너리 형태
#             }
#         ]
#     }
#     return ans_format

@time_tracker
def error_format(message, status, qry_id=""):
    ans_format = {
        "status_code": status,
        "result": message,
        "qry_id": qry_id,  # 추가: qry_id 포함
        "detail": "",
        "evt_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
    }
    return json.dumps(ans_format)

# @time_tracker
# def send_data_to_server(data, url):
#     headers = {
#         "Content-Type": "application/json; charset=utf-8"
#     }
#     try:
#         # 다른 서버로 데이터를 전송 (POST 요청)
#         response = requests.post(url, json=data, headers=headers)
#         if response.status_code == 200:
#             print(f"Data sent successfully: {data}")
#         else:
#             print(f"Failed to send data: {response.status_code}")
#             print(f"Failed data: {data}")
#     except requests.exceptions.RequestException as e:
#         print(f"Error sending data: {e}")
@time_tracker     
def send_data_to_server(data, url):
    try:
        if not data or "data_list" not in data:
            print("[ERROR] Empty or Invalid data structure")
            return
        # Log reference data if present
        for item in data["data_list"]:
            if item.get("rsp_type") == "A" and "references" in str(item):
                print(f"[DEBUG] Sending reference data: {json.dumps(data, ensure_ascii=False, indent=2)}")
        response = requests.post(url, json=data, timeout=10)
        
        if response.status_code != 200:
            print(f"[ERROR] Failed to send data: {response.status_code}, {response.text}")
        
        return response

    except Exception as e:
        print(f"[ERROR] send_data_to_server encountered an error: {str(e)}")

```


--- utils/utils_load.py

```python

# utils/utils_load.py
import json
import numpy as np
import torch
import random
import shutil
from datetime import datetime, timedelta
from transformers import (
    AutoModel,
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    AutoConfig,
)

import os

# 전역 캐시 변수 - 데이터의 변화를 감지하기 위한
_cached_data = None
_cached_data_mtime = 0

# Import vLLM utilities
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

# Define the minimum valid file size (e.g., 10MB)
MIN_WEIGHT_SIZE = 10 * 1024 * 1024

# For tracking execution time of functions
from utils.tracking import time_tracker

# Logging
import logging
# -------------------------------------------------
# Function: find_weight_directory - 허깅페이스 권한 문제 해결 후에 잘 사용되지 아니함
# -------------------------------------------------
# Recursively searches for weight files (safetensors or pytorch_model.bin) in a given base path.
# This method Find the files searching the whole directory
# Because, vLLM not automatically find out the model files.
# -------------------------------------------------
@time_tracker
def find_weight_directory(base_path):
    # ---- Recursively searches for weight files in a given base path ----
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if ".safetensors" in file or "pytorch_model.bin" in file:
                file_path = os.path.join(root, file)
                try:
                    if os.path.getsize(file_path) >= MIN_WEIGHT_SIZE:
                        return root, "safetensors" if ".safetensors" in file else "pt"
                    else:
                        logging.debug(
                            f"파일 {file_path}의 크기가 너무 작음: {os.path.getsize(file_path)} bytes"
                        )
                except Exception as ex:
                    logging.debug(f"파일 크기 확인 실패: {file_path} - {ex}")
    return None, None

# -------------------------------------------------
# Function: load_model
# -------------------------------------------------
@time_tracker
def load_model(config):
    # Loads the embedding model and the main LLM model (using vLLM if specified in the config).
    
    # Get the HF token from the environment variable.
    logging.info("Starting model loading...")
    token = os.getenv("HF_TOKEN_PATH")
    # Check if token is likely a file path.
    if token is not None and not token.startswith("hf_"):
        if os.path.exists(token) and os.path.isfile(token):
            try:
                with open(token, "r") as f:
                    token = f.read().strip()
            except Exception as e:
                print("DEBUG: Exception while reading token file:", e)
                logging.warning("Failed to read token from file: %s", e)
                token = None
        else:
            logging.warning("The HF_TOKEN path does not exist: %s", token)
            token = None
    else:
        print("DEBUG: HF_TOKEN appears to be a token string; using it directly:")

    if token is None or token == "":
        logging.warning("HF_TOKEN is not set. Access to gated models may fail.")
        token = None

    # -------------------------------
    # Load the embedding model and tokenizer.
    # -------------------------------
    print("Loading embedding model")
    try:
        embed_model = AutoModel.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,  # using 'token' parameter
        )
    except Exception as e:
        raise e
    try:
        embed_tokenizer = AutoTokenizer.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,
        )
    except Exception as e:
        raise e
    print(":Embedding tokenizer loaded successfully.")
    embed_model.eval()
    embed_tokenizer.model_max_length = 4096

    # -------------------------------
    # Load the main LLM model via vLLM.
    # -------------------------------
    if config.use_vllm:
        print("vLLM mode enabled. Starting to load main LLM model via vLLM.")
        if config.model.quantization_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
            print("Using 4-bit quantization.")
        elif config.model.quantization_8bit:
            bnb_config = BitsAndBytesConfig(load_in_8bit=True)
            print("Using 8-bit quantization.")
        else:
            bnb_config = None
            print("Using pure option of Model(No quantization)")

        local_model_path = os.path.join(
            config.cache_dir, "models--" + config.model_id.replace("/", "--")
        )
        local_model_path = os.path.abspath(local_model_path)

        config_file = os.path.join(local_model_path, "config.json")
        need_patch = False

        if not os.path.exists(config_file):
            os.makedirs(local_model_path, exist_ok=True)
            try:
                hf_config = AutoConfig.from_pretrained(
                    config.model_id,
                    cache_dir=config.cache_dir,
                    trust_remote_code=True,
                    token=token,
                )
            except Exception as e:
                raise e
            # 패치: vocab_size 속성이 없으면 embed_tokenizer의 값을 사용하여 추가
            if not hasattr(hf_config, "vocab_size"):
                print("[MODEL-LOADING] 'vocab_size' 속성이 없어서 기본값으로 추가합니다.")
                hf_config.vocab_size = getattr(embed_tokenizer, "vocab_size", 32000)
            config_dict = hf_config.to_dict()
            # 패치: architectures 속성이 없으면 Gemma2로 기본 설정
            if not config_dict.get("architectures"):
                print("[MODEL-LOADING] Config file의 architectures 정보 없음, Default Gemma2 아키텍처 설정")
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
            # 확인
            print(f"[DEBUG] => Saving HF Config with architectures={config_dict['architectures']}")
                
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config_dict, f)
        else:
            # 이미 config_file이 존재하는 경우
            with open(config_file, "r", encoding="utf-8") as f:
                config_dict = json.load(f)
            
            # 패치: vocab_size 속성이 없으면 embed_tokenizer의 값을 사용하여 추가
            if "vocab_size" not in config_dict:
                # embed_tokenizer의 vocab_size가 존재하면 사용하고, 없으면 기본값 30522로 설정
                config_dict["vocab_size"] = getattr(embed_tokenizer, "vocab_size", 30522)
                print("[MODEL-LOADING] 'vocab_size' 속성이 없어서 기본값으로 추가합니다:", config_dict["vocab_size"])
            # 패치: architectures 속성이 없으면 Gemma2로 기본 설정
            if not config_dict.get("architectures"):
                print("[MODEL-LOADING] Config file의 architectures 정보 없음, Default Gemma2 아키텍처 설정")
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
            # 확인
            print(f"[DEBUG] => Saving HF Config with architectures={config_dict['architectures']}")

            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config_dict, f)

        weight_dir, weight_format = find_weight_directory(local_model_path)
        if weight_dir is None:
            print("DEBUG: No model weights found. Attempting to download model snapshot.")
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    print(f"DEBUG: Snapshot download attempt {attempt+1}...")
                    # Attempt to download the model snapshot using the Hugging Face hub function.
                    from huggingface_hub import snapshot_download
                    snapshot_download(config.model_id, cache_dir=config.cache_dir, token=token)
                    break  # If download succeeds, break out of the loop.
                except Exception as e:
                    print(f"DEBUG: Snapshot download attempt {attempt+1} failed:", e)
                    if attempt < max_retries - 1:
                        print("DEBUG: Retrying snapshot download...")
                    else:
                        raise RuntimeError(f"Snapshot download failed after {max_retries} attempts: {e}")
            # After download, try to find the weights again.
            weight_dir, weight_format = find_weight_directory(local_model_path)
            if weight_dir is None:
                raise RuntimeError(f"Unable to find model weights even after snapshot download in {local_model_path}.")

        snapshot_config = os.path.join(weight_dir, "config.json")
        if not os.path.exists(snapshot_config):
            shutil.copy(config_file, snapshot_config)
        engine_args = AsyncEngineArgs(
            model=weight_dir,
            tokenizer=config.model_id,
            download_dir=config.cache_dir,
            trust_remote_code=True,
            config_format="hf",
            load_format=weight_format,
        )
        
        vllm_conf = config.get("vllm", {})
        
        # --- 기존 엔진 설정들
        engine_args.enable_prefix_caching = True
        # engine_args.scheduler_delay_factor = vllm_conf.get("scheduler_delay_factor", 0.1)
        engine_args.enable_chunked_prefill = True
        engine_args.tensor_parallel_size = vllm_conf.get("tensor_parallel_size", 1) # How many use the parllel Multi-GPU
        engine_args.max_num_seqs = vllm_conf.get("max_num_seqs")
        engine_args.max_num_batched_tokens = vllm_conf.get("max_num_batched_tokens", 8192)
        # engine_args.block_size = vllm_conf.get("block_size", 128)
        engine_args.gpu_memory_utilization = vllm_conf.get("gpu_memory_utilization")
        
        # --- 다중 GPU 사용 관련 설정 ---
        if vllm_conf.get("disable_custom_all_reduce", False):
            engine_args.disable_custom_all_reduce = True # For Fixing the Multi GPU problem
        
        # --- 모델의 Context Length ---
        engine_args.max_model_len = vllm_conf.get("max_model_len")
        
        # --- 멀티모달 (Image) 관련 설정 ---
        engine_args.mm_processor_kwargs = vllm_conf.get("mm_processor_kwargs", {"do_pan_and_scan": True})
        engine_args.disable_mm_preprocessor_cache = vllm_conf.get("disable_mm_preprocessor_cache", False)
        engine_args.limit_mm_per_prompt = vllm_conf.get("limit_mm_per_prompt", {"image": 5})
        
        # # --- Metrics (Monitoring) 관련 설정 ---
        # from vllm.config import ObservabilityConfig
        # observability_config = ObservabilityConfig(
        #     show_hidden_metrics=True,
        #     otlp_traces_endpoint=None,
        #     collect_model_forward_time=True,
        #     collect_model_execute_time=True
        # )

        # # Ensure it's properly assigned to engine_args
        # engine_args.observability_config = observability_config
    
        engine_args.show_hidden_metrics_for_version = "0.7"
    
        print("EngineArgs setting be finished")
        
        try:
            # --- v1 구동 해결책: 현재 스레드가 메인 스레드가 아니면 signal 함수를 임시 패치 ---
            import threading, signal
            if threading.current_thread() is not threading.main_thread():
                original_signal = signal.signal
                signal.signal = lambda s, h: None  # signal 설정 무시
                print("비메인 스레드에서 signal.signal을 monkey-patch 하였습니다.")
            # --- v1 구동 해결책: -------------------------------------------------------------
            engine = AsyncLLMEngine.from_engine_args(engine_args) # Original
            # v1 구동 해결책: 엔진 생성 후 원래 signal.signal으로 복원 (필요 시) -----------------
            if threading.current_thread() is not threading.main_thread():
                signal.signal = original_signal
            # --- v1 구동 해결책: -------------------------------------------------------------
            print("DEBUG: vLLM engine successfully created.") # Original
            
        except Exception as e:
            print("DEBUG: Exception during engine creation:", e)
            if "HeaderTooSmall" in str(e):
                print("DEBUG: Falling back to PyTorch weights.")
                fallback_dir = None
                for root, dirs, files in os.walk(local_model_path):
                    for file in files:
                        if (
                            "pytorch_model.bin" in file
                            and os.path.getsize(os.path.join(root, file))
                            >= MIN_WEIGHT_SIZE
                        ):
                            fallback_dir = root
                            break
                    if fallback_dir:
                        break
                if fallback_dir is None:
                    logging.error(
                        "DEBUG: No PyTorch weight file found in", local_model_path
                    )
                    raise e
                engine_args.load_format = "pt"
                engine_args.model = fallback_dir
                print("DEBUG: New EngineArgs for fallback:", engine_args)
                engine = AsyncLLMEngine.from_engine_args(engine_args)
                print("DEBUG: vLLM engine created with PyTorch fallback.")
            else:
                logging.error("DEBUG: Engine creation failed:", e)
                raise e

        engine.is_vllm = True

        print("DEBUG: Loading main LLM tokenizer with token authentication.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
                local_files_only=True  # Force loading from local cache to avoid hub requests
            )
        except Exception as e:
            print("DEBUG: Exception loading main tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        return engine, tokenizer, embed_model, embed_tokenizer

    else:
        print("DEBUG: vLLM is not used. Loading model via standard HF method.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        try:
            model = AutoModelForCausalLM.from_pretrained(
                config.model_id,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                cache_dir=config.cache_dir,
                # quantization_config=bnb_config,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading model:", e)
            raise e
        model.eval()
        return model, tokenizer, embed_model, embed_tokenizer

# -------------------------------------------------
# Function: load_data
# -------------------------------------------------
@time_tracker
def load_data(data_path):
    global _cached_data, _cached_data_mtime
    try:
        current_mtime = os.path.getmtime(data_path)
    except Exception as e:
        print("파일 수정 시간 확인 실패:", e)
        return None

    # 캐시가 비어있거나 파일 수정 시간이 변경된 경우 데이터 재로드
    if _cached_data is None or current_mtime != _cached_data_mtime:
        with open(data_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # --- 디버그 함수: 벡터 포맷 검사 ---
        debug_vector_format(data)

        # 데이터 전처리 (예: 리스트 변환 및 numpy, torch 변환)
        file_names = []
        chunk_ids = []  # >>> CHANGED: Added to record each chunk's ID
        titles = []
        times = []
        vectors = []
        texts = []
        texts_short = []
        texts_vis = []
        missing_time = 0

        for file_obj in data:
            for chunk in file_obj["chunks"]:
                file_names.append(file_obj["file_name"])
                chunk_ids.append(chunk.get("chunk_id", 0))  # >>> CHANGED: Record chunk_id
                try:
                    arr = np.array(chunk["vector"])
                    vectors.append(arr)
                except Exception as e:
                    logging.warning(f"[load_data] 벡터 변환 오류: {e} → 빈 벡터로 대체")
                    vectors.append(np.zeros((1, 768), dtype=np.float32))  # 임의로 1x768 형식
                
                titles.append(chunk["title"])
                
                # 날짜 파싱
                if chunk["date"]:
                    try:
                        times.append(datetime.strptime(chunk["date"], "%Y-%m-%d"))
                    except ValueError:
                        logging.warning(f"잘못된 날짜 형식: {chunk['date']} → 기본 날짜로 대체")
                        times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))
                        missing_time += 1
                else:
                    missing_time += 1
                    times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))

                texts.append(chunk["text"])
                texts_short.append(chunk["text_short"])
                texts_vis.append(chunk["text_vis"])

        # 실제 텐서로 변환
        try:
            vectors = np.array(vectors)
            vectors = torch.from_numpy(vectors).to(torch.float32)
        except Exception as e:
            logging.error(f"[load_data] 최종 벡터 텐서 변환 오류: {str(e)}")
            # 필요 시 추가 처리

        _cached_data = {
            "file_names": file_names,
            "chunk_ids": chunk_ids,  # >>> CHANGED: Saved chunk IDs here
            "titles": titles,
            "times": times,
            "vectors": vectors,
            "texts": texts,
            "texts_short": texts_short,
            "texts_vis": texts_vis,
        }
        _cached_data_mtime = current_mtime
        print(f"Data loaded! Length: {len(titles)}, Missing times: {missing_time}")
    else:
        print("Using cached data")

    return _cached_data

# -------------------------------------------------
# Function: debug_vector_format
# -------------------------------------------------
def debug_vector_format(data):
    """
    data(List[Dict]): load_data에서 JSON으로 로드된 객체.
    각 file_obj에 대해 chunks 리스트를 순회하며 vector 형식을 디버깅 출력.
    """
    print("\n[DEBUG] ===== 벡터 형식 검사 시작 =====")
    for f_i, file_obj in enumerate(data):
        file_name = file_obj.get("file_name", f"Unknown_{f_i}")
        chunks = file_obj.get("chunks", [])
        for c_i, chunk in enumerate(chunks):
            vector_data = chunk.get("vector", None)
            if vector_data is None:
                # print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector 없음(None)")
                continue
            # 자료형, 길이, shape 등 확인
            vector_type = type(vector_data)
            # shape을 안전하게 얻기 위해 np.array 변환 시도
            try:
                arr = np.array(vector_data)
                shape = arr.shape
                # print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector_type={vector_type}, shape={shape}")
            except Exception as e:
                print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector 변환 실패: {str(e)}")
    print("[DEBUG] ===== 벡터 형식 검사 종료 =====\n")

# -------------------------------------------------
# Function: random_seed
# -------------------------------------------------
@time_tracker
def random_seed(seed):
    # Set random seed for Python's built-in random module
    random.seed(seed)

    # Set random seed for NumPy
    np.random.seed(seed)

    # Set random seed for PyTorch
    torch.manual_seed(seed)

    # Ensure the same behavior on different devices (CPU vs GPU)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # If using multi-GPU.

    # Enable deterministic algorithms
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

```


--- utils/debug_tracking.py

```python

# utils/debug_tracking.py의 수정된 버전

import os
import time
import torch
import psutil
import threading
import logging
import json
from typing import Dict, Any, List, Optional, Tuple
from datetime import datetime
import gc
import uuid

# 로거 설정
logger = logging.getLogger("performance")

class PerformanceMonitor:
    """성능 메트릭을 수집하고 로깅하는 클래스"""
    
    # 클래스 변수로 싱글톤 인스턴스 관리
    _instance = None
    
    @classmethod
    def get_instance(cls, log_interval: int = 5):
        """
        싱글톤 인스턴스 반환 (Ray 직렬화 호환성을 위해 클래스 메서드로 구현)
        """
        if cls._instance is None:
            cls._instance = cls(log_interval)
        return cls._instance
    
    def __init__(self, log_interval: int = 5):
        """
        성능 모니터 초기화 (직접 호출 대신 get_instance() 사용 권장)
        
        Args:
            log_interval: 정기 로깅 간격 (초)
        """
        self.requests = {}  # request_id로 인덱싱된 요청 성능 데이터
        # threading.Lock() 대신 각 메서드에서 직접 동기화 관리
        self.log_interval = log_interval
        
        # 전역 통계
        self.global_stats = {
            "total_requests": 0,
            "completed_requests": 0,
            "failed_requests": 0,
            "total_input_tokens": 0,
            "total_output_tokens": 0,
            "total_processing_time": 0.0,
            "avg_tokens_per_second": 0.0,
            "avg_first_token_latency": 0.0,
            "first_token_latencies": [],  # 첫 토큰 레이턴시 기록 목록
        }
        
        # 정기 로깅 시작
        self._start_periodic_logging()
        logger.info(f"성능 모니터링 시스템 초기화 완료 (로깅 간격: {log_interval}초)")
    
    def _start_periodic_logging(self):
        """정기적인 성능 로깅을 수행하는 백그라운드 스레드 시작"""
        def log_periodically():
            while True:
                try:
                    self.log_current_state()
                except Exception as e:
                    logger.error(f"정기 로깅 중 오류: {e}")
                time.sleep(self.log_interval)
        
        thread = threading.Thread(target=log_periodically, daemon=True)
        thread.start()

    def start_request(self, request_id: str, input_text: str = "", tokenizer = None) -> None:
        """
        새 요청 추적 시작
        
        Args:
            request_id: 요청 ID
            input_text: 입력 텍스트
            tokenizer: 토큰 계산에 사용할 토크나이저 (선택 사항)
        """
        # 이미 존재하는 요청이면 건너뜀
        if request_id in self.requests:
            logger.warning(f"이미 추적 중인 요청 ID: {request_id}")
            return
            
        # 입력 토큰 수 계산 (토크나이저가 있으면 정확하게, 없으면 추정)
        input_tokens = 0
        if tokenizer:
            try:
                input_tokens = len(tokenizer.encode(input_text))
            except Exception:
                # 토크나이저 에러 시 공백 기준 단어 수로 추정
                input_tokens = len(input_text.split())
        else:
            input_tokens = len(input_text.split())
        
        # 요청 성능 데이터 초기화
        self.requests[request_id] = {
            "start_time": time.time(),
            "last_update_time": time.time(),
            "first_token_time": None,
            "input_tokens": input_tokens,
            "output_tokens": 0,
            "output_sequence": "",
            "tokens_per_second": 0.0,
            "status": "running",
            "gpu_stats": self._get_gpu_stats() if torch.cuda.is_available() else {},
            "checkpoint_times": []  # 주요 이벤트의 타임스탬프를 기록
        }
        
        # 전역 통계 업데이트
        self.global_stats["total_requests"] += 1
        self.global_stats["total_input_tokens"] += input_tokens
        
        logger.info(f"요청 추적 시작: {request_id} (입력 토큰: {input_tokens})")
    
    def update_request(self, request_id: str, tokens_generated: int, 
                      current_output: str = "", checkpoint: str = None) -> None:
        """
        요청 성능 데이터 업데이트
        
        Args:
            request_id: 요청 ID
            tokens_generated: 현재까지 생성된 토큰 수
            current_output: 현재까지 생성된 출력 텍스트 (선택 사항)
            checkpoint: 체크포인트 이름 (예: "retrieved_docs", "first_token", 등)
        """
        if request_id not in self.requests:
            logger.warning(f"알 수 없는 요청 ID: {request_id}")
            return
        
        req_data = self.requests[request_id]
        current_time = time.time()
        
        # 요청 데이터 업데이트 (토큰 수 무조건 업데이트)
        prev_tokens = req_data["output_tokens"]
        req_data["output_tokens"] = tokens_generated
        
        # 첫 토큰 시간 기록 (처음으로 토큰이 생성된 경우)
        if req_data["first_token_time"] is None and tokens_generated > 0:
            req_data["first_token_time"] = current_time
            
            # 첫 토큰 레이턴시 계산 및 로깅
            first_token_latency = current_time - req_data["start_time"]
            logger.info(f"첫 토큰 생성: {request_id} (latency: {first_token_latency:.3f}s)")
            
            # 체크포인트에 "first_token" 추가
            req_data["checkpoint_times"].append({
                "name": "first_token",
                "time": current_time,
                "elapsed": first_token_latency
            })
        
        # 토큰 속도 계산 (초당 토큰 수)
        time_diff = current_time - req_data["last_update_time"]
        token_diff = tokens_generated - prev_tokens
        
        if time_diff > 0 and token_diff > 0:
            # 실제 토큰 속도 계산
            tokens_per_second = token_diff / time_diff
            
            # v0 호환 속도 계산 (스케일링 팩터 적용)
            v0_compatible_rate = tokens_per_second * 2.1  # v0과 v1의 측정 방식 차이를 보정하는 계수
            
            # 지수 이동 평균으로 토큰 속도 부드럽게 업데이트
            alpha = 0.3  # 가중치 계수
            current_rate = req_data.get("tokens_per_second", 0)
            if current_rate > 0:
                req_data["tokens_per_second"] = (1-alpha) * current_rate + alpha * v0_compatible_rate
            else:
                req_data["tokens_per_second"] = v0_compatible_rate
            
            logger.debug(f"토큰 생성 속도 업데이트: {request_id}, {req_data['tokens_per_second']:.2f} tokens/s")
        
        # 출력 텍스트 업데이트
        if current_output:
            req_data["output_sequence"] = current_output
        
        # 마지막 업데이트 시간 기록
        req_data["last_update_time"] = current_time
        
        # 체크포인트 기록 (제공된 경우)
        if checkpoint:
            elapsed = current_time - req_data["start_time"]
            req_data["checkpoint_times"].append({
                "name": checkpoint,
                "time": current_time,
                "elapsed": elapsed
            })
            logger.info(f"체크포인트 {checkpoint}: {request_id} (경과: {elapsed:.3f}s, 토큰: {tokens_generated})")
        
        # GPU 통계 업데이트 (가능한 경우)
        # 체크포인트에서만 GPU 통계 업데이트 (성능 향상)
        if checkpoint and torch.cuda.is_available():
            req_data["gpu_stats"] = self._get_gpu_stats()
            
    
    def log_current_state(self) -> Dict[str, Any]:
        """
        현재 시스템 상태와 성능 메트릭을 로깅하고 반환
        
        Returns:
            Dict[str, Any]: 현재 성능 메트릭
        """
        # 시스템 리소스 사용량
        process = psutil.Process(os.getpid())
        mem_info = process.memory_info()
        cpu_percent = process.cpu_percent(interval=0.1)
        
        # GPU 사용량 (가능한 경우)
        gpu_stats = self._get_gpu_stats() if torch.cuda.is_available() else {}
        
        # 활성 요청 요약
        active_requests = len(self.requests)
        
        # 현재 초당 평균 토큰 수 계산
        current_tokens_per_second = 0.0
        total_tokens_generated = 0
        active_request_details = []
        
        if active_requests > 0:
            token_rates = []
            for req_id, req_data in self.requests.items():
                req_token_rate = req_data.get("tokens_per_second", 0)
                if req_token_rate > 0:
                    token_rates.append(req_token_rate)
                total_tokens_generated += req_data.get("output_tokens", 0)
                
                # 요청별 세부 정보
                active_request_details.append({
                    "id": req_id[:8] + ".." if len(req_id) > 10 else req_id,
                    "tokens": req_data.get("output_tokens", 0),
                    "rate": req_token_rate,
                    "elapsed": time.time() - req_data.get("start_time", time.time())
                })
            
            if token_rates:
                current_tokens_per_second = sum(token_rates) / len(token_rates)
        
        # 글로벌 성능 지표 업데이트 - 활성 요청이 없을 때는 현재 속도를 0으로 리셋
        if active_requests == 0:
            cooldown_period = 5.0  # 요청 완료 후 5초 지나면 지표 리셋
            # 활성 요청이 없으면 현재 속도는 0
            current_tokens_per_second = 0
            
            # 최근 완료된 요청이 없거나 일정 시간이 지났으면 모든 지표 리셋
            if not hasattr(self, '_last_completed_time') or time.time() - self._last_completed_time > cooldown_period:
                self.global_stats["avg_first_token_latency"] = 0
                self.global_stats["avg_tokens_per_second"] = 0
                
                # 로그 스킵 로직 - 유휴 상태에서는 30초에 한 번만 로그 출력
                if not hasattr(self, '_last_idle_log') or time.time() - self._last_idle_log > 30.0:
                    self._last_idle_log = time.time()
                    logger.info("시스템 유휴 상태: 활성 요청 없음")
                    # 최소한의 지표만 로깅 및 반환
                    return self._create_minimal_metrics()
                else:
                    # 중간 유휴 시간에는 로깅하지 않고 지표만 반환
                    return self._create_minimal_metrics()
                
        # 메트릭 구성
        metrics = {
            "timestamp": datetime.now().isoformat(),
            "process": {
                "memory_rss_gb": mem_info.rss / (1024**3),
                "memory_vms_gb": mem_info.vms / (1024**3),
                "cpu_percent": cpu_percent
            },
            "gpu": gpu_stats,
            "requests": {
                "active": active_requests,
                "total": self.global_stats["total_requests"],
                "completed": self.global_stats["completed_requests"],
                "failed": self.global_stats["failed_requests"],
                "active_details": active_request_details
            },
            "performance": {
                "current_tokens_per_second": current_tokens_per_second,
                "avg_tokens_per_second": self.global_stats["avg_tokens_per_second"] if active_requests > 0 or (hasattr(self, '_last_completed_time') and time.time() - self._last_completed_time < cooldown_period) else 0,
                "avg_first_token_latency": self.global_stats.get("avg_first_token_latency", 0) * 1000,  # ms
                "total_input_tokens": self.global_stats["total_input_tokens"],
                "total_output_tokens": self.global_stats["total_output_tokens"],
                "current_output_tokens": total_tokens_generated
            }
        }
        

        
        # GPU 메모리 정보 문자열 구성
        gpu_memory_str = ""
        if gpu_stats:
            gpu_memory_str = ', '.join([f'GPU {i}: {stats["allocated_gb"]:.1f}GB' for i, stats in gpu_stats.items()])
        
        # 활성 요청 세부 정보 문자열
        active_req_str = ""
        if active_request_details:
            req_details = [f"{d['id']}({d['tokens']}t, {d['rate']:.1f}t/s)" for d in active_request_details]
            active_req_str = f" | Active: {', '.join(req_details)}"
        
        # vLLM 스타일 로그 포맷 (v1에서 나타나지 않는 문제 대응)
        vllm_style_log = (
            f"[성능 지표] "
            f"Prompt: {metrics['performance']['current_tokens_per_second']:.1f} t/s, "
            f"생성: {metrics['performance']['avg_tokens_per_second']:.1f} t/s | "
            f"요청: {metrics['requests']['active']} 실행, "
            f"{metrics['requests']['completed']} 완료, "
            f"{metrics['performance']['current_output_tokens']} 토큰"
            f"{active_req_str} | "
            f"{gpu_memory_str} | "
            f"첫 토큰: {metrics['performance']['avg_first_token_latency']:.1f}ms"
        )
        
        logger.info(vllm_style_log)
        return metrics
    
    def _create_minimal_metrics(self) -> Dict[str, Any]:
        """유휴 상태에서 사용할 최소한의 메트릭 생성"""
        return {
            "timestamp": datetime.now().isoformat(),
            "process": {"memory_rss_gb": 0, "memory_vms_gb": 0, "cpu_percent": 0},
            "gpu": self._get_gpu_stats() if torch.cuda.is_available() else {},
            "requests": {
                "active": 0,
                "total": self.global_stats["total_requests"],
                "completed": self.global_stats["completed_requests"],
                "failed": self.global_stats["failed_requests"],
                "active_details": []
            },
            "performance": {
                "current_tokens_per_second": 0,
                "avg_tokens_per_second": 0,
                "avg_first_token_latency": 0,
                "total_input_tokens": self.global_stats["total_input_tokens"],
                "total_output_tokens": self.global_stats["total_output_tokens"],
                "current_output_tokens": 0
            }
        }

    def complete_request(self, request_id: str, success: bool = True) -> Dict[str, Any]:
        """
        요청 완료 처리 및 최종 성능 데이터 반환
        
        Args:
            request_id: 요청 ID
            success: 요청 성공 여부
            
        Returns:
            Dict[str, Any]: 완료된 요청의 성능 데이터
        """
        if request_id not in self.requests:
            logger.warning(f"알 수 없는 요청 ID: {request_id}")
            return {}
        
        req_data = self.requests[request_id]
        end_time = time.time()
        total_time = end_time - req_data["start_time"]
        
        # 요청 상태 업데이트
        req_data["status"] = "completed" if success else "failed"
        req_data["total_time"] = total_time
        
        # 전역 통계 업데이트
        if success:
            self.global_stats["completed_requests"] += 1
        else:
            self.global_stats["failed_requests"] += 1
        
        self.global_stats["total_output_tokens"] += req_data["output_tokens"]
        self.global_stats["total_processing_time"] += total_time
        
        # 현재까지의 평균 계산
        completed = self.global_stats["completed_requests"]
        if completed > 0:
            self.global_stats["avg_tokens_per_second"] = (
                self.global_stats["total_output_tokens"] / 
                self.global_stats["total_processing_time"]
            )
        
        # 첫 토큰 레이턴시가 측정된 경우, 평균 업데이트
        if req_data["first_token_time"]:
            first_token_latency = req_data["first_token_time"] - req_data["start_time"]
            
            # 첫 토큰 레이턴시 목록에 추가
            self.global_stats["first_token_latencies"].append(first_token_latency)
            
            # 최근 20개 샘플만 유지 (메모리 관리)
            if len(self.global_stats["first_token_latencies"]) > 20:
                self.global_stats["first_token_latencies"] = self.global_stats["first_token_latencies"][-20:]
            
            # 평균 계산
            self.global_stats["avg_first_token_latency"] = sum(
                self.global_stats["first_token_latencies"]
            ) / len(self.global_stats["first_token_latencies"])
        
        # 마지막 완료 시간 기록 - 지표 표시 제어를 위함
        self._last_completed_time = end_time
        
        # 완료 로깅
        tokens_per_second = req_data["output_tokens"] / total_time if total_time > 0 else 0
        logger.info(
            f"요청 완료: {request_id} "
            f"({req_data['status']}, "
            f"시간: {total_time:.3f}s, "
            f"토큰: {req_data['output_tokens']}, "
            f"속도: {tokens_per_second:.2f} tokens/s)"
        )
        
        # 요청 데이터 복사 후 반환 (메모리 최적화를 위해 원본 제거)
        result = req_data.copy()
        del self.requests[request_id]
        
        return result
    
    def _get_gpu_stats(self) -> Dict[str, Dict[str, float]]:
        """
        모든 가용 GPU의 메모리 사용량 통계 수집
        
        Returns:
            Dict[str, Dict[str, float]]: GPU별 메모리 사용량 통계
        """
        stats = {}
        try:
            for i in range(torch.cuda.device_count()):
                allocated = torch.cuda.memory_allocated(i)
                reserved = torch.cuda.memory_reserved(i)
                total = torch.cuda.get_device_properties(i).total_memory
                
                stats[str(i)] = {
                    "allocated_bytes": allocated,
                    "reserved_bytes": reserved,
                    "total_bytes": total,
                    "allocated_gb": allocated / (1024**3),
                    "reserved_gb": reserved / (1024**3),
                    "total_gb": total / (1024**3),
                    "utilization": allocated / total if total > 0 else 0
                }
                
                # 필요시 CUDA 유틸리티 추가 (CUDA 설치된 경우)
                if hasattr(torch.cuda, 'utilization'):
                    stats[str(i)]["compute_utilization"] = torch.cuda.utilization(i)
        except Exception as e:
            logger.error(f"GPU 상태 수집 중 오류: {e}")
        
        return stats
    
    def _log_performance_metrics(self):
        """주기적으로 성능 지표를 로깅"""
        active_requests = len(self.requests)
        if active_requests == 0:
            logging.getLogger("performance").info("시스템 유휴 상태: 활성 요청 없음")
            return
            
        # 통계 계산
        total_prompt_tokens = 0
        total_output_tokens = 0
        prompt_speed = 0
        generation_speed = 0
        gpu_usage = 0
        
        active_request_info = []
        for req_id, req_data in self.requests.items():
            tokens = req_data.get("token_count", 0)
            total_output_tokens += tokens
            
            # 토큰 생성 속도 계산
            elapsed = time.time() - req_data.get("start_time", time.time())
            if elapsed > 0:
                req_speed = tokens / elapsed
                generation_speed += req_speed
                
                # 요약 정보 추가
                active_request_info.append(
                    f"{req_id[:8]}..({tokens}t, {req_speed:.1f}t/s)"
                )
            
            # GPU 사용량 누적
            gpu_usage = max(gpu_usage, req_data.get("gpu_usage", 0))
        
        # 첫 토큰 시간 측정값 추출
        first_token_times = []
        for req_id, req_data in self.requests.items():
            for checkpoint in req_data.get("checkpoints", []):
                if "first_token_generated" in checkpoint.get("name", ""):
                    # 괄호 안의 지연 시간 추출
                    latency_match = re.search(r"latency: ([\d\.]+)s", checkpoint.get("name", ""))
                    if latency_match:
                        first_token_times.append(float(latency_match.group(1)) * 1000)  # ms 단위로 변환
        
        # 첫 토큰 평균 시간 계산
        avg_first_token = sum(first_token_times) / len(first_token_times) if first_token_times else 0
        
        # 메트릭 로깅
        logging.getLogger("performance").info(
            f"[성능 지표] Prompt: {prompt_speed:.1f} t/s, 생성: {generation_speed:.1f} t/s | "
            f"요청: {active_requests} 실행, {len(self.completed_requests)} 완료, {total_output_tokens} 토큰 | "
            f"Active: {', '.join(active_request_info)} | "
            f"GPU {0}: {gpu_usage:.1f}GB | "
            f"첫 토큰: {avg_first_token:.1f}ms"
        )
        
# 성능 모니터 가져오기 - Ray와 호환되는 방식
def get_performance_monitor() -> PerformanceMonitor:
    """
    PerformanceMonitor의 싱글톤 인스턴스 반환 (Ray 호환 방식)
    
    Returns:
        PerformanceMonitor: 싱글톤 인스턴스
    """
    log_interval = int(os.environ.get("VLLM_LOG_STATS_INTERVAL", "5"))
    return PerformanceMonitor.get_instance(log_interval)

# 하위 호환성 함수들
def log_system_info(label=""):
    """현재 시스템 상태 로깅"""
    monitor = get_performance_monitor()
    monitor.log_current_state()

def log_batch_info(batch):
    """배치 정보 로깅"""
    if not batch:
        return
    
    batch_size = len(batch)
    token_counts = []
    
    for item in batch:
        # item은 (http_query, future, sse_queue) 튜플
        http_query = item[0]
        # http_query가 dict라면 qry_contents를 가져옵니다.
        query = http_query.get("qry_contents", "") if isinstance(http_query, dict) else ""
        tokens = query.split()
        token_counts.append(len(tokens))
    
    logger.info(f"[Batch Tracking] Batch size: {batch_size}, Token counts: {token_counts}")

# 스트리밍 토큰 카운터
class StreamingTokenCounter:
    """스트리밍 생성 시 토큰 수를 정확하게 추적하는 클래스"""
    
    def __init__(self, request_id: str, tokenizer=None):
        """토큰 카운터 초기화"""
        self.request_id = request_id
        self.tokenizer = tokenizer
        self.accumulated_text = ""
        self.token_count = 0
        self.start_time = time.time()
        self.last_update_time = self.start_time
        self.first_token_time = None
        self.generation_start_time = time.time()  # 생성 시작 시간 추가
        self.vllm_tokens = []  # vLLM 토큰 정보를 누적할 리스트
        # 첫 토큰 처리를 추적하기 위한 플래그 추가
        self.first_token_reported = False
        # Monritor
        self.perf_monitor = get_performance_monitor()
        # 객체 직렬화 시 다시 초기화될 메서드
        self._init_monitor()
    
    def _init_monitor(self):
        """성능 모니터 참조 초기화 (Ray 액터 내부에서 호출될 것임)"""
        self.monitor = get_performance_monitor()
        
    def update(self, new_text, vllm_info=None):
        """토큰 카운터 업데이트 및 성능 모니터링 갱신"""
        current_time = time.time()
        
        # vLLM 정보가 있으면 더 정확한 토큰 수를 얻을 수 있음
        if vllm_info and "token_ids" in vllm_info:
            new_tokens = len(vllm_info["token_ids"])
        else:
            # 간단한 추정 (더 정확한 토큰화 메서드로 대체 가능)
            new_tokens = len(new_text.split())
        
        self.token_count += new_tokens
        
        # 첫 토큰 시간 측정 및 보고
        if not self.first_token_reported and self.token_count > 0:
            first_token_latency = current_time - self.start_time
            self.perf_monitor.update_request(
                self.request_id, 
                self.token_count,
                checkpoint=f"first_token_generated (latency: {first_token_latency:.3f}s)",
                current_output=new_text
            )
            self.first_token_reported = True
            
        # GPU 사용량 측정 추가
        gpu_usage = self._measure_gpu_usage()
            
        # 성능 모니터에 현재 상태 업데이트
        self.perf_monitor.update_request(
            self.request_id, 
            self.token_count,
            generation_speed=(self.token_count / (current_time - self.start_time)),
            gpu_usage=gpu_usage,
            current_output=new_text
        )
        
        self.last_update_time = current_time

    def _measure_gpu_usage(self):
        """현재 GPU 사용량을 MB 단위로 측정"""
        try:
            import torch
            if torch.cuda.is_available():
                # 현재 활성 GPU의 메모리 사용량 반환 (GB 단위)
                return torch.cuda.memory_allocated() / (1024 ** 3)
        except (ImportError, Exception):
            pass
        return 0  # 측정 실패 시 0 반환

# 메모리 사용량 모니터링 함수
def print_memory_usage(label: str = "") -> Dict[str, Any]:
    """현재 메모리 사용량을 로깅하고 반환"""
    process = psutil.Process(os.getpid())
    mem_info = process.memory_info()
    
    metrics = {
        "rss_gb": mem_info.rss / (1024**3),
        "vms_gb": mem_info.vms / (1024**3),
    }
    
    if torch.cuda.is_available():
        for i in range(torch.cuda.device_count()):
            allocated = torch.cuda.memory_allocated(i) / (1024**3)
            reserved = torch.cuda.memory_reserved(i) / (1024**3)
            metrics[f"gpu{i}_allocated_gb"] = allocated
            metrics[f"gpu{i}_reserved_gb"] = reserved
    
    prefix = f"[{label}] " if label else ""
    logger.info(
        f"{prefix}Memory usage: "
        f"RAM: {metrics['rss_gb']:.2f}GB (RSS), {metrics['vms_gb']:.2f}GB (VMS)"
    )
    
    if torch.cuda.is_available():
        gpu_info = ", ".join(
            f"GPU{i}: {metrics[f'gpu{i}_allocated_gb']:.2f}GB/{metrics[f'gpu{i}_reserved_gb']:.2f}GB" 
            for i in range(torch.cuda.device_count())
        )
        logger.info(f"{prefix}GPU memory: {gpu_info}")
    
    return metrics

# 가비지 컬렉션 함수
def force_gc() -> None:
    """메모리 해제를 위한 강제 가비지 컬렉션 수행"""
    before = print_memory_usage("GC 전")
    
    gc.collect()
    
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        torch.cuda.synchronize()
    
    after = print_memory_usage("GC 후")
    
    # 변화량 계산
    rss_diff = before["rss_gb"] - after["rss_gb"]
    vms_diff = before["vms_gb"] - after["vms_gb"]
    
    logger.info(f"GC 효과: RAM: {rss_diff:.2f}GB (RSS), {vms_diff:.2f}GB (VMS)")

```


--- core/generation.py

```python

# core/generation.py
"""
텍스트 생성 관련 기능을 제공하는 모듈

주요 기능:
1. LLM을 통한 텍스트 생성
2. 스트리밍 방식의 텍스트 생성
3. vLLM 관련 유틸리티 기능
"""

import logging
import asyncio
import concurrent.futures
from typing import Dict, Any, List, Generator, AsyncGenerator
import uuid

from utils.tracking import time_tracker
from prompt import GENERATE_PROMPT_TEMPLATE

# vLLM 관련 임포트
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm import SamplingParams

# 로깅 설정
logger = logging.getLogger("RAG.Generation")

@time_tracker
async def generate(
    docs: str, 
    query: str, 
    model, 
    tokenizer, 
    config
) -> str:
    """
    검색된 문서와 질문을 바탕으로 응답 생성
    
    Args:
        docs: 검색된 문서 내용
        query: 사용자 질문
        model: 언어 모델
        tokenizer: 토크나이저
        config: 설정
        
    Returns:
        str: 생성된 텍스트
    """
    # 프롬프트 구성
    prompt = GENERATE_PROMPT_TEMPLATE.format(docs=docs, query=query)
    logger.info("텍스트 생성 시작: prompt_length=%d", len(prompt))
    
    # vLLM 모드인 경우
    if config.use_vllm:
        from vllm import SamplingParams
        
        # 샘플링 파라미터 설정
        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        
        # 요청 ID 생성
        accepted_request_id = str(uuid.uuid4())
        
        # vLLM으로 텍스트 생성
        answer = await collect_vllm_text(prompt, model, sampling_params, accepted_request_id)
    
    # 표준 HuggingFace 모델 사용
    else:
        # 입력 토큰화
        input_ids = tokenizer(
            prompt, 
            return_tensors="pt", 
            truncation=True, 
            max_length=4024
        ).to("cuda")
        
        # 토큰 수 계산
        token_count = input_ids["input_ids"].shape[1]
        
        # 텍스트 생성
        outputs = model.generate(
            **input_ids,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )
        
        # 생성된 텍스트만 추출
        answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)
        logger.debug("생성된 답변: %s", answer)
    
    logger.info("텍스트 생성 완료")
    return answer

@time_tracker
async def collect_vllm_text(
    prompt: Any, 
    model: AsyncLLMEngine, 
    sampling_params: SamplingParams, 
    request_id: str
) -> str:
    """
    vLLM 모델에서 텍스트 생성 후 결과 수집
    
    Args:
        prompt: 프롬프트 (텍스트 또는 멀티모달 요청 객체)
        model: vLLM 엔진
        sampling_params: 샘플링 파라미터
        request_id: 요청 ID
        
    Returns:
        str: 생성된 텍스트
    """
    logger.info(f"vLLM 텍스트 생성 시작: request_id={request_id}")
    
    outputs = []
    async for output in model.generate(prompt, request_id=request_id, sampling_params=sampling_params):
        outputs.append(output)
    
    # 출력이 없는 경우 오류 발생
    if not outputs:
        error_msg = "모델이 출력을 생성하지 않음"
        logger.error(error_msg)
        raise RuntimeError(error_msg)
    
    # 마지막 또는 완료된 출력 선택
    final_output = next((o for o in outputs if getattr(o, "finished", False)), outputs[-1])
    
    # 텍스트 추출
    answer = "".join([getattr(comp, "text", "") for comp in getattr(final_output, "outputs", [])])
    
    logger.info(f"vLLM 텍스트 생성 완료: text_length={len(answer)}")
    return answer

@time_tracker
async def collect_vllm_text_stream(
    prompt: Any, 
    engine: AsyncLLMEngine, 
    sampling_params: SamplingParams, 
    request_id: str
) -> AsyncGenerator[str, None]:
    """
    vLLM 모델에서 스트리밍 방식으로 텍스트 생성 및 반환
    
    Args:
        prompt: 프롬프트 (텍스트 또는 멀티모달 요청 객체)
        engine: vLLM 엔진
        sampling_params: 샘플링 파라미터
        request_id: 요청 ID
        
    Yields:
        str: 생성된 부분 텍스트
    """
    logger.info(f"vLLM 스트리밍 텍스트 생성 시작: request_id={request_id}")
    
    # 토큰 카운터 초기화
    from utils.debug_tracking import StreamingTokenCounter
    token_counter = StreamingTokenCounter(request_id)
    
    accumulated_text = ""
    
    try:
        async for request_output in engine.generate(prompt, request_id=request_id, sampling_params=sampling_params):
            if not request_output.outputs:
                continue
                
            for completion in request_output.outputs:
                new_text = completion.text
                
                # 새 텍스트가 생성된 경우에만 처리
                if new_text != accumulated_text:
                    # 새로 생성된 텍스트 부분만 추출
                    new_portion = new_text[len(accumulated_text):]
                    accumulated_text = new_text
                    
                    # vLLM 정보 추출 (가능한 경우)
                    vllm_info = None
                    if hasattr(completion, "token_ids"):
                        vllm_info = {"token_ids": completion.token_ids}
                    elif hasattr(completion, "logprobs") and completion.logprobs:
                        # 로그 확률이 있는 경우 토큰 ID 추출 가능
                        vllm_info = {"token_ids": [lp.token_id for lp in completion.logprobs if hasattr(lp, "token_id")]}
                    
                    # 토큰 카운터 업데이트
                    token_counter.update(new_portion, vllm_info)
                    
                    yield new_text
    except Exception as e:
        logger.error(f"vLLM 스트리밍 생성 중 오류: {str(e)}")
        raise

    logger.info(f"vLLM 스트리밍 텍스트 생성 완료: request_id={request_id}")

```


--- core/image_processing.py

```python

# core/image_processing.py
"""
이미지 처리 관련 기능을 제공하는 모듈

주요 기능:
1. 이미지 로드 및 처리
2. 이미지에 대한 설명 생성
3. 멀티모달 요청 준비
"""

import base64
import io
import logging
import json
import re
import uuid
from typing import Dict, Any, Optional, Union
from PIL import Image

from utils.tracking import time_tracker
from prompt import IMAGE_DESCRIPTION_PROMPT

# vLLM 관련 임포트
from vllm.multimodal.utils import fetch_image

# 로깅 설정
logger = logging.getLogger("RAG.ImageProcessing")

@time_tracker
async def prepare_image(image_data: Union[str, bytes]) -> Optional[Image.Image]:
    """
    다양한 형식의 이미지 데이터를 PIL Image로 변환
    
    Args:
        image_data: 이미지 데이터 (URL 또는 base64 인코딩된 데이터)
        
    Returns:
        Optional[Image.Image]: 변환된 PIL 이미지, 실패 시 None
    """
    try:
        logger.info("이미지 변환 시작: 데이터 타입=%s", type(image_data))
        
        # URL인 경우
        if isinstance(image_data, str) and (image_data.startswith("http://") or image_data.startswith("https://")):
            logger.info("URL 형식 이미지 로드")
            pil_image = fetch_image(image_data)
            
        # Base64 인코딩 데이터인 경우
        else:
            logger.info("Base64 형식 이미지 로드")
            # Data URL 형식인 경우 헤더 제거
            if isinstance(image_data, str) and image_data.startswith("data:image/"):
                logger.info("'data:image/' 접두사 감지 - 분리")
                image_data = image_data.split(",", 1)[-1]
                
            # Base64 디코딩
            decoded = base64.b64decode(image_data)
            logger.info("Base64 디코딩 완료: %d 바이트", len(decoded))
            
            # PIL 이미지로 변환
            pil_image = Image.open(io.BytesIO(decoded)).convert("RGB")
            
        logger.info("이미지 로드 성공: 크기=%s", pil_image.size)
        return pil_image
        
    except Exception as e:
        logger.error("이미지 로드 실패: %s", str(e), exc_info=True)
        return None

@time_tracker
async def prepare_processor(model_id: str) -> Any:
    """
    이미지 처리를 위한 프로세서 로드
    
    Args:
        model_id: 모델 ID
        
    Returns:
        Any: 로드된 프로세서
    """
    try:
        from transformers import AutoProcessor
        
        logger.info(f"프로세서 로드: model_id='{model_id}'")
        processor = AutoProcessor.from_pretrained(
            model_id,
            use_fast=False  # 모델이 fast processor를 지원한다면 True로 시도 가능
        )
        
        logger.info("프로세서 로드 완료: 타입=%s", type(processor).__name__)
        return processor
        
    except Exception as e:
        logger.error("프로세서 로드 실패: %s", str(e), exc_info=True)
        raise

@time_tracker
async def prepare_multimodal_request(
    prompt: str, 
    pil_image: Image.Image, 
    model_id: Any, 
    tokenizer: Any
) -> Dict[str, Any]:
    """
    멀티모달 요청 준비
    
    Args:
        prompt: 텍스트 프롬프트
        pil_image: PIL 이미지
        model: 언어 모델
        tokenizer: 토크나이저
        
    Returns:
        Dict[str, Any]: 멀티모달 요청 객체
    """
    try:
        from transformers import AutoProcessor
        
        # 모델 프로세서 로드
        processor = await prepare_processor(model_id)
        
        logger.info("멀티모달 메시지 구성")
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "url": pil_image},
                    {"type": "text", "text": prompt},
                ],
            }
        ]
        
        # 채팅 템플릿 적용
        prompt_string = processor.apply_chat_template(
            messages,
            tokenize=False,  # 토큰화 없이 문자열만 생성
            add_generation_prompt=True,
        )
        
        logger.info("멀티모달 요청 객체 생성: prompt_length=%d", len(prompt_string))
        generate_request = {
            "prompt": prompt_string,
            "multi_modal_data": {
                "image": [pil_image]  # 여러 이미지 지원 시 리스트에 추가
            }
        }
        
        return generate_request
        
    except Exception as e:
        logger.error("멀티모달 요청 준비 실패: %s", str(e), exc_info=True)
        raise

@time_tracker
async def image_query(http_query: Dict[str, Any], model, config) -> Dict[str, Any]:
    """
    이미지 입력을 받아 설명 생성 (비 스트리밍)
    
    Args:
        http_query: HTTP 요청 데이터
        model: 언어 모델
        config: 설정
        
    Returns:
        Dict[str, Any]: 이미지 설명 결과
    """
    logger.info("이미지 분석 쿼리 처리 시작")
    
    # 파라미터 파싱
    request_id = http_query.get("request_id", str(uuid.uuid4()))
    image_input = http_query.get("image_data")
    user_query = http_query.get("qry_contents", "이 이미지를 한국어로 잘 설명해주세요.")
    
    logger.info(f"요청 정보: request_id={request_id}, user_query='{user_query}'")
    
    # 이미지 변환
    pil_image = await prepare_image(image_input)
    if not pil_image:
        error_msg = "이미지 로드 실패"
        logger.error(error_msg)
        return {"type": "error", "message": error_msg}
    
    # 프로세서 로드
    try:
        processor = await prepare_processor(config.model_id)
    except Exception as e:
        error_msg = f"프로세서 로드 실패: {str(e)}"
        logger.error(error_msg)
        return {"type": "error", "message": error_msg}
    
    # 이미지 정보 및 프롬프트 구성
    image_info = "Image data provided"  # 실제 서비스에서는 이미지 메타데이터 추출 가능
    prompt_image_sorting = IMAGE_DESCRIPTION_PROMPT.format(image_info=image_info, user_query=user_query)
    
    # 메시지 구성
    messages = [
        {
            "role": "system",
            "content": [{"type": "text", "text": prompt_image_sorting}]
        },
        {
            "role": "user",
            "content": [
                {"type": "image", "url": pil_image},
                {"type": "text", "text": user_query},
            ],
        }
    ]
    
    # 채팅 템플릿 적용
    try:
        prompt_string = processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        logger.info("프롬프트 생성 완료: length=%d", len(prompt_string))
    except Exception as e:
        error_msg = f"채팅 템플릿 적용 오류: {str(e)}"
        logger.error(error_msg)
        return {"type": "error", "message": error_msg}
    
    # 샘플링 파라미터 설정
    from vllm import SamplingParams
    sampling_params = SamplingParams(
        max_tokens=config.model.max_new_tokens,
        temperature=config.model.temperature,
        top_k=config.model.top_k,
        top_p=config.model.top_p,
        repetition_penalty=config.model.repetition_penalty,
    )
    
    # vLLM으로 생성
    result_chunks = []
    try:
        # 멀티모달 요청 구성
        generate_request = {
            "prompt": prompt_string,
            "multi_modal_data": {
                "image": [pil_image]
            }
        }

        # 생성 실행
        async for out in model.generate(
            prompt=generate_request,
            sampling_params=sampling_params,
            request_id=request_id,
        ):
            result_chunks.append(out)
            
        logger.info("생성 완료: 청크 수=%d", len(result_chunks))

    except Exception as e:
        error_msg = f"생성 중 오류: {str(e)}"
        logger.error(error_msg)
        return {"type": "error", "message": error_msg}

    # 출력이 없는 경우
    if not result_chunks:
        logger.error("모델 출력 없음")
        return {"type": "error", "message": "모델에서 출력이 없습니다."}

    # 최종 출력 추출
    final_output = next((c for c in result_chunks if getattr(c, "finished", False)), result_chunks[-1])
    answer_text = "".join(piece.text for piece in final_output.outputs)
    logger.info(f"최종 답변: length={len(answer_text)}")
    
    # JSON 형식 추출 시도
    try:
        # JSON 블록 추출
        match = re.search(r'\{.*\}', answer_text, re.DOTALL)
        if match:
            result = json.loads(match.group(0))
        else:
            raise ValueError("JSON 응답 형식 미확인")
            
        if "is_structured" not in result or "description" not in result:
            raise ValueError("응답에 필요한 키가 누락됨")
            
    except Exception as e:
        # 파싱 실패 시 기본 처리
        logger.warning(f"JSON 파싱 실패: {str(e)} - 기본값으로 대체")
        result = {"is_structured": False, "description": http_query.get("qry_contents", "")}
        
    return result

@time_tracker
async def image_streaming_query(
    http_query: Dict[str, Any], 
    model, 
    tokenizer, 
    config
) -> Any:
    """
    이미지 입력을 받아 스트리밍 방식으로 설명 생성
    
    Args:
        http_query: HTTP 요청 데이터
        model: 언어 모델
        tokenizer: 토크나이저
        config: 설정
        
    Yields:
        str: 생성된 부분 텍스트
    """
    logger.info("이미지 스트리밍 쿼리 처리 시작")
    
    # 파라미터 파싱
    request_id = http_query.get("request_id", str(uuid.uuid4()))
    image_input = http_query.get("image_data")
    user_query = http_query.get("qry_contents", "이 이미지를 설명해주세요.")
    
    logger.info(f"요청 정보: request_id={request_id}, user_query='{user_query}'")
    
    # 이미지 변환
    try:
        pil_image = await prepare_image(image_input)
        if not pil_image:
            error_msg = "이미지 로드 실패"
            logger.error(error_msg)
            yield {"type": "error", "message": error_msg}
            return
    except Exception as e:
        error_msg = f"이미지 로드 실패: {str(e)}"
        logger.error(error_msg)
        yield {"type": "error", "message": error_msg}
        return
    
    # 프로세서 로드
    try:
        processor = await prepare_processor(config.model_id)
    except Exception as e:
        error_msg = f"프로세서 로드 실패: {str(e)}"
        logger.error(error_msg)
        yield {"type": "error", "message": error_msg}
        return
    
    # 메시지 구성
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "url": pil_image},
                {"type": "text", "text": user_query},
            ],
        }
    ]
    
    # 채팅 템플릿 적용
    try:
        prompt_string = processor.apply_chat_template(
            messages,
            tokenize=False,
            add_generation_prompt=True,
        )
        logger.info("프롬프트 생성 완료")
    except Exception as e:
        error_msg = f"채팅 템플릿 적용 오류: {str(e)}"
        logger.error(error_msg)
        yield {"type": "error", "message": error_msg}
        return
    
    # 샘플링 파라미터 설정
    from vllm import SamplingParams
    sampling_params = SamplingParams(
        max_tokens=config.model.max_new_tokens,
        temperature=config.model.temperature,
        top_k=config.model.top_k,
        top_p=config.model.top_p,
        repetition_penalty=config.model.repetition_penalty,
    )
    
    # 멀티모달 요청 구성
    generate_request = {
        "prompt": prompt_string,
        "multi_modal_data": {
            "image": [pil_image]
        }
    }
    
    # 스트리밍 생성
    from core.generation import collect_vllm_text_stream
    
    async for partial_chunk in collect_vllm_text_stream(
        generate_request, model, sampling_params, request_id
    ):
        yield partial_chunk

```


--- core/RAG.py

```python

# core/RAG.py
"""
RAG(Retrieval-Augmented Generation) 시스템의 메인 모듈

이 모듈은 다음 핵심 기능들을 제공합니다:
1. 질문 분류 및 구체화 (query_sort, specific_question)
2. RAG 실행 및 조정 (execute_rag)
3. 응답 생성 (generate_answer, generate_answer_stream)

기타 세부 기능들은 하위 모듈로 분리되었습니다:
- retrieval: 문서 검색 관련 기능
- generation: 텍스트 생성 관련 기능
- image_processing: 이미지 처리 관련 기능
- query_processing: 쿼리 처리 관련 기능
- sql_processing: SQL 쿼리 관련 기능
"""

import asyncio
import logging
import re
import uuid
from typing import Dict, List, Tuple, Any, Optional, Union, Generator

# 내부 모듈 임포트
from core.retrieval import retrieve, expand_time_range_if_needed
from core.generation import generate, collect_vllm_text, collect_vllm_text_stream
from core.sql_processing import generate_sql

# SQL 관련 함수 임포트
from core.SQL_NS import run_sql_unno, run_sql_bl, get_metadata

# 프롬프트 템플릿 불러오기
from prompt import (
    QUERY_SORT_PROMPT, 
    GENERATE_PROMPT_TEMPLATE, 
    STREAM_PROMPT_TEMPLATE
)

# 유틸리티 임포트
from utils.tracking import time_tracker

# vLLM 관련 임포트
from vllm import SamplingParams
from vllm.engine.async_llm_engine import AsyncLLMEngine
from vllm.model_executor.models.interfaces import SupportsMultiModal

# Tracking
from utils.debug_tracking import get_performance_monitor

# 로깅 설정
logger = logging.getLogger("RAG")

# 글로벌 구분자
SECTION_SEPARATOR = "-" * 100

@time_tracker
async def execute_rag(
    query: str, 
    keywords: str, 
    needs_table: str, 
    time_range: str, 
    **kwargs
) -> Tuple[str, List[Dict]]:
    """
    RAG(Retrieval-Augmented Generation) 실행의 메인 진입점
    
    Args:
        query: 구체화된 사용자 질문
        keywords: 검색에 사용할 키워드
        needs_table: 테이블 데이터 필요 여부 ("yes" 또는 "no")
        time_range: 검색 시간 범위 ("all" 또는 "시작일:종료일" 형식)
        **kwargs: 추가 파라미터 (model, tokenizer, embed_model, embed_tokenizer, data, config)
    
    Returns:
        Tuple[str, List[Dict]]: (검색된 문서 내용, 문서 메타데이터 리스트)
    """
    logger.info("execute_rag 진입: query='%s', needs_table=%s", query, needs_table)
    
    # 필수 파라미터 추출
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    embed_model = kwargs.get("embed_model")
    embed_tokenizer = kwargs.get("embed_tokenizer")
    data = kwargs.get("data")
    config = kwargs.get("config")
    
    # 테이블 필요 여부 확인
    if needs_table == "yes":
        logger.info("테이블 데이터 필요: SQL 생성 시작")
        try:
            # SQL 생성 및 실행
            result = await generate_sql(query, model, tokenizer, config)
            
            # SQL 실행 결과가 없는 경우
            if result is None:
                logger.warning("SQL 실행 결과 없음")
                docs = "테이블 조회 결과가 비어 있습니다. 조회할 데이터가 없거나 SQL 오류가 발생했습니다."
                docs_list = []
                return docs, docs_list
            
            # SQL 실행 결과 처리
            final_sql_query, title, explain, table_json, chart_json, detailed_result = result
            
            # LLM 입력용 프롬프트 구성
            prompt = (
                f"실제 사용된 SQL문: {final_sql_query}\n\n"
                f"추가 설명: {explain}\n\n"
                f"실제 SQL 추출된 데이터: {str(table_json)}\n\n"
                f"실제 선적된 B/L 데이터: {str(detailed_result)}\n\n"
            )
            
            # 결과 메타데이터 구성
            docs_list = [
                {"title": title, "data": table_json},
                {"title": "DG B/L 상세 정보", "data": detailed_result},
            ]
            
            logger.info("테이블 데이터 처리 완료")
            return prompt, docs_list
            
        except Exception as e:
            logger.error("SQL 처리 중 오류 발생: %s", str(e), exc_info=True)
            docs = f"테이블 조회 시도 중 예외가 발생했습니다. 해당 SQL을 실행할 수 없어서 테이블 데이터를 가져오지 못했습니다. 오류: {str(e)}"
            docs_list = []
            return docs, docs_list
    else:
        logger.info("표준 검색 실행: 키워드='%s', 시간 범위='%s'", keywords, time_range)
        
        # 적응형 시간 필터링 적용
        filtered_data = expand_time_range_if_needed(time_range, data, min_docs=50)
        
        # 문서 검색 실행
        logger.info("검색에 사용되는 문서 수: %d", len(filtered_data.get("vectors", [])))
        docs, docs_list = retrieve(
            keywords, 
            filtered_data, 
            config.N, 
            embed_model, 
            embed_tokenizer
        )
        
        return docs, docs_list

@time_tracker
async def generate_answer(
    query: str, 
    docs: str, 
    **kwargs
) -> str:
    """
    검색된 문서를 기반으로 최종 답변 생성
    
    Args:
        query: 사용자 질문
        docs: 검색된 문서 내용
        **kwargs: 추가 파라미터 (model, tokenizer, config)
    
    Returns:
        str: 생성된 답변
    """
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    config = kwargs.get("config")

    answer = await generate(docs, query, model, tokenizer, config)
    return answer

@time_tracker
async def query_sort(params: Dict[str, Any]) -> Tuple[str, str, str, str]:
    """
    사용자 질문을 분석하여 구체화하고 RAG 파라미터를 추출
    
    Args:
        params: 파라미터 딕셔너리 (user_input, model, tokenizer, embed_model, embed_tokenizer, data, config)
    
    Returns:
        Tuple[str, str, str, str]: (구체화된 질문, 키워드, 테이블 필요 여부, 시간 범위)
    """
    max_attempts = 3
    attempt = 0
    
    while attempt < max_attempts:
        # 필요 파라미터 추출
        query = params["user_input"]
        model = params["model"]
        tokenizer = params["tokenizer"]
        config = params["config"]
        
        # 프롬프트 구성
        prompt = QUERY_SORT_PROMPT.format(user_query=query)
        logger.info("query_sort 시작 (시도 %d)", attempt + 1)
        
        # LLM에서 응답 생성
        if config.use_vllm:
            sampling_params = SamplingParams(
                max_tokens=config.model.max_new_tokens,
                temperature=config.model.temperature,
                top_k=config.model.top_k,
                top_p=config.model.top_p,
                repetition_penalty=config.model.repetition_penalty,
            )
            request_id = str(uuid.uuid4())
            answer = await collect_vllm_text(prompt, model, sampling_params, request_id)
        else:
            input_ids = tokenizer(
                prompt, return_tensors="pt", truncation=True, max_length=4024
            ).to("cuda")
            token_count = input_ids["input_ids"].shape[1]
            outputs = model.generate(
                **input_ids,
                max_new_tokens=config.model.max_new_tokens,
                do_sample=config.model.do_sample,
                temperature=config.model.temperature,
                top_k=config.model.top_k,
                top_p=config.model.top_p,
                repetition_penalty=config.model.repetition_penalty,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id,
            )
            answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)
        
        logger.debug("Generated answer: %s", answer)
        
        # 응답에서 태그로 감싸진 정보 추출
        query_pattern = r"<query.*?>(.*?)<query.*?>"
        keyword_pattern = r"<keyword.*?>(.*?)<keyword.*?>"
        table_pattern = r"<table.*?>(.*?)<table.*?>"
        time_pattern = r"<time.*?>(.*?)<time.*?>"
        
        m_query = re.search(query_pattern, answer, re.DOTALL)
        m_keyword = re.search(keyword_pattern, answer, re.DOTALL)
        m_table = re.search(table_pattern, answer, re.DOTALL)
        m_time = re.search(time_pattern, answer, re.DOTALL)
        
        # 모든 필수 태그가 존재하는 경우
        if m_query and m_keyword and m_table and m_time:
            qu = m_query.group(1).strip()
            ke = m_keyword.group(1).strip()
            ta = m_table.group(1).strip()
            ti = m_time.group(1).strip()
            
            # 'all' 시간 범위 처리
            if ti == "all":
                ti = "1900-01-01:2099-01-01"
                
            logger.info("질문 구체화 결과: 질문='%s', 키워드='%s', 테이블='%s', 시간='%s'", qu, ke, ta, ti)
            return qu, ke, ta, ti
        else:
            logger.error("필요한 태그가 누락됨. 재시도 %d", attempt + 1)
            attempt += 1
    
    # 최대 시도 횟수를 초과한 경우
    raise ValueError("LLM이 올바른 태그 형식의 답변을 생성하지 못했습니다.")

@time_tracker
async def specific_question(params: Dict[str, Any]) -> Tuple[str, str, str, str]:
    """
    대화 이력을 고려한 구체적인 질문 생성
    (query_sort와 유사하지만 대화 이력 처리에 차이가 있을 수 있음)
    
    Args:
        params: 파라미터 딕셔너리
    
    Returns:
        Tuple[str, str, str, str]: (구체화된 질문, 키워드, 테이블 필요 여부, 시간 범위)
    """
    # query_sort와 동일한 로직 사용
    return await query_sort(params)

@time_tracker
async def generate_answer_stream(
    query: str, 
    docs: str, 
    model, 
    tokenizer, 
    config, 
    http_query: Dict
) -> Generator[str, None, None]:
    """
    스트리밍 방식으로 답변 생성
    
    Args:
        query: 사용자 질문
        docs: 검색된 문서 내용
        model: 언어 모델
        tokenizer: 토크나이저
        config: 설정
        http_query: HTTP 요청 정보
    
    Yields:
        str: 생성된 부분 텍스트
    """
    # 프롬프트 구성
    prompt = STREAM_PROMPT_TEMPLATE.format(docs=docs, query=query)
    logger.info("스트리밍 답변 생성 시작: prompt_length=%d", len(prompt))
    
    # 이미지 처리 관련 파라미터
    image_data = http_query.get("image_data")
    pil_image = None
    
    # 요청 ID 추출 (성능 모니터링 용)
    request_id = http_query.get("page_id") or http_query.get("qry_id")
    if not request_id:
        request_id = str(uuid.uuid4())
    
    # 이미지 데이터가 있는 경우 처리
    if image_data:
        try:
            # 이미지 처리 로직은 image_processing 모듈로 이동
            from core.image_processing import prepare_image
            pil_image = await prepare_image(image_data)
            logger.info("이미지 로드 성공: %s", str(pil_image.size if pil_image else None))
        except Exception as e:
            logger.error("이미지 로드 실패: %s", str(e))
    
    # 스트리밍 방식으로 답변 생성
    if config.use_vllm:
        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        
        # 멀티모달 요청 구성
        if pil_image:
            # 멀티모달 처리를 위한 요청 구성
            from core.image_processing import prepare_multimodal_request
            generate_request = await prepare_multimodal_request(prompt, pil_image, config.model_id, tokenizer)
        else:
            generate_request = prompt
        
        # 성능 모니터링 위한 초기화
        from utils.debug_tracking import get_performance_monitor
        perf_monitor = get_performance_monitor()
        if request_id:
            perf_monitor.update_request(
                request_id, 0, 
                checkpoint="start_vllm_generation",
                current_output=""
            )
        
        # 스트리밍 생성
        async for partial_chunk in collect_vllm_text_stream(generate_request, model, sampling_params, request_id):
            # 성능 모니터링 업데이트는 collect_vllm_text_stream 내부에서 수행
            yield partial_chunk
    else:
        # HuggingFace 모델 사용 (비 vLLM)
        import torch
        from transformers import TextIteratorStreamer
        
        input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4024).to("cuda")
        streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
        generation_kwargs = dict(
            **input_ids,
            streamer=streamer,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        
        import threading
        t = threading.Thread(target=model.generate, kwargs=generation_kwargs)
        t.start()
        
        for new_token in streamer:
            yield new_token

# 이 RAG.py 파일은 이제 다른 모듈에서 필요한 함수들만 노출하며,
# 세부 구현은 각각의 특화된 모듈로 이동되었습니다.

```


--- core/retrieval.py

```python

# core/retrieval.py
"""
문서 검색 관련 기능을 제공하는 모듈

주요 기능:
1. 검색어(키워드)에 기반한 문서 검색
2. 검색 결과 스코어링 및 정렬 
3. 시간 기준 필터링 및 범위 조정
"""

import logging
import numpy as np
import torch
import rank_bm25
from datetime import datetime, timedelta
from typing import Dict, List, Tuple, Any, Optional, Union

from utils.tracking import time_tracker

# 로깅 설정
logger = logging.getLogger("RAG.Retrieval")

@time_tracker
def retrieve(
    query: str, 
    data: Dict[str, Any], 
    top_n: int, 
    embed_model, 
    embed_tokenizer
) -> Tuple[str, List[Dict]]:
    """
    주어진 쿼리와 관련된 문서를 검색하고 순위를 매긴 뒤 반환
    
    Args:
        query: 검색 쿼리 또는 키워드
        data: 벡터화된 문서 데이터베이스
        top_n: 반환할 상위 문서 수
        embed_model: 임베딩 모델
        embed_tokenizer: 임베딩 토크나이저
    
    Returns:
        Tuple[str, List[Dict]]: (문서 내용 문자열, 문서 메타데이터 리스트)
    """
    logger.info(f"검색 시작: 쿼리='{query}', 문서 수={len(data['vectors'])}")
    
    try:
        # 유사도 스코어 계산
        sim_score = cal_sim_score(query, data["vectors"], embed_model, embed_tokenizer)
        logger.info(f"유사도 스코어 계산 완료: shape={sim_score.shape}")
        
        # BM25 스코어 계산
        bm25_score = cal_bm25_score(query, data["texts_short"], embed_tokenizer)
        logger.info(f"BM25 스코어 계산 완료: shape={bm25_score.shape}")
        
        # 스코어 정규화
        scaled_sim_score = min_max_scaling(sim_score)
        scaled_bm25_score = min_max_scaling(bm25_score)
        
        # 최종 스코어 계산 (가중치 적용)
        score = scaled_sim_score * 0.4 + scaled_bm25_score * 0.6
        score_values = score[:, 0, 0]
        
        # 상위 N개 문서 선택
        top_k = score[:, 0, 0].argsort()[-top_n:][::-1]
        
        logger.info(f"상위 {top_n}개 문서 인덱스: {top_k}")
        logger.info(f"상위 {top_n}개 문서 스코어: {[score[:, 0, 0][i] for i in top_k]}")
        
        # 문서 컨텐츠와 메타데이터 구성
        documents = ""
        documents_list = []
        
        for i, index in enumerate(top_k):
            score_str = f"{score_values[index]:.4f}"
            # 문서 텍스트 컨텐츠 구성
            documents += f"{i+1}번째 검색자료 (출처:{data['file_names'][index]}) :\n{data['texts_short'][index]}, , Score: {score_str}\n"
            
            # 문서 메타데이터 구성
            documents_list.append({
                "file_name": data["file_names"][index],
                "title": data["titles"][index],
                "contents": data["texts_vis"][index],
                "chunk_id": data["chunk_ids"][index],
            })
        
        logger.info("검색 완료: %d개 문서 반환", len(documents_list))
        return documents, documents_list
        
    except Exception as e:
        logger.error(f"검색 중 오류 발생: {str(e)}", exc_info=True)
        return "", []

@time_tracker
def cal_sim_score(
    query: str, 
    chunks: Union[torch.Tensor, List[torch.Tensor]], 
    embed_model, 
    embed_tokenizer
) -> np.ndarray:
    """
    쿼리와 문서 청크 간의 코사인 유사도 스코어 계산
    
    Args:
        query: 검색 쿼리
        chunks: 문서 청크 임베딩 벡터 (텐서 또는 텐서 리스트)
        embed_model: 임베딩 모델
        embed_tokenizer: 임베딩 토크나이저
        
    Returns:
        np.ndarray: 유사도 스코어 배열
    """
    logger.info(f"유사도 계산 시작: 쿼리='{query}'")
    
    # 쿼리 임베딩
    query_vector = embed(query, embed_model, embed_tokenizer)
    
    if len(query_vector.shape) == 1:
        query_vector = query_vector.unsqueeze(0)
    
    # 각 청크와의 유사도 계산
    score = []
    for chunk in chunks:
        if len(chunk.shape) == 1:
            chunk = chunk.unsqueeze(0)
        
        # 정규화 및 코사인 유사도 계산
        query_norm = query_vector / query_vector.norm(dim=1)[:, None]
        chunk_norm = chunk / chunk.norm(dim=1)[:, None]
        sim_score = torch.mm(query_norm, chunk_norm.transpose(0, 1)) * 100
        score.append(sim_score.detach())
    
    return np.array(score)

@time_tracker
def cal_bm25_score(
    query: str, 
    indexes: List[str], 
    embed_tokenizer
) -> np.ndarray:
    """
    BM25 알고리즘을 사용하여 쿼리와 문서 간의 관련성 스코어 계산
    
    Args:
        query: 검색 쿼리
        indexes: 문서 텍스트 목록
        embed_tokenizer: 토크나이저
        
    Returns:
        np.ndarray: BM25 스코어 배열
    """
    logger.info(f"BM25 계산 시작: 쿼리='{query}', 문서 수={len(indexes)}")
    
    if not indexes:
        logger.warning("BM25에 빈 문서 리스트가 제공됨")
        return np.zeros(0)
    
    # 토큰화
    tokenized_corpus = []
    for i, text in enumerate(indexes):
        try:
            tokens = embed_tokenizer(
                text,
                return_token_type_ids=False,
                return_attention_mask=False,
                return_offsets_mapping=False,
            )
            tokens = embed_tokenizer.convert_ids_to_tokens(tokens["input_ids"])
            
            if len(tokens) == 0:
                logger.warning(f"문서 {i}가 빈 토큰 리스트로 토큰화됨")
                tokens = ["<empty>"]
                
            tokenized_corpus.append(tokens)
            
        except Exception as e:
            logger.error(f"문서 {i} 토큰화 실패: {str(e)}")
            tokenized_corpus.append(["<error>"])
    
    # BM25 계산
    try:
        bm25 = rank_bm25.BM25Okapi(tokenized_corpus)
        tokenized_query = embed_tokenizer.convert_ids_to_tokens(embed_tokenizer(query)["input_ids"])
        scores = bm25.get_scores(tokenized_query)
        
        # NaN/Inf 값 처리
        if np.isnan(scores).any() or np.isinf(scores).any():
            logger.warning("BM25에서 NaN/Inf 스코어 발생 - 0으로 대체")
            scores = np.nan_to_num(scores)
        
        logger.info(
            f"BM25 스코어: min={scores.min():.4f}, max={scores.max():.4f}, mean={scores.mean():.4f}"
        )
        return scores
        
    except Exception as e:
        logger.error(f"BM25 계산 실패: {str(e)}")
        return np.zeros(len(indexes))

@time_tracker
def embed(query: str, embed_model, embed_tokenizer) -> torch.Tensor:
    """
    텍스트를 임베딩 벡터로 변환
    
    Args:
        query: 임베딩할 텍스트
        embed_model: 임베딩 모델
        embed_tokenizer: 임베딩 토크나이저
        
    Returns:
        torch.Tensor: 임베딩 벡터
    """
    logger.info(f"임베딩 시작: '{query}'")
    
    inputs = embed_tokenizer(query, padding=True, truncation=True, return_tensors="pt")
    embeddings, _ = embed_model(**inputs, return_dict=False)
    
    logger.info("임베딩 완료")
    return embeddings[0][0]

@time_tracker
def min_max_scaling(arr: np.ndarray) -> np.ndarray:
    """
    배열을 0-1 범위로 정규화
    
    Args:
        arr: 정규화할 배열
        
    Returns:
        np.ndarray: 정규화된 배열
    """
    arr_min = arr.min()
    arr_max = arr.max()
    
    if arr_max == arr_min:
        logger.warning("정규화 범위가 0 - 0으로 들어옴")
        return np.zeros_like(arr)
        
    return (arr - arr_min) / (arr_max - arr_min)

@time_tracker
def sort_by_time(time_bound: str, data: Dict[str, Any]) -> Dict[str, Any]:
    """
    시간 범위에 따라 데이터 필터링
    
    Args:
        time_bound: 시간 범위 ("all" 또는 "시작일:종료일" 형식)
        data: 원본 데이터
        
    Returns:
        Dict[str, Any]: 필터링된 데이터
    """
    original_count = len(data["times"])
    logger.info(f"시간 필터링 전 문서 수: {original_count}")
    
    # 전체 기간 선택 시 원본 반환
    if time_bound == "all" or time_bound == "1900-01-01:2099-01-01":
        logger.info("전체 기간 사용 - 모든 문서 포함")
        return data
    
    # 시간 범위 파싱
    date_format = "%Y-%m-%d"
    target_date_start = datetime.strptime(time_bound.split(":")[0], date_format)
    target_date_end = datetime.strptime(time_bound.split(":")[1], date_format)
    
    # 시간 범위에 맞는 인덱스 찾기
    matching_indices = [
        i
        for i, date in enumerate(data["times"])
        if (not isinstance(date, str)) and (target_date_start < date < target_date_end)
    ]
    
    filtered_count = len(matching_indices)
    logger.info(f"시간 필터링 후 문서 수: {filtered_count}, 기간: {time_bound}")
    
    # 필터링 결과가 너무 적은 경우 경고
    if filtered_count < 50 and filtered_count < original_count * 0.1:
        logger.warning(f"시간 필터로 인해 문서가 크게 줄었습니다: {original_count} → {filtered_count}")
    
    # 필터링된 데이터 구성
    filtered_data = {}
    filtered_data["file_names"] = [data["file_names"][i] for i in matching_indices]
    filtered_data["titles"] = [data["titles"][i] for i in matching_indices]
    filtered_data["times"] = [data["times"][i] for i in matching_indices]
    filtered_data["chunk_ids"] = [data["chunk_ids"][i] for i in matching_indices]
    
    # 벡터 데이터 처리
    if isinstance(data["vectors"], torch.Tensor):
        filtered_data["vectors"] = data["vectors"][matching_indices]
    else:
        filtered_data["vectors"] = [data["vectors"][i] for i in matching_indices]
    
    # 텍스트 데이터 처리
    filtered_data["texts"] = [data["texts"][i] for i in matching_indices]
    filtered_data["texts_short"] = [data["texts_short"][i] for i in matching_indices]
    filtered_data["texts_vis"] = [data["texts_vis"][i] for i in matching_indices]
    
    return filtered_data

@time_tracker
def expand_time_range_if_needed(
    time_bound: str, 
    data: Dict[str, Any], 
    min_docs: int = 50
) -> Dict[str, Any]:
    """
    검색 결과가 너무 적은 경우 시간 범위를 자동으로 확장
    
    Args:
        time_bound: 시간 범위
        data: 원본 데이터
        min_docs: 최소 필요 문서 수
        
    Returns:
        Dict[str, Any]: 필터링된 데이터 (필요시 확장된 범위 적용)
    """
    # 전체 기간 선택 시 원본 반환
    if time_bound == "all" or time_bound == "1900-01-01:2099-01-01":
        logger.info("전체 기간 사용")
        return data
    
    # 초기 필터링 시도
    filtered_data = sort_by_time(time_bound, data)
    filtered_count = len(filtered_data.get("times", []))
    
    # 충분한 문서가 있으면 그대로 반환
    if filtered_count >= min_docs:
        logger.info(f"원래 범위로 충분한 문서 확보: {filtered_count}개")
        return filtered_data
    
    logger.info(f"문서 수 부족: {filtered_count}개 (최소 필요: {min_docs}개) - 범위 확장 시도")
    
    # 시간 범위 파싱
    date_format = "%Y-%m-%d"
    try:
        start_date = datetime.strptime(time_bound.split(":")[0], date_format)
        end_date = datetime.strptime(time_bound.split(":")[1], date_format)
    except Exception as e:
        logger.error(f"날짜 형식 오류: {time_bound}, 오류: {e}")
        return data
    
    # 단계적으로 범위 확장 시도
    expansions = [
        (3, "3개월"),
        (6, "6개월"),
        (12, "1년"),
        (24, "2년"),
        (60, "5년"),
    ]
    
    for months, label in expansions:
        # 확장된 범위 계산
        new_start = start_date - timedelta(days=30 * months // 2)
        new_end = end_date + timedelta(days=30 * months // 2)
        
        new_range = f"{new_start.strftime(date_format)}:{new_end.strftime(date_format)}"
        logger.info(f"시간 범위 {label} 확장 시도: {new_range}")
        
        # 확장된 범위로 필터링
        expanded_data = sort_by_time(new_range, data)
        expanded_count = len(expanded_data.get("times", []))
        
        # 충분한 문서를 찾았으면 반환
        if expanded_count >= min_docs:
            logger.info(f"{label} 확장으로 {expanded_count}개 문서 확보")
            return expanded_data
    
    # 모든 확장 시도 실패 시 전체 데이터 반환
    logger.warning("모든 확장 시도 실패, 전체 데이터셋 사용")
    return data

```


-----------------

# Base-Knowledge

 - 위 파일들은 LLM 모델을 활용한 사내 RAG 서비스의 소스 코드입니다.
 - 파일 트리와 각 파일의 내용이 코드 블록 내에 포함되어, 프로젝트의 현재 구조와 상태를 한눈에 파악할 수 있습니다.
 - vLLM과 ray를 활용하여 사용성 및 추론 성능을 개선하였습니다.
 - Langchain을 활용하여 reqeust_id별로 대화를 저장하고 활용할 수 있습니다.
 - 에러 발생 시 로깅을 통해 문제를 추적할 수 있도록 설계되었습니다.


# Answer-Rule

 1. 추후 소스 코드 개선, 구조 변경, 에러 로그 추가 등 다양한 요구사항을 반영할 수 있는 확장성을 고려합니다.
 2. 전체 코드는 한국어로 주석 및 설명이 포함되어, 이해와 유지보수가 용이하도록 작성됩니다.


# My-Requirements

 1. User requirements.
 2. My requirements.
