# Project Tree of RAG for company

```
├─ app.py
├─ config.yaml
├─ core/RAG.py
├─ ray_deploy/ray_utils.py
└─ utils/utils_load.py
```

--- config.yaml

```yaml

# config.yaml
# Server : 2x H100 (80 GB SXM5), 52 CPU cores, 483.2 GB RAM, 6 TB SSD
### Model
model_id : 'google/gemma-3-27b-it'
response_url : "http://202.20.84.16:8083/responseToUI"
# response_url : "https://eo5smhcazmp1bqe.m.pipedream.net"

### ray
ray:
  actor_count: 1                  # 총 Actor 개수(same as num_replicas)
  num_gpus: 1                     # 각 Actor(Node)가 점유하고 있는 GPU 갯수
  num_cpus: 24                    # 각 Actor(Node)가 점유하고 있는 CPU 갯수 (1 actor 시에 gpu 48개, 2 actor 시에 gpu 24개 할당)
  max_batch_size: 5               # max_concurrency(actor 최대 동시 처리량, default 1000)로 대체해도 됨
  batch_wait_timeout: 0.05        
  max_ongoing_requests: 100       # ray.serve에서 deployment setting으로 동시 요청 처리 갯수를 의미함(Batch랑 다름)

### vllm
use_vllm: True                    # vLLM 사용 여부
vllm:
  enable_prefix_caching: True
  scheduler_delay_factor: 0.1
  enable_chunked_prefill: True
  tensor_parallel_size: 1         # vLLM의 GPU 사용 갯수 (!!!! num_gpus 보다 작아야 함 !!!!)
  max_num_seqs: 128               # v1에 따른 상향
  max_num_batched_tokens: 34000   # v1에 따른 상향
  block_size: 128                 # 미적용
  gpu_memory_utilization: 0.99    # v0: 0.95 / v1: 0.99로 상향
  ### 모델 변경에 따른 추가된 설정
  max_model_len: 20000            # For the new model (Gemma2 : 8192) / Gemma3는 1xH100(SXM5)일 경우 최대 22000~23000, so that 20000으로 세팅
  ### v1에 따른 새로운 인자값
  disable_custom_all_reduce: true
  # enable_memory_defrag: True      # Gemma3 이식 작업에서 도입, 현재 미사용
  # disable_sliding_window: True    # Gemma3 이식 작업에서 도입, 현재 미사용, sliding window 비활성화 - cascade attention과 충돌이 나서 이를 비활성화
  ### Gemma3 - Multi Modal 이미지 기능에 따른 새로운 인자값
  mm_processor_kwargs:            # 이미지 처리 시 사용되는 추가 인자 정의
    do_pan_and_scan: True         # 이미지 내 객체 감지 및 영역 스캔 기능을 활성화
  disable_mm_preprocessor_cache: False # 이미지 전처리 캐시를 비활성화 할지 여부
  limit_mm_per_prompt:            # 프롬프트 당 허용되는 멀티모달(예, 이미지) 입력의 최대 개수
    image: 2                      # 최대 image 처리 개수

### model huggingface setting and sampling params
model:
  quantization_4bit : False       # Quantize 4-bit
  quantization_8bit : False       # Quantize 8-bit
  max_new_tokens : 2048           # 생성할 최대 토큰 수
  do_sample : False               # True 일때만 아래가 적용
  temperature : 1.0               # 텍스트 다양성 조정: 높을수록 창의력 향상 (1.0)
  top_k : 30                      # top-k 샘플링: 상위 k개의 후보 토큰 중 하나를 선택 (50)
  top_p : 1.0                     # top-p 샘플링: 누적 확률을 기준으로 후보 토큰을 선택 (1.0 보다 낮을수록 창의력 증가)
  repetition_penalty : 1.0        # 같은 단어를 반복해서 출력하지 않도록 패널티를 부여 (1.0 보다 클수록 페널티 증가)
embed_model_id : 'BM-K/KoSimCSE-roberta-multitask'
# cache_dir : "D:/huggingface" # Windows Local
# cache_dir : "/media/user/7340afbb-e4ce-4a38-8210-c6362e85eae7/RAG/RAG_application/huggingface" # Local
cache_dir : "/workspace/huggingface"  # Docker

### Data
data_path : '/workspace/data/0228_DB_.json'     # VectorDB Path - New one (계약서 데이터 포함)
# data_path : 'data/1104_NS_DB_old.json' # VectorDB Path - Old one
metadata_path : '/workspace/data/Metadata.json' # Metadata.json Path
metadata_unno : '/workspace/data/METADATA_OPRAIMDG.json'
sql_data_path : '/workspace/data/poc.db'        # SQLite 데이터베이스 Path

### Retrieve
N : 5 # Retrieve top N chunks

### Others
beep : '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------'
seed : 4734                     # Radom Seed
k : 15                          # SQL Max Rows (None=MAX)

```


--- app.py

```python

import os
# Setting environment variable
# os.environ["TRANSFORMERS_CACHE"] = "/workspace/huggingface"
os.environ["HF_HOME"] = "/workspace/huggingface"
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# For the Huggingface Token setting
os.environ["HF_TOKEN_PATH"] = "/root/.cache/huggingface/token"
# Change to GNU to using OpenMP. Because this is more friendly with CUDA(NVIDIA),
# and Some library(Pytorch, Numpy, vLLM etc) use the OpenMP so that set the GNU is better.
# OpenMP: Open-Multi-Processing API
os.environ["MKL_THREADING_LAYER"] = "GNU"
# Increase download timeout (in seconds)
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "60"
# Use the vLLM as v1 version
os.environ["VLLM_USE_V1"] = "1"
os.environ["VLLM_STANDBY_MEM"] = "0"
os.environ["VLLM_METRICS_LEVEL"] = "1"
os.environ["VLLM_PROFILE_MEMORY"]= "1"
# GPU 단독 사용(박상제 연구원님이랑 분기점 - 연구원님 0번 GPU, 수완 1번 GPU - 기본 안정화 세팅은 0번 GPU)
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

# 토크나이저 병렬 처리 명시적 비활성화
os.environ["TOKENIZERS_PARALLELISM"] = "false"

print("[[TEST]]")

from flask import (
    Flask,
    request,
    Response,
    render_template,
    jsonify,
    g,
    stream_with_context,
)
import json
import yaml
from box import Box
from utils import random_seed, error_format, send_data_to_server, process_format_to_response
from datetime import datetime

# Import the Ray modules
from ray_deploy import init_ray, InferenceActor, InferenceService, SSEQueueManager

# ------ checking process of the thread level
import logging
import threading

# 로깅 설정: 요청 처리 시간과 현재 스레드 이름을 기록
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s %(levelname)s [%(threadName)s] %(message)s'
)

import ray
from ray import serve
import uuid
import asyncio
import time

# Configuration
with open("./config.yaml", "r") as f:
    config_yaml = yaml.load(f, Loader=yaml.FullLoader)
    config = Box(config_yaml)
random_seed(config.seed)

########## Ray Dashboard 8265 port ##########
init_ray()  # Initialize the Ray
sse_manager = SSEQueueManager.options(name="SSEQueueManager").remote()
serve.start(detached=True)

#### Ray-Actor 다중 ####
inference_service = InferenceService.options(num_replicas=config.ray.actor_count).bind(config)
serve.run(inference_service)
inference_handle = serve.get_deployment_handle("inference", app_name="default")

#### Ray-Actor 단독 ####
# inference_actor = InferenceActor.options(num_cpus=config.ray.num_cpus, num_gpus=config.ray.num_gpus).remote(config)

########## FLASK APP setting ##########
app = Flask(__name__)
content_type = "application/json; charset=utf-8"

# 기본 페이지를 불러오는 라우트
@app.route("/")
def index():
    return render_template("index.html")  # index.html을 렌더링

# Test 페이지를 불러오는 라우트
@app.route("/test")
def test_page():
    return render_template("index_test.html")

# chatroomPage 페이지를 불러오는 라우트
@app.route("/chat")
def chat_page():
    return render_template("chatroom.html")

# data 관리
from data_control.data_control import data_control_bp
app.register_blueprint(data_control_bp, url_prefix="/data")

# --------------------- Non Streaming part ----------------------------
@app.route("/query", methods=["POST"])
async def query():
    try:
        # Log when the query is received
        receive_time = datetime.now().isoformat()
        print(f"[APP] Received /query request at {receive_time}")
        
        # Optionally, attach the client time if desired:
        http_query = request.json  # 클라이언트로부터 JSON 요청 수신
        
        http_query["server_receive_time"] = receive_time
        
        # Ray Serve 배포된 서비스를 통해 추론 요청 (자동으로 로드밸런싱됨)
        # result = await inference_actor.process_query.remote(http_query) # 단일
        result = await inference_handle.query.remote(http_query) # 다중
        if isinstance(result, dict):
            result = json.dumps(result, ensure_ascii=False)
        print("APP.py - 결과: ", result)
        return Response(result, content_type=content_type)
    except Exception as e:
        error_resp = error_format(f"서버 처리 중 오류 발생: {str(e)}", 500)
        return Response(error_resp, content_type=content_type)

# --------------------- Streaming part ----------------------------
@app.route("/query_stream", methods=["POST"])
def query_stream():
    """
    POST 방식 SSE 스트리밍 엔드포인트.
    클라이언트가 아래 필드들을 포함한 JSON을 보내면:
      - qry_id, user_id, page_id, auth_class, qry_contents, qry_time
    auth_class는 내부적으로 'admin'으로 통일합니다.
    """
    body = request.json or {}
    # 새로운 필드 추출
    qry_id = body.get("qry_id")
    user_id = body.get("user_id")
    page_id = body.get("page_id")
    auth_class = "admin"  # 어떤 값이 와도 'admin'으로 통일
    qry_contents = body.get("qry_contents", "")
    qry_time = body.get("qry_time")  # 클라이언트 측 타임스탬프

    print(f"[DEBUG] /query_stream called with qry_id='{qry_id}', user_id='{user_id}', page_id='{page_id}', qry_contents='{qry_contents}', qry_time='{qry_time}'")
    
    # 새로운 http_query 생성 – 내부 로직에서는 page_id를 채팅방 id로 사용
    http_query = {
        "qry_id": qry_id,
        "user_id": user_id,
        "page_id": page_id if page_id else str(uuid.uuid4()),
        "auth_class": auth_class,
        "qry_contents": qry_contents,
        "qry_time": qry_time
    }
    
    # 기존 request_id 대신 page_id를 SSE queue key로 사용
    print(f"[DEBUG] Built http_query: {http_query}")
    
    # Ray Serve를 통한 streaming 호출 (변경 없음, 내부 인자는 수정된 http_query)
    response = inference_handle.process_query_stream.remote(http_query)
    obj_ref = response._to_object_ref_sync()
    chat_id = ray.get(obj_ref)  # chat_id는 page_id
    print(f"[DEBUG] streaming chat_id={chat_id}")
    
    def sse_generator():
        try:
            while True:
                # SSEQueueManager에서 토큰을 가져옴 (chat_id 사용)
                token = ray.get(sse_manager.get_token.remote(chat_id, 120))
                if token is None or token == "[[STREAM_DONE]]":
                    break
                yield f"data: {token}\n\n"
        except Exception as e:
            error_token = json.dumps({"type": "error", "message": str(e)})
            yield f"data: {error_token}\n\n"
        finally:
            try:
                obj_ref = inference_handle.close_sse_queue.remote(chat_id)._to_object_ref_sync()
                ray.get(obj_ref)
            except Exception as ex:
                print(f"[DEBUG] Error closing SSE queue for {chat_id}: {str(ex)}")
            print("[DEBUG] SSE closed.")

    return Response(sse_generator(), mimetype="text/event-stream")

# --------------------- CLT Streaming part ----------------------------
@app.route("/queryToSLLM", methods=["POST"])
def query_stream_to_clt():
    """
    POST 방식 SSE 스트리밍 엔드포인트.
    클라이언트가 {"qry_id": "...", "user_id": "...", "page_id": "...", "qry_contents": "...", "qry_time": "..." }
    형태의 JSON을 보내면, 내부 Ray Serve SSE 스트림을 통해 처리한 후 지정된 response_url로 SSE 청크를 전송합니다.
    """
    # POST 요청 파라미터 파싱
    body = request.json or {}
    qry_id = body.get("qry_id", "")
    user_id = body.get("user_id", "")
    page_id = body.get("page_id", "")
    auth_class = "admin"  # 모든 요청을 'admin'으로 처리
    user_input = body.get("qry_contents", "")
    qry_time = body.get("qry_time", "")
    
    response_url = config.response_url

    print(f"[DEBUG] /queryToSLLM called with qry_id='{qry_id}', user_id='{user_id}', "
          f"page_id='{page_id}', qry_contents='{user_input}', qry_time='{qry_time}', url={response_url}")
    
    # 내부 로직에서는 page_id를 채팅방 ID(또는 request_id)로 사용합니다.
    http_query = {
        "qry_id": qry_id,
        "user_id": user_id,
        "page_id": page_id if page_id else str(uuid.uuid4()),
        "auth_class": auth_class,
        "qry_contents": user_input,
        "qry_time": qry_time,
        "response_url": response_url
    }
    print(f"[DEBUG] Built http_query={http_query}")

    # Ray Serve에 SSE 스트리밍 요청 보내기
    response = inference_handle.process_query_stream.remote(http_query)
    obj_ref = response._to_object_ref_sync()
    request_id = ray.get(obj_ref)
    print(f"[DEBUG] streaming request_id={request_id}")
    
    def sse_generator(request_id, response_url):
        token_buffer = []  # To collect tokens (for answer tokens only)
        last_sent_time = time.time()  # To track the last time data was sent
        answer_counter = 1  # 답변 업데이트 순번
        try:
            while True:
                token = ray.get(sse_manager.get_token.remote(request_id, 120))
                if token is None:
                    print("[DEBUG] 토큰이 None 반환됨. 종료합니다.")
                    break

                if isinstance(token, str):
                    token = token.strip()
                if token == "[[STREAM_DONE]]":
                    print("[DEBUG] 종료 토큰([[STREAM_DONE]]) 수신됨. 스트림 종료.")
                    break

                try:
                    token_dict = json.loads(token) if isinstance(token, str) else token
                except Exception as e:
                    print(f"[ERROR] JSON 파싱 실패: {e}. 원시 토큰: '{token}'")
                    continue

                # If token is a reference token, send it immediately
                if token_dict.get("type") == "reference":
                    print(f"[DEBUG] Reference token details: {token_dict}")
                    ref_format = process_format_to_response([token_dict], qry_id, continue_="C", update_index=answer_counter)
                    print(f"[DEBUG] Sending reference data: {json.dumps(ref_format, ensure_ascii=False, indent=2)}")
                    send_data_to_server(ref_format, response_url)
                    continue

                # Otherwise, accumulate answer tokens
                token_buffer.append(token_dict)
                current_time = time.time()
                # If 1 second has passed, flush the accumulated answer tokens
                if current_time - last_sent_time >= 1:
                    if len(token_buffer) > 0:
                        # Check if any token in the buffer signals termination.
                        final_continue = "E" if any(t.get("continue") == "E" for t in token_buffer) else "C"
                        print(f"[DEBUG] Flushing {len(token_buffer)} tokens with continue flag: {final_continue}")
                        buffer_format = process_format_to_response(token_buffer, qry_id, continue_=final_continue, update_index=answer_counter)
                        send_data_to_server(buffer_format, response_url)
                        token_buffer = []  # Reset the buffer
                        last_sent_time = current_time  # Update the last sent time
                        answer_counter += 1
                if token_dict.get("continue") == "E":
                    # Immediately flush the buffer with termination flag if needed
                    if len(token_buffer) > 0:
                        print(f"[DEBUG] Immediate flush due to termination flag in buffer (size {len(token_buffer)}).")
                        buffer_format = process_format_to_response(token_buffer, qry_id, continue_="E", update_index=answer_counter)
                        send_data_to_server(buffer_format, response_url)
                        token_buffer = []
                    break
            # After loop: if tokens remain, flush them with termination flag
            if len(token_buffer) > 0:
                print(f"[DEBUG] Final flush of remaining {len(token_buffer)} tokens with end flag.")
                buffer_format = process_format_to_response(token_buffer, qry_id, continue_="E", update_index=answer_counter)
                send_data_to_server(buffer_format, response_url)
        except Exception as e:
            print(f"[ERROR] sse_generator encountered an error: {e}")
        finally:
            try:
                obj_ref = inference_handle.close_sse_queue.remote(request_id)._to_object_ref_sync()
                ray.get(obj_ref)
            except Exception as ex:
                print(f"[DEBUG] Error closing SSE queue for {request_id}: {str(ex)}")
            print("[DEBUG] SSE closed.")

    
    # 별도의 스레드에서 SSE generator 실행
    job = threading.Thread(target=sse_generator, args=(request_id, response_url), daemon=False)
    job.start()

    # 클라이언트에는 즉시 "수신양호" 메시지를 JSON 형식으로 응답
    return Response(error_format("수신양호", 200, qry_id), content_type="application/json")

# --------------------- History & Reference part ----------------------------

# 새로 추가1: request_id로 대화 기록을 조회하는 API 엔드포인트
@app.route("/history", methods=["GET"])
def conversation_history():
    request_id = request.args.get("request_id", "")
    last_index = request.args.get("last_index")
    if not request_id:
        error_resp = error_format("request_id 파라미터가 필요합니다.", 400)
        return Response(error_resp, content_type="application/json; charset=utf-8")
    
    try:
        last_index = int(last_index) if last_index is not None else None
        response = inference_handle.get_history.remote(request_id, last_index=last_index)
        # DeploymentResponse를 ObjectRef로 변환
        obj_ref = response._to_object_ref_sync()
        history_data = ray.get(obj_ref)
        return jsonify(history_data)
    except Exception as e:
        print(f"[ERROR /history] {e}")
        error_resp = error_format(f"대화 기록 조회 오류: {str(e)}", 500)
        return Response(error_resp, content_type="application/json; charset=utf-8")

# 새로 추가2: request_id로 해당 답변의 참고자료를 볼 수 있는 API
@app.route("/reference", methods=["GET"])
def get_reference():
    request_id = request.args.get("request_id", "")
    msg_index_str = request.args.get("msg_index", "")
    if not request_id or not msg_index_str:
        error_resp = error_format("request_id와 msg_index 파라미터가 필요합니다.", 400)
        return Response(error_resp, content_type="application/json; charset=utf-8")
    
    try:
        msg_index = int(msg_index_str)
        # 먼저 history를 가져옴
        response = inference_handle.get_history.remote(request_id)
        obj_ref = response._to_object_ref_sync()
        history_data = ray.get(obj_ref)
        
        history_list = history_data.get("history", [])
        if msg_index < 0 or msg_index >= len(history_list):
            return jsonify({"error": "유효하지 않은 메시지 인덱스"}), 400
        
        message = history_list[msg_index]
        if message.get("role") != "ai":
            return jsonify({"error": "해당 메시지는 AI 응답이 아닙니다."}), 400
        
        chunk_ids = message.get("references", [])
        if not chunk_ids:
            return jsonify({"references": []})
        
        # chunk_ids에 해당하는 실제 참조 데이터 조회
        ref_response = inference_handle.get_reference_data.remote(chunk_ids)
        ref_obj_ref = ref_response._to_object_ref_sync()
        references = ray.get(ref_obj_ref)
        return jsonify({"references": references})
    except Exception as e:
        print(f"[ERROR /reference] {e}")
        error_resp = error_format(f"참조 조회 오류: {str(e)}", 500)
        return Response(error_resp, content_type="application/json; charset=utf-8")
    
@app.route("/image_query", methods=["POST"])
async def image_query():
    """
    JSON 예시:
    {
        "image_data": "base64....",
        "qry_contents": "이미지 관련 질문",
        "request_id": "some_id"
    }
    """
    try:
        http_query = request.json
        # Ray Actor(inference_handle)로 전달
        result_ref = inference_handle.image_query.remote(http_query)
        result = await result_ref
        return jsonify(result)
    except Exception as e:
        return jsonify({"error": str(e)}), 500

# Flask app 실행
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=False)

```


--- ray_deploy/ray_utils.py

```python


# ray_deploy/ray_utils.py
import ray  # Ray library
from ray import serve
import json
import asyncio  # async I/O process module
from concurrent.futures import ProcessPoolExecutor  # 스레드 컨트롤
import uuid  # --- NEW OR MODIFIED ---
import time
from typing import Dict, Optional  # --- NEW OR MODIFIED ---
import threading  # To find out the usage of thread
import datetime

from core.RAG import (
    query_sort,
    specific_question,
    execute_rag,
    generate_answer,
    generate_answer_stream,
)  # hypothetically
from utils import (
    load_model,
    load_data,
    process_format_to_response,
    process_to_format,
    error_format,
)
# from summarizer import summarize_conversation
from utils.summarizer import summarize_conversation
from utils.debug_tracking import log_batch_info, log_system_info
# Langchain Memory system
from ray_deploy.langchain import CustomConversationBufferMemory

@ray.remote  # From Decorator, Each Actor is allocated 1 GPU
class InferenceActor:
    async def __init__(self, config):
        self.config = config
        # 액터 내부에서 모델 및 토크나이저를 새로 로드 (GPU에 한 번만 로드)
        self.model, self.tokenizer, self.embed_model, self.embed_tokenizer = load_model(
            config
        )
        # 데이터는 캐시 파일을 통해 로드
        self.data = load_data(config.data_path)
        
        # 비동기 큐와 배치 처리 설정
        self.request_queue = asyncio.Queue()
        self.max_batch_size = config.ray.max_batch_size  # 최대 배치 수
        self.batch_wait_timeout = config.ray.batch_wait_timeout  # 배치당 처리 시간 - 이제 사용하지 아니함(연속 배치이기에 배치당 처리 시간이 무의미)

        # Actor 내부에서 ProcessPoolExecutor 생성 (직렬화 문제 회피)
        max_workers = int(min(config.ray.num_cpus * 0.8, (26*config.ray.actor_count)-4))
        self.process_pool = ProcessPoolExecutor(max_workers)

        # --- SSE Queue Manager --- 
        # Key = request_id, Value = an asyncio.Queue of partial token strings
        # A dictionary to store SSE queues for streaming requests
        self.queue_manager = ray.get_actor("SSEQueueManager")
        self.active_sse_queues: Dict[str, asyncio.Queue] = {}

        self.batch_counter = 0  # New counter to track batches

        self.memory_map = {}
        
        # 활성 작업 추적을 위한 변수 추가
        self.active_tasks = set()
        self.max_concurrent_tasks = config.ray.max_batch_size
        
        # 연속 배치 처리기 시작 (Continuous batch)
        asyncio.create_task(self.continuous_batch_processor())
        
    # -------------------------------------------------------------------------
    # GET MEMORY FOR SESSION
    # -------------------------------------------------------------------------
    def get_memory_for_session(self, request_id: str) -> CustomConversationBufferMemory:
        """
        세션별 Memory를 안전하게 가져오는 헬퍼 메서드.
        만약 memory_map에 request_id가 없으면 새로 생성해 저장 후 반환.
        """
        if request_id not in self.memory_map:
            print(f"[DEBUG] Creating new CustomConversationBufferMemory for session={request_id}")
            self.memory_map[request_id] = CustomConversationBufferMemory(return_messages=True)
        return self.memory_map[request_id]
    # -------------------------------------------------------------------------
    # CONTINUOUOS BATCH PROCESSOR
    # -------------------------------------------------------------------------
    async def continuous_batch_processor(self):
        """
        연속적인 배치 처리 - 최대 max_batch_size개의 요청이 항상 동시에 처리되도록 함
        """
        while True:
            # 사용 가능한 슬롯 확인
            available_slots = self.max_concurrent_tasks - len(self.active_tasks)
            
            if available_slots > 0:
                try:
                    # 새 요청 받기 (짧은 타임아웃으로 non-blocking 유지)
                    request_tuple = await asyncio.wait_for(
                        self.request_queue.get(), 
                        timeout=0.01
                    )
                    
                    # 비동기 처리 작업 생성
                    request_obj, fut, sse_queue = request_tuple
                    task = asyncio.create_task(
                        self._process_single_query(request_obj, fut, sse_queue)
                    )
                    
                    # 작업 완료 시 활성 목록에서 제거하는 콜백 설정
                    task.add_done_callback(
                        lambda t, task_ref=task: self.active_tasks.discard(task_ref)
                    )
                    
                    # 활성 작업 목록에 추가
                    self.active_tasks.add(task)
                    
                    print(f"[Continuous Batching] +1 request => active tasks now {len(self.active_tasks)}/{self.max_concurrent_tasks}")
                
                except asyncio.TimeoutError:
                    # 새 요청이 없으면 잠시 대기
                    await asyncio.sleep(0.01)
            else:
                # 모든 슬롯이 사용 중이면 작업 완료될 때까지 대기
                if self.active_tasks:
                    # 작업 중 하나라도 완료될 때까지 대기
                    done, pending = await asyncio.wait(
                        self.active_tasks, 
                        return_when=asyncio.FIRST_COMPLETED
                    )
                    
                    # 완료된 작업 확인
                    for task in done:
                        try:
                            await task
                        except Exception as e:
                            print(f"[ERROR] Task failed: {e}")
                    
                    print(f"[Continuous Batching] Tasks completed: {len(done)} => active tasks now {len(self.active_tasks)}/{self.max_concurrent_tasks}")
                else:
                    await asyncio.sleep(0.01)

    # -------------------------------------------------------------------------
    # PROCESS SINGLE QUERY (TEXT TO TEXT)
    # -------------------------------------------------------------------------
    async def _process_single_query(self, http_query_or_stream_dict, future, sse_queue):
        """
        Process a single query from the micro-batch. If 'sse_queue' is given,
        we do partial-token streaming. Otherwise, normal final result.
        """
        # 스트리밍 요청인 경우 request_id를 미리 초기화
        request_id = None
        print(
            f"[DEBUG] _process_single_query 시작: {time.strftime('%H:%M:%S')}, 요청 내용: {http_query_or_stream_dict}, 현재 스레드: {threading.current_thread().name}"
        )
        try:
            # 1) 스트리밍 구분
            if (isinstance(http_query_or_stream_dict, dict)
                and "request_id" in http_query_or_stream_dict):
                # 스트리밍
                request_id = http_query_or_stream_dict["request_id"]
                http_query = http_query_or_stream_dict["http_query"]
                is_streaming = True
                print(f"[STREAM] _process_single_query: request_id={request_id}")
            else:
                # Non-스트리밍
                request_id = None
                http_query = http_query_or_stream_dict
                is_streaming = False
                print("[NORMAL] _process_single_query started...")
                
            # 2) Memory 객체 정보 가져오기 (없으면 새로 생성)
            page_id = http_query.get("page_id", request_id)
            memory = self.get_memory_for_session(page_id)

            # 3) 유저가 현재 입력한 쿼리 가져오기
            user_input = http_query.get("qry_contents", "")
            
            # 4) LangChain Memory에서 이전 대화 이력(history) 추출
            past_context = memory.load_memory_variables({}).get("history", [])
            # history가 리스트 형식인 경우 (각 메시지가 별도 항목으로 저장되어 있다면)
            if isinstance(past_context, list):
                recent_messages = [msg if isinstance(msg, str) else msg.content for msg in past_context[-5:]]
                past_context = "\n\n".join(recent_messages)
            else:
                # 문자열인 경우, 메시지 구분자를 "\n\n"으로 가정하여 분리
                messages = str(past_context).split("\n\n")
                recent_messages = messages[-5:]
                past_context = "\n\n".join(recent_messages)
            
            # # 2) 추가: 전체 토큰 수가 4000개를 초과하면 마지막 4000 토큰만 유지
            # past_tokens = self.tokenizer.tokenize(str(past_context))
            # if len(past_tokens) > 4000:
            #     past_tokens = past_tokens[-4000:]
            #     past_context = self.tokenizer.convert_tokens_to_string(past_tokens)
            
            # ★ 토큰 수 계산 코드 추가 ★
            # retrieval 자료는 dict나 리스트일 수 있으므로 문자열로 변환하여 토큰화합니다.
            # 각 입력값을 명시적으로 str()로 변환합니다.
            past_tokens = self.tokenizer.tokenize(str(past_context))
            query_tokens = self.tokenizer.tokenize(str(user_input))
            total_tokens = len(past_tokens) + len(query_tokens)
            print(f"[DEBUG] Token counts - 이전 대화: {len(past_tokens)}, 사용자 입력 질문: {len(query_tokens)}, 총합: {total_tokens}")
            
            # # To Calculate the token
            # tokens = self.tokenizer(user_input, add_special_tokens=True)["input_ids"]
            # print(f"[DEBUG] Processing query: '{user_input}' with {len(tokens)} tokens")

            # 5) 필요하다면 RAG 데이터를 다시 로드(1.16version 유지)
            self.data = load_data(
                self.config.data_path
            )  # if you want always-latest, else skip

            # 6) 현재 사용중인 Thread 확인
            print("   ... calling query_sort() ...")
            # print(
            #     f"[DEBUG] query_sort 시작 (offload) - 스레드: {threading.current_thread().name}"
            # )
            # 7) “대화 이력 + 현재 사용자 질문”을 Prompt에 합쳐서 RAG 수행
            #    방법 1) query_sort() 전에 past_context를 참조해 query를 확장
            #    방법 2) generate_answer()에서 Prompt 앞부분에 붙임
            # 여기서는 예시로 “query_sort”에 past_context를 넘겨
            # 호출부 수정 "user_input": f"{past_context}\n사용자 질문: {user_input}",
            params = {
                "user_input": f"사용자 질문: {user_input}",
                "model": self.model,
                "tokenizer": self.tokenizer,
                "embed_model": self.embed_model,
                "embed_tokenizer": self.embed_tokenizer,
                "data": self.data,
                "config": self.config,
            }
            QU, KE, TA, TI = await query_sort(params)
            print(f"   ... query_sort => QU={QU}, KE={KE}, TA={TA}, TI={TI}")

            # 4) RAG
            if TA == "yes":
                try:
                    print("[SOOWAN] config 설정 : ", self.config)
                    docs, docs_list = await execute_rag(
                        QU,
                        KE,
                        TA,
                        TI,
                        model=self.model,
                        tokenizer=self.tokenizer,
                        embed_model=self.embed_model,
                        embed_tokenizer=self.embed_tokenizer,
                        data=self.data,
                        config=self.config,
                    )
                    try:
                                                # 기존 방식
                        retrieval, chart = process_to_format(docs_list, type="SQL")
                        # 수정된 방식 - Talbe,Chart 없이 Answer Part에 SQL 결과 전송.
                        # retrieval_sql = process_to_format(docs, type="Answer")
                        # await self.queue_manager.put_token.remote(request_id, retrieval_sql)
                    except Exception as e:
                        print("[ERROR] process_to_format (SQL) failed:", str(e))
                        retrieval, chart = [], None

                    # If streaming => partial tokens
                    if is_streaming:
                        print(
                            f"[STREAM] Starting partial generation for request_id={request_id}"
                        )
                        await self._stream_partial_answer(
                            QU, docs, retrieval, chart, request_id, future, user_input
                        )
                    else:
                        # normal final result
                        output = await generate_answer(
                            QU,
                            docs,
                            model=self.model,
                            tokenizer=self.tokenizer,
                            config=self.config,
                        )
                        answer = process_to_format([output, chart], type="Answer")
                        final_data = [retrieval, answer]
                        outputs = process_format_to_response(final_data, qry_id=None, continue_="C")
                        
                        # >>> Record used chunk IDs
                        # 변경 후: retrieval 결과에서 추출
                        chunk_ids_used = []
                        print("---------------- chunk_id 찾기 : ", retrieval.get("rsp_data", []))
                        for doc in retrieval.get("rsp_data", []):
                            if "chunk_id" in doc:
                                chunk_ids_used.append(doc["chunk_id"])
                                                        
                        # >>> CHANGED: summarize the conversation
                        # loop = asyncio.get_event_loop()
                        # prev_summary = memory.load_memory_variables({}).get("summary", "")
                        # new_entry = f"User: {user_input}\nAssistant: {output}\nUsed Chunks: {chunk_ids_used}\n"
                        # updated_conversation = prev_summary + "\n" + new_entry
                        # # Summarized CPU 사용
                        # # import concurrent.futures

                        # # Create a dedicated pool with more workers (e.g., 4)
                        # # summary_pool = concurrent.futures.ProcessPoolExecutor(max_workers=4)

                        # # Later, when calling the summarization function:
                        # summarized = loop.run_in_executor(None, summarize_conversation, updated_conversation)
                        # # # After obtaining 'summarized' in _process_single_query:
                        # # if not summarized:
                        # #     print("[ERROR] Summarization returned an empty string.")
                        # # else:
                        # #     print(f"[CHECK] Summarized conversation: {summarized}")
                        # memory.save_context({"input": user_input}, {"output": output, "chunk_ids": chunk_ids_used})
                                            # 메모리에 저장
                        try:
                            memory.save_context(
                                {
                                    "qry_contents": user_input,
                                    "qry_id": http_query.get("qry_id"),
                                    "user_id": http_query.get("user_id"),
                                    "auth_class": http_query.get("auth_class"),
                                    "qry_time": http_query.get("qry_time")
                                },
                                {
                                    "output": output,
                                    "chunk_ids": chunk_ids_used
                                }
                            )
                        except Exception as e:
                            print(f"[ERROR memory.save_context] {e}")
                        # >>> CHANGED -----------------------------------------------------
                        future.set_result(outputs)

                except Exception as e:
                    outputs = error_format("내부 Excel 에 해당 자료가 없습니다.", 551)
                    future.set_result(outputs)

            else:
                try:
                    print("[SOOWAN] TA is No, before make a retrieval")
                    QU, KE, TA, TI = await specific_question(params) # TA == no, so that have to remake the question based on history
                    
                    docs, docs_list = await execute_rag(
                        QU,
                        KE,
                        TA,
                        TI,
                        model=self.model,
                        tokenizer=self.tokenizer,
                        embed_model=self.embed_model,
                        embed_tokenizer=self.embed_tokenizer,
                        data=self.data,
                        config=self.config,
                    )
                    retrieval = process_to_format(docs_list, type="Retrieval")
                    print("[SOOWAN] TA is No, and make a retrieval is successed")
                    if is_streaming:
                        print(
                            f"[STREAM] Starting partial generation for request_id={request_id}"
                        )
                        await self._stream_partial_answer(
                            QU, docs, retrieval, None, request_id, future, user_input
                        )
                    else:
                        output = await generate_answer(
                            QU,
                            docs,
                            model=self.model,
                            tokenizer=self.tokenizer,
                            config=self.config,
                        )
                        print("process_to_format 이후에 OUTPUT 생성 완료")
                        answer = process_to_format([output], type="Answer")
                        print("process_to_format 이후에 ANSWER까지 생성 완료")
                        final_data = [retrieval, answer]
                        outputs = process_format_to_response(final_data, qry_id=None, continue_="C")
                        
                        # >>> CHANGED: Record used chunk ID
                        chunk_ids_used = []
                        print("---------------- chunk_id 찾기 : ", retrieval.get("rsp_data", []))
                        for doc in retrieval.get("rsp_data", []):
                            if "chunk_id" in doc:
                                chunk_ids_used.append(doc["chunk_id"])
                                
                        
                        # loop = asyncio.get_event_loop()
                        # prev_summary = memory.load_memory_variables({}).get("summary", "")
                        # new_entry = f"User: {user_input}\nAssistant: {output}\nUsed Chunks: {chunk_ids_used}\n"
                        # updated_conversation = prev_summary + "\n" + new_entry
                        # # # Summarized CPU 사용
                        # # import concurrent.futures
                        
                        # # # Create a dedicated pool with more workers (e.g., 4)
                        # # summary_pool = concurrent.futures.ProcessPoolExecutor(max_workers=4)
                        
                        # # Later, when calling the summarization function:
                        # summarized = loop.run_in_executor(None, summarize_conversation, updated_conversation)
                        
                        # memory.save_context({"input": user_input}, {"output": output, "chunk_ids": chunk_ids_used})
                                            # 메모리 저장
                        try:
                            memory.save_context(
                                {
                                    "qry_contents": user_input,
                                    "qry_id": http_query.get("qry_id"),
                                    "user_id": http_query.get("user_id"),
                                    "auth_class": http_query.get("auth_class"),
                                    "qry_time": http_query.get("qry_time")
                                },
                                {
                                    "output": output,
                                    "chunk_ids": chunk_ids_used
                                }
                            )
                        except Exception as e:
                            print(f"[ERROR memory.save_context] {e}")
                        # --------------------------------------------------------------------
                        
                        future.set_result(outputs)

                except Exception as e:
                    # ====== 이 부분에서 SSE를 즉시 닫고 스트리밍 종료 ======
                    err_msg = f"[ERROR] 처리 중 오류 발생: {str(e)}"
                    print(err_msg)

                    # SSE 전송 (error 이벤트)
                    if request_id:
                        try:
                            error_token = json.dumps({"type": "error", "message": err_msg}, ensure_ascii=False)
                            await self.queue_manager.put_token.remote(request_id, error_token)
                            # 스트리밍 종료
                            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
                        except Exception as e2:
                            print(f"[ERROR] SSE 전송 중 추가 예외 발생: {str(e2)}")
                        finally:
                            # SSEQueue 정리
                            await self.close_sse_queue(request_id)

                    # Future 응답도 에러로
                    future.set_result(error_format(str(e), 500))
                    return
                
        except Exception as e:
            err_msg = f"[ERROR] 처리 중 오류 발생: {str(e)}"
            print("[ERROR]", err_msg)
            # SSE 스트리밍인 경우 error 토큰과 종료 토큰 전송
            if request_id:
                try:
                    error_token = json.dumps({"type": "error", "message": err_msg}, ensure_ascii=False)
                    await self.queue_manager.put_token.remote(request_id, error_token)
                except Exception as e2:
                    print(f"[ERROR] SSE 전송 중 추가 예외 발생: {str(e2)}")
            future.set_result(error_format(err_msg, 500))
        finally:
            # 스트리밍 요청인 경우 반드시 SSE 큐에 종료 토큰을 넣고 큐를 정리한다.
            if request_id:
                try:
                    await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
                except Exception as ex:
                    print(f"[DEBUG] Error putting STREAM_DONE: {str(ex)}")
                await self.close_sse_queue(request_id)
                
    # ---------------------------------------------------------
    # PROCESS IMAGE QUERY (IMAGE TO TEXT)
    # ---------------------------------------------------------
    async def process_image_query(self, http_query):
        import uuid
        import base64
        import io
        from PIL import Image
        from transformers import AutoProcessor
        from vllm import SamplingParams
        from vllm.multimodal.utils import fetch_image

        request_id = http_query.get("request_id", str(uuid.uuid4()))
        print(f"[DEBUG] [process_image_query] START => request_id={request_id}")

        # 1) 파라미터 파싱
        image_input = http_query.get("image_data")  # base64 or URL
        user_query = http_query.get("qry_contents", "이 이미지에 대해 설명해줘.")
        print(f"[DEBUG] Step1 - user_query='{user_query}', image_input type={type(image_input)}")

        # 2) 이미지 -> PIL
        pil_image = None
        try:
            print("[DEBUG] Step2 - Converting image data to PIL...")
            if isinstance(image_input, str) and (
                image_input.startswith("http://") or image_input.startswith("https://")
            ):
                print("[DEBUG]   => image_input is a URL; using fetch_image()")
                pil_image = fetch_image(image_input)
            else:
                print("[DEBUG]   => image_input is presumably base64")
                if isinstance(image_input, str) and image_input.startswith("data:image/"):
                    print("[DEBUG]   => detected 'data:image/' prefix => splitting off base64 header")
                    image_input = image_input.split(",", 1)[-1]
                decoded = base64.b64decode(image_input)
                print(f"[DEBUG]   => decoded base64 length={len(decoded)} bytes")
                pil_image = Image.open(io.BytesIO(decoded)).convert("RGB")
            print("[DEBUG] Step2 - PIL image loaded successfully:", pil_image.size)
        except Exception as e:
            err_msg = f"[ERROR-step2] Failed to load image: {str(e)}"
            print(err_msg)
            return {"type": "error", "message": err_msg}

        # 3) HF Processor 로드
        try:
            print(f"[DEBUG] Step3 - Loading processor from '{self.config.model_id}' ... (use_fast=False)")
            processor = AutoProcessor.from_pretrained(
                self.config.model_id,
                use_fast=False  # 모델이 fast processor를 지원한다면 True로 시도 가능
            )
            print("[DEBUG]   => processor loaded:", type(processor).__name__)
        except Exception as e:
            err_msg = f"[ERROR-step3] Failed to load processor: {str(e)}"
            print(err_msg)
            return {"type": "error", "message": err_msg}

        # 4) Chat Template 적용 (tokenize=False => 최종 prompt string만 얻음)
        print("[DEBUG] Step4 - Constructing messages & applying chat template (tokenize=False)...")
        messages = [
            {
                "role": "user",
                "content": [
                    {"type": "image", "url": pil_image},   # PIL object
                    {"type": "text",  "text": user_query}, # 사용자 질의
                ],
            }
        ]
        try:
            # tokenize=False => prompt를 'raw string' 형태로 얻음
            prompt_string = processor.apply_chat_template(
                messages,
                tokenize=False,           # 핵심!
                add_generation_prompt=True,
            )
            print("[DEBUG]   => prompt_string (first ~200 chars):", prompt_string[:200])
        except Exception as e:
            err_msg = f"[ERROR-step4] Error in processor.apply_chat_template: {str(e)}"
            print(err_msg)
            return {"type": "error", "message": err_msg}

        # 5) Sampling Params
        print("[DEBUG] Step5 - Setting sampling params...")
        sampling_params = SamplingParams(
            max_tokens=self.config.model.max_new_tokens,
            temperature=self.config.model.temperature,
            top_k=self.config.model.top_k,
            top_p=self.config.model.top_p,
            repetition_penalty=self.config.model.repetition_penalty,
        )
        print("[DEBUG]   => sampling_params =", sampling_params)

        # 6) Generate 호출
        print("[DEBUG] Step6 - Starting vLLM generate(...) using multi_modal_data")
        result_chunks = []
        try:
            # vLLM에 prompt와 함께 multi_modal_data를 넘김
            # => vLLM 내부에서 Gemma3MultiModalProcessor가 image 임베딩 및 token placement 처리
            generate_request = {
                "prompt": prompt_string,
                "multi_modal_data": {
                    "image": [pil_image]  # 여러 장이라면 list에 더 추가
                }
            }

            async for out in self.model.generate(
                prompt=generate_request,
                sampling_params=sampling_params,
                request_id=request_id,
            ):
                result_chunks.append(out)

            print("[DEBUG] Step6 - All chunks retrieved: total:", len(result_chunks))

        except Exception as e:
            err_msg = f"[ERROR-step6] Error in model.generate: {str(e)}"
            print(err_msg)
            return {"type": "error", "message": err_msg}

        if not result_chunks:
            print("[DEBUG] Step6 - No output from model => returning error")
            return {"type": "error", "message": "No output from model."}

        final_output = next((c for c in result_chunks if getattr(c, "finished", False)), result_chunks[-1])
        answer_text = "".join(piece.text for piece in final_output.outputs)
        print(f"[DEBUG] Final answer: {answer_text}")

        # 완료
        print(f"[DEBUG] [process_image_query] DONE => request_id={request_id}")
        return {
            "result": answer_text,
            "request_id": request_id,
            "status_code": 200
        }

    # async def process_image_query(self, http_query):
    #     import uuid
    #     import base64
    #     import io
    #     from PIL import Image
    #     from transformers import AutoProcessor
    #     from vllm import SamplingParams
    #     from vllm.multimodal.utils import fetch_image

    #     request_id = http_query.get("request_id", str(uuid.uuid4()))
    #     print(f"[DEBUG] [process_image_query] START => request_id={request_id}")

    #     # 1) 파라미터 파싱
    #     image_input = http_query.get("image_data")  # base64 or URL
    #     user_query = http_query.get("qry_contents", "이 이미지에 대해 설명해줘.")
    #     print(f"[DEBUG] Step1 - user_query='{user_query}', image_input type={type(image_input)}")

    #     # 2) 이미지 -> PIL
    #     pil_image = None
    #     try:
    #         print("[DEBUG] Step2 - Converting image data to PIL...")
    #         if isinstance(image_input, str) and (image_input.startswith("http://") or image_input.startswith("https://")):
    #             print("[DEBUG]   => image_input is a URL; using fetch_image()")
    #             pil_image = fetch_image(image_input)
    #         else:
    #             print("[DEBUG]   => image_input is presumably base64")
    #             if isinstance(image_input, str) and image_input.startswith("data:image/"):
    #                 # 'data:image/png;base64,' 같은 prefix 제거
    #                 print("[DEBUG]   => detected 'data:image/' prefix => splitting off base64 header")
    #                 image_input = image_input.split(",", 1)[-1]
    #             decoded = base64.b64decode(image_input)
    #             print(f"[DEBUG]   => decoded base64 length={len(decoded)} bytes")
    #             pil_image = Image.open(io.BytesIO(decoded)).convert("RGB")
    #         print("[DEBUG] Step2 - PIL image loaded successfully:", pil_image.size)
    #     except Exception as e:
    #         err_msg = f"[ERROR-step2] Failed to load image: {str(e)}"
    #         print(err_msg)
    #         return {"type": "error", "message": err_msg}
        
    #     # 2) generate 호출
    #     print("[DEBUG] Step3 - Starting vLLM generate(...)")
    #     result_chunks = []
    #     try:
    #         async for out in self.model.generate(
    #             {
    #                 "prompt": user_query,  # The text from apply_chat_template
    #                 "multi_modal_data": {
    #                 "image": [pil_image]    # or list of images
    #                 }
    #             },
    #                 sampling_params=SamplingParams(
    #                 temperature=0.7,
    #                 max_tokens=256,
    #             ),
    #             request_id=request_id,
    #         ):
    #             # 디버그: 스트리밍 중간 결과 로그 (너무 잦은 로그는 성능에 영향을 줄 수 있음)
    #             # print(f"[DEBUG]   => partial chunk: {out}")
    #             result_chunks.append(out)
    #         print("[DEBUG] Step3 - All chunks retrieved: total:", len(result_chunks))
    #     except Exception as e:
    #         err_msg = f"[ERROR-step3] Error in model.generate: {str(e)}"
    #         print(err_msg)
    #         return {"type": "error", "message": err_msg}

    #     if not result_chunks:
    #         print("[DEBUG] Step7 - No output from model => returning error")
    #         return {"type": "error", "message": "No output from model."}

    #     final_output = next((c for c in result_chunks if getattr(c, "finished", False)), result_chunks[-1])
    #     answer_text = "".join(piece.text for piece in final_output.outputs)
    #     print(f"[DEBUG] Final answer: {answer_text}")

    #     # 완료
    #     print(f"[DEBUG] [process_image_query] DONE => request_id={request_id}")
    #     return {
    #         "result": answer_text,
    #         "request_id": request_id,
    #         "status_code": 200
    #     }



    # ------------------------------------------------------------
    # HELPER FOR STREAMING PARTIAL ANSWERS (Modified to send reference)
    # ------------------------------------------------------------
    async def _stream_partial_answer(
        self, QU, docs, retrieval, chart, request_id, future, user_input
    ):
        """
        Instead of returning a final string, we generate partial tokens
        and push them to the SSE queue in real time.
        We'll do a "delta" approach so each chunk is only what's newly added.
        """
        print(
            f"[STREAM] _stream_partial_answer => request_id={request_id}, chart={chart}"
        )

        # 단일
        # queue = self.active_sse_queues.get(request_id)
        # if not queue:
        #     print(f"[STREAM] SSE queue not found => fallback to normal final (request_id={request_id})")
        #     # fallback...
        #     return

        # This will hold the entire text so far. We'll yield only new pieces.
        
        # 먼저, 참조 데이터 전송: type을 "reference"로 명시
        reference_json = json.dumps({
            "type": "reference",
            "status_code": 200,
            "result": "OK",
            "detail": "Reference data",
            "evt_time": datetime.datetime.now().isoformat(),
            "data_list": [retrieval]
        }, ensure_ascii=False)
        # Debug: print the reference JSON before sending
        print(f"[DEBUG] Prepared reference data: {reference_json}")
        await self.queue_manager.put_token.remote(request_id, reference_json)
        
        print(f"[STREAM] Sent reference data for request_id={request_id}")
        
        # 1) 메모리 가져오기 (없으면 생성)
        try:
            memory = self.get_memory_for_session(request_id)
        except Exception as e:
            msg = f"[STREAM] Error retrieving memory for {request_id}: {str(e)}"
            print(msg)
            # 에러 응답을 SSE로 전송하고 종료
            error_token = json.dumps({"type":"error","message":msg}, ensure_ascii=False)
            await self.queue_manager.put_token.remote(request_id, error_token)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            future.set_result(error_format(msg, 500))
            return
        
        # 2) 과거 대화 이력 로드
        try:
            past_context = memory.load_memory_variables({})["history"]
            # history가 리스트 형식인 경우 (각 메시지가 별도 항목으로 저장되어 있다면)
            if isinstance(past_context, list):
                recent_messages = [msg if isinstance(msg, str) else msg.content for msg in past_context[-5:]]
                past_context = "\n\n".join(recent_messages)
            else:
                # 문자열인 경우, 메시지 구분자를 "\n\n"으로 가정하여 분리
                messages = str(past_context).split("\n\n")
                recent_messages = messages[-5:]
                past_context = "\n\n".join(recent_messages)
            
            # # 2) 추가: 전체 토큰 수가 4000개를 초과하면 마지막 4000 토큰만 유지
            # past_tokens = self.tokenizer.tokenize(str(past_context))
            # if len(past_tokens) > 4000:
            #     past_tokens = past_tokens[-4000:]
            #     past_context = self.tokenizer.convert_tokens_to_string(past_tokens)
        except KeyError:
            # 만약 "history" 키가 없으면 빈 문자열로 처리
            print(f"[STREAM] No 'history' in memory for {request_id}, using empty.")
            past_context = ""
        except Exception as e:
            msg = f"[STREAM] load_memory_variables error for {request_id}: {str(e)}"
            print(msg)
            error_token = json.dumps({"type":"error","message":msg}, ensure_ascii=False)
            await self.queue_manager.put_token.remote(request_id, error_token)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            future.set_result(error_format(msg, 500))
            return

        # 3) 최종 프롬프트 구성
        final_query = f"{past_context}\n\n[사용자 질문]\n{QU}"
        print(f"[STREAM] final_query = \n{final_query}")
        
        # ★ 토큰 수 계산 코드 추가 ★
        # retrieval 자료는 dict나 리스트일 수 있으므로 문자열로 변환하여 토큰화합니다.
        retrieval_str = str(retrieval)
        # 각 입력값을 명시적으로 str()로 변환합니다.
        past_tokens = self.tokenizer.tokenize(str(past_context))
        query_tokens = self.tokenizer.tokenize(str(QU))
        retrieval_tokens = self.tokenizer.tokenize(retrieval_str)
        total_tokens = len(self.tokenizer.tokenize(str(final_query))) + len(retrieval_tokens)
        print(f"[DEBUG] Token counts - 이전 대화: {len(past_tokens)}, RAG 검색 자료: {len(retrieval_tokens)}, 사용자 구체화 질문: {len(query_tokens)}, 총합: {total_tokens}")
        
        partial_accumulator = ""

        try:
            print(
                f"[STREAM] SSE: calling generate_answer_stream for request_id={request_id}"
            )
            async for partial_text in generate_answer_stream(
                final_query, docs, self.model, self.tokenizer, self.config
            ):
                # print(f"[STREAM] Received partial_text: {partial_text}")
                new_text = partial_text[len(partial_accumulator) :]
                partial_accumulator = partial_text
                # # 원래 코드
                # if not new_text.strip():
                #     continue

                # 수정 예시: new_text가 완전히 빈 문자열("")인 경우에만 건너뛰기
                if new_text == "":
                    continue
                
                # Wrap answer tokens in a JSON object with type "answer"
                answer_json = json.dumps({
                    "type": "answer",
                    "answer": new_text
                }, ensure_ascii=False)
                # Use the central SSEQueueManager to put tokens
                # print(f"[STREAM] Sending token: {answer_json}")
                await self.queue_manager.put_token.remote(request_id, answer_json)
            final_text = partial_accumulator
            # # 이제 memory에 저장 (이미 request_id를 알고 있다고 가정) # 랭체인
            # try:
            #     memory.save_context({"input": user_input}, {"output": final_text})
            # except Exception as e:
            #     msg = f"[STREAM] memory.save_context failed: {str(e)}"
            #     print(msg)
                
                
            # >>> CHANGED: Update conversation summary in streaming branch as well
            chunk_ids_used = []
            print("---------------- chunk_id 찾기 : ", retrieval.get("rsp_data", []))
            for doc in retrieval.get("rsp_data", []):
                if "chunk_id" in doc:
                    chunk_ids_used.append(doc["chunk_id"])
                    
            # memory.save_context({"input": user_input}, {"output": final_text, "chunk_ids": chunk_ids_used})
            
            # 메모리 저장
            try:
                memory.save_context(
                    {
                        "qry_contents": user_input,
                        "qry_id": "",  # 필요한 경우 http_query에 있는 값을 넣음
                    },
                    {
                        "output": final_text,
                        "chunk_ids": chunk_ids_used
                    }
                )
            except Exception as e:
                print(f"[ERROR memory.save_context in stream] {e}")
            
            print("메시지 저장 직후 chunk_id 확인 : ", memory)
            # >>> CHANGED: -------------------------------------------------------
            
            # 최종 응답 구조
            if chart is not None:
                ans = process_to_format([final_text, chart], type="Answer")
                final_res = process_format_to_response(retrieval, ans)
            else:
                ans = process_to_format([final_text], type="Answer")
                final_res = process_format_to_response(retrieval, ans)
                
            # 담아서 보내기
            future.set_result(final_res)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            print(
                f"[STREAM] done => placed [[STREAM_DONE]] for request_id={request_id}"
            )
        except Exception as e:
            msg = f"[STREAM] error in partial streaming => {str(e)}"
            print(msg)
            future.set_result(error_format(msg, 500))
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")

    # --------------------------------------------------------
    # EXISTING METHODS FOR NORMAL QUERIES (unchanged)
    # --------------------------------------------------------
    async def process_query(self, http_query):
        """
        Existing synchronous method. Returns final string/dict once done.
        """
        loop = asyncio.get_event_loop()
        future = loop.create_future()
        # There's no SSE queue for normal queries
        sse_queue = None
        await self.request_queue.put((http_query, future, sse_queue))
        # print("self.request_queue : ", self.request_queue)
        return await future
    # ----------------------
    # 1) Streaming Entrypoint
    # ----------------------
    async def process_query_stream(self, http_query: dict) -> str:
        """
        /query_stream 호출 시 page_id(채팅방 id)를 기반으로 SSE queue 생성하고,
        대화 저장에 활용할 수 있도록 합니다.
        """
        # page_id를 채팅방 id로 사용 (없으면 생성)
        chat_id = http_query.get("page_id")
        if not chat_id:
            chat_id = str(uuid.uuid4())
        http_query["page_id"] = chat_id  # 강제 할당
        await self.queue_manager.create_queue.remote(chat_id)
        print(f"[STREAM] process_query_stream => chat_id={chat_id}, http_query={http_query}")

        loop = asyncio.get_event_loop()
        final_future = loop.create_future()

        sse_queue = asyncio.Queue()
        self.active_sse_queues[chat_id] = sse_queue
        print(f"[STREAM] Created SSE queue for chat_id={chat_id}")

        # 기존과 동일하게 micro-batch queue에 푸시 (http_query에 새 필드들이 포함됨)
        queued_item = {
            "request_id": chat_id,   # 내부적으로 page_id를 request_id처럼 사용
            "http_query": http_query,
        }

        print(f"[STREAM] Putting item into request_queue for chat_id={chat_id}")
        await self.request_queue.put((queued_item, final_future, sse_queue))
        print(f"[STREAM] Done putting item in queue => chat_id={chat_id}")

        return chat_id


    # ----------------------
    # 2) SSE token popping
    # ----------------------
    async def pop_sse_token(self, request_id: str) -> Optional[str]:
        """
        The SSE route calls this repeatedly to get partial tokens.
        If no token is available, we block up to 120s, else return None.
        """
        if request_id not in self.active_sse_queues:
            print(
                f"[STREAM] pop_sse_token => no SSE queue found for request_id={request_id}"
            )
            return None

        queue = self.active_sse_queues[request_id]
        try:
            token = await asyncio.wait_for(queue.get(), timeout=120.0)
            # print(f"[STREAM] pop_sse_token => got token from queue: {token}")
            return token
        except asyncio.TimeoutError:
            print(
                f"[STREAM] pop_sse_token => timed out waiting for token, request_id={request_id}"
            )
            return None

    # ----------------------
    # 3) SSE queue cleanup
    # ----------------------
    async def close_sse_queue(self, request_id: str):
        """
        Called by the SSE route after finishing.
        Remove the queue from memory.
        """
        if request_id in self.active_sse_queues:
            print(
                f"[STREAM] close_sse_queue => removing SSE queue for request_id={request_id}"
            )
            del self.active_sse_queues[request_id]
        else:
            print(f"[STREAM] close_sse_queue => no SSE queue found for {request_id}")
    
    # ----------------------
    # /history | 대화 기록 가져오기
    # ----------------------
    async def get_conversation_history(self, request_id: str) -> dict:
        """
        Returns the conversation history for the given request_id.
        The messages are serialized into a JSON-friendly format.
        """
        try:
            if request_id in self.memory_map:
                memory = self.memory_map[request_id]
                history_obj = memory.load_memory_variables({})
                if "history" in history_obj and isinstance(history_obj["history"], list):
                    # 직렬화
                    serialized = [serialize_message(msg) for msg in history_obj["history"]]
                    print("[HISTORY] 대화 기록 반환(직렬화) : ", serialized)
                    return {"history": serialized}
                else:
                    print("[HISTORY] 대화 기록 반환(직렬화X) : ", history_obj)
                    return {"history": []}
            else:
                return {"history": []}
        except Exception as e:
            print(f"[ERROR get_conversation_history] {e}")
            return {"history": []}
        
    # ----------------------
    # /reference | 해당 답변의 출처 가져오기
    # ----------------------
    async def get_reference_data(self, chunk_ids: list):
        try:
            result = []
            data = self.data
            for cid in chunk_ids:
                if cid in data["chunk_ids"]:
                    idx = data["chunk_ids"].index(cid)
                    record = {
                        "file_name": data["file_names"][idx],
                        "title": data["titles"][idx],
                        "text": data["texts_vis"][idx],
                        "date": str(data["times"][idx])
                    }
                    result.append(record)
            return result
        except Exception as e:
            print(f"[ERROR get_reference_data] {e}")
            return []

# Ray Serve를 통한 배포
@serve.deployment(name="inference", max_ongoing_requests=100)
class InferenceService:
    def __init__(self, config):
        self.config = config
        self.actor = InferenceActor.options(
            num_gpus=config.ray.num_gpus, 
            num_cpus=config.ray.num_cpus
        ).remote(config)

    async def query(self, http_query: dict):
        result = await self.actor.process_query.remote(http_query)
        return result

    async def process_query_stream(self, http_query: dict) -> str:
        req_id = await self.actor.process_query_stream.remote(http_query)
        return req_id

    async def pop_sse_token(self, req_id: str) -> str:
        token = await self.actor.pop_sse_token.remote(req_id)
        return token

    async def close_sse_queue(self, req_id: str) -> str:
        await self.actor.close_sse_queue.remote(req_id)
        return "closed"
    
    # /history
    async def get_history(self, request_id: str, last_index: int = None):
        result = await self.actor.get_conversation_history.remote(request_id)
        if last_index is not None and isinstance(result.get("history"), list):
            result["history"] = result["history"][last_index+1:]
        return result

    # /reference
    async def get_reference_data(self, chunk_ids: list):
        result = await self.actor.get_reference_data.remote(chunk_ids)
        return result
    
    # InferenceService 클래스에 메서드 추가
    async def image_query(self, http_query: dict):
        result = await self.actor.process_image_query.remote(http_query)
        return result

```


--- utils/utils_load.py

```python

# utils/utils_load.py
import json
import numpy as np
import torch
import random
import shutil
from datetime import datetime, timedelta
from transformers import (
    AutoModel,
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    AutoConfig,
)

import os

# 전역 캐시 변수 - 데이터의 변화를 감지하기 위한
_cached_data = None
_cached_data_mtime = 0

# Import vLLM utilities
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

# Define the minimum valid file size (e.g., 10MB)
MIN_WEIGHT_SIZE = 10 * 1024 * 1024

# For tracking execution time of functions
from utils.tracking import time_tracker

# Logging
import logging
# -------------------------------------------------
# Function: find_weight_directory - 허깅페이스 권한 문제 해결 후에 잘 사용되지 아니함
# -------------------------------------------------
# Recursively searches for weight files (safetensors or pytorch_model.bin) in a given base path.
# This method Find the files searching the whole directory
# Because, vLLM not automatically find out the model files.
# -------------------------------------------------
@time_tracker
def find_weight_directory(base_path):
    # ---- Recursively searches for weight files in a given base path ----
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if ".safetensors" in file or "pytorch_model.bin" in file:
                file_path = os.path.join(root, file)
                try:
                    if os.path.getsize(file_path) >= MIN_WEIGHT_SIZE:
                        return root, "safetensors" if ".safetensors" in file else "pt"
                    else:
                        logging.debug(
                            f"파일 {file_path}의 크기가 너무 작음: {os.path.getsize(file_path)} bytes"
                        )
                except Exception as ex:
                    logging.debug(f"파일 크기 확인 실패: {file_path} - {ex}")
    return None, None

# -------------------------------------------------
# Function: load_model
# -------------------------------------------------
@time_tracker
def load_model(config):
    # Loads the embedding model and the main LLM model (using vLLM if specified in the config).
    
    # Get the HF token from the environment variable.
    logging.info("Starting model loading...")
    token = os.getenv("HF_TOKEN_PATH")
    # Check if token is likely a file path.
    if token is not None and not token.startswith("hf_"):
        if os.path.exists(token) and os.path.isfile(token):
            try:
                with open(token, "r") as f:
                    token = f.read().strip()
            except Exception as e:
                print("DEBUG: Exception while reading token file:", e)
                logging.warning("Failed to read token from file: %s", e)
                token = None
        else:
            logging.warning("The HF_TOKEN path does not exist: %s", token)
            token = None
    else:
        print("DEBUG: HF_TOKEN appears to be a token string; using it directly:")

    if token is None or token == "":
        logging.warning("HF_TOKEN is not set. Access to gated models may fail.")
        token = None

    # -------------------------------
    # Load the embedding model and tokenizer.
    # -------------------------------
    print("Loading embedding model")
    try:
        embed_model = AutoModel.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,  # using 'token' parameter
        )
    except Exception as e:
        raise e
    try:
        embed_tokenizer = AutoTokenizer.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,
        )
    except Exception as e:
        raise e
    print(":Embedding tokenizer loaded successfully.")
    embed_model.eval()
    embed_tokenizer.model_max_length = 4096

    # -------------------------------
    # Load the main LLM model via vLLM.
    # -------------------------------
    if config.use_vllm:
        print("vLLM mode enabled. Starting to load main LLM model via vLLM.")
        if config.model.quantization_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
            print("Using 4-bit quantization.")
        elif config.model.quantization_8bit:
            bnb_config = BitsAndBytesConfig(load_in_8bit=True)
            print("Using 8-bit quantization.")
        else:
            bnb_config = None
            print("Using pure option of Model(No quantization)")

        local_model_path = os.path.join(
            config.cache_dir, "models--" + config.model_id.replace("/", "--")
        )
        local_model_path = os.path.abspath(local_model_path)

        config_file = os.path.join(local_model_path, "config.json")
        need_patch = False

        if not os.path.exists(config_file):
            os.makedirs(local_model_path, exist_ok=True)
            try:
                hf_config = AutoConfig.from_pretrained(
                    config.model_id,
                    cache_dir=config.cache_dir,
                    trust_remote_code=True,
                    token=token,
                )
            except Exception as e:
                raise e
            # 패치: vocab_size 속성이 없으면 embed_tokenizer의 값을 사용하여 추가
            if not hasattr(hf_config, "vocab_size"):
                print("[MODEL-LOADING] 'vocab_size' 속성이 없어서 기본값으로 추가합니다.")
                hf_config.vocab_size = getattr(embed_tokenizer, "vocab_size", 32000)
            config_dict = hf_config.to_dict()
            # 패치: architectures 속성이 없으면 Gemma2로 기본 설정
            if not config_dict.get("architectures"):
                print("[MODEL-LOADING] Config file의 architectures 정보 없음, Default Gemma2 아키텍처 설정")
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
            # 확인
            print(f"[DEBUG] => Saving HF Config with architectures={config_dict['architectures']}")
                
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config_dict, f)
        else:
            # 이미 config_file이 존재하는 경우
            with open(config_file, "r", encoding="utf-8") as f:
                config_dict = json.load(f)
            
            # 패치: vocab_size 속성이 없으면 embed_tokenizer의 값을 사용하여 추가
            if "vocab_size" not in config_dict:
                # embed_tokenizer의 vocab_size가 존재하면 사용하고, 없으면 기본값 30522로 설정
                config_dict["vocab_size"] = getattr(embed_tokenizer, "vocab_size", 30522)
                print("[MODEL-LOADING] 'vocab_size' 속성이 없어서 기본값으로 추가합니다:", config_dict["vocab_size"])
            # 패치: architectures 속성이 없으면 Gemma2로 기본 설정
            if not config_dict.get("architectures"):
                print("[MODEL-LOADING] Config file의 architectures 정보 없음, Default Gemma2 아키텍처 설정")
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
            # 확인
            print(f"[DEBUG] => Saving HF Config with architectures={config_dict['architectures']}")

            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config_dict, f)

        weight_dir, weight_format = find_weight_directory(local_model_path)
        if weight_dir is None:
            print("DEBUG: No model weights found. Attempting to download model snapshot.")
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    print(f"DEBUG: Snapshot download attempt {attempt+1}...")
                    # Attempt to download the model snapshot using the Hugging Face hub function.
                    from huggingface_hub import snapshot_download
                    snapshot_download(config.model_id, cache_dir=config.cache_dir, token=token)
                    break  # If download succeeds, break out of the loop.
                except Exception as e:
                    print(f"DEBUG: Snapshot download attempt {attempt+1} failed:", e)
                    if attempt < max_retries - 1:
                        print("DEBUG: Retrying snapshot download...")
                    else:
                        raise RuntimeError(f"Snapshot download failed after {max_retries} attempts: {e}")
            # After download, try to find the weights again.
            weight_dir, weight_format = find_weight_directory(local_model_path)
            if weight_dir is None:
                raise RuntimeError(f"Unable to find model weights even after snapshot download in {local_model_path}.")

        snapshot_config = os.path.join(weight_dir, "config.json")
        if not os.path.exists(snapshot_config):
            shutil.copy(config_file, snapshot_config)
        engine_args = AsyncEngineArgs(
            model=weight_dir,
            tokenizer=config.model_id,
            download_dir=config.cache_dir,
            trust_remote_code=True,
            config_format="hf",
            load_format=weight_format,
        )
        
        vllm_conf = config.get("vllm", {})
        
        # --- 기존 엔진 설정들
        engine_args.enable_prefix_caching = True
        # engine_args.scheduler_delay_factor = vllm_conf.get("scheduler_delay_factor", 0.1)
        engine_args.enable_chunked_prefill = True
        engine_args.tensor_parallel_size = vllm_conf.get("tensor_parallel_size", 1) # How many use the parllel Multi-GPU
        engine_args.max_num_seqs = vllm_conf.get("max_num_seqs")
        engine_args.max_num_batched_tokens = vllm_conf.get("max_num_batched_tokens", 8192)
        # engine_args.block_size = vllm_conf.get("block_size", 128)
        engine_args.gpu_memory_utilization = vllm_conf.get("gpu_memory_utilization")
        
        # --- 다중 GPU 사용 관련 설정 ---
        if vllm_conf.get("disable_custom_all_reduce", False):
            engine_args.disable_custom_all_reduce = True # For Fixing the Multi GPU problem
        
        # --- 모델의 Context Length ---
        engine_args.max_model_len = vllm_conf.get("max_model_len")
        
        # --- 멀티모달 (Image) 관련 설정 ---
        engine_args.mm_processor_kwargs = vllm_conf.get("mm_processor_kwargs", {"do_pan_and_scan": True})
        engine_args.disable_mm_preprocessor_cache = vllm_conf.get("disable_mm_preprocessor_cache", False)
        engine_args.limit_mm_per_prompt = vllm_conf.get("limit_mm_per_prompt", {"image": 2})
        
        print("EngineArgs setting be finished")
        
        try:
            # --- v1 구동 해결책: 현재 스레드가 메인 스레드가 아니면 signal 함수를 임시 패치 ---
            import threading, signal
            if threading.current_thread() is not threading.main_thread():
                original_signal = signal.signal
                signal.signal = lambda s, h: None  # signal 설정 무시
                print("비메인 스레드에서 signal.signal을 monkey-patch 하였습니다.")
            # --- v1 구동 해결책: ------------------------------------------------------ ---
            engine = AsyncLLMEngine.from_engine_args(engine_args) # Original
            # v1 구동 해결책: 엔진 생성 후 원래 signal.signal으로 복원 (필요 시) ----------------- ---
            if threading.current_thread() is not threading.main_thread():
                signal.signal = original_signal
            # --- v1 구동 해결책: ------------------------------------------------------ ---
            print("DEBUG: vLLM engine successfully created.") # Original
            
        except Exception as e:
            print("DEBUG: Exception during engine creation:", e)
            if "HeaderTooSmall" in str(e):
                print("DEBUG: Falling back to PyTorch weights.")
                fallback_dir = None
                for root, dirs, files in os.walk(local_model_path):
                    for file in files:
                        if (
                            "pytorch_model.bin" in file
                            and os.path.getsize(os.path.join(root, file))
                            >= MIN_WEIGHT_SIZE
                        ):
                            fallback_dir = root
                            break
                    if fallback_dir:
                        break
                if fallback_dir is None:
                    logging.error(
                        "DEBUG: No PyTorch weight file found in", local_model_path
                    )
                    raise e
                engine_args.load_format = "pt"
                engine_args.model = fallback_dir
                print("DEBUG: New EngineArgs for fallback:", engine_args)
                engine = AsyncLLMEngine.from_engine_args(engine_args)
                print("DEBUG: vLLM engine created with PyTorch fallback.")
            else:
                logging.error("DEBUG: Engine creation failed:", e)
                raise e

        engine.is_vllm = True

        print("DEBUG: Loading main LLM tokenizer with token authentication.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
                local_files_only=True  # Force loading from local cache to avoid hub requests
            )
        except Exception as e:
            print("DEBUG: Exception loading main tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        return engine, tokenizer, embed_model, embed_tokenizer

    else:
        print("DEBUG: vLLM is not used. Loading model via standard HF method.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        try:
            model = AutoModelForCausalLM.from_pretrained(
                config.model_id,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                cache_dir=config.cache_dir,
                # quantization_config=bnb_config,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading model:", e)
            raise e
        model.eval()
        return model, tokenizer, embed_model, embed_tokenizer

# -------------------------------------------------
# Function: load_data
# -------------------------------------------------
@time_tracker
def load_data(data_path):
    global _cached_data, _cached_data_mtime
    try:
        current_mtime = os.path.getmtime(data_path)
    except Exception as e:
        print("파일 수정 시간 확인 실패:", e)
        return None

    # 캐시가 비어있거나 파일 수정 시간이 변경된 경우 데이터 재로드
    if _cached_data is None or current_mtime != _cached_data_mtime:
        with open(data_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # --- 디버그 함수: 벡터 포맷 검사 ---
        debug_vector_format(data)

        # 데이터 전처리 (예: 리스트 변환 및 numpy, torch 변환)
        file_names = []
        chunk_ids = []  # >>> CHANGED: Added to record each chunk's ID
        titles = []
        times = []
        vectors = []
        texts = []
        texts_short = []
        texts_vis = []
        missing_time = 0

        for file_obj in data:
            for chunk in file_obj["chunks"]:
                file_names.append(file_obj["file_name"])
                chunk_ids.append(chunk.get("chunk_id", 0))  # >>> CHANGED: Record chunk_id
                try:
                    arr = np.array(chunk["vector"])
                    vectors.append(arr)
                except Exception as e:
                    logging.warning(f"[load_data] 벡터 변환 오류: {e} → 빈 벡터로 대체")
                    vectors.append(np.zeros((1, 768), dtype=np.float32))  # 임의로 1x768 형식
                
                titles.append(chunk["title"])
                
                # 날짜 파싱
                if chunk["date"]:
                    try:
                        times.append(datetime.strptime(chunk["date"], "%Y-%m-%d"))
                    except ValueError:
                        logging.warning(f"잘못된 날짜 형식: {chunk['date']} → 기본 날짜로 대체")
                        times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))
                        missing_time += 1
                else:
                    missing_time += 1
                    times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))

                texts.append(chunk["text"])
                texts_short.append(chunk["text_short"])
                texts_vis.append(chunk["text_vis"])

        # 실제 텐서로 변환
        try:
            vectors = np.array(vectors)
            vectors = torch.from_numpy(vectors).to(torch.float32)
        except Exception as e:
            logging.error(f"[load_data] 최종 벡터 텐서 변환 오류: {str(e)}")
            # 필요 시 추가 처리

        _cached_data = {
            "file_names": file_names,
            "chunk_ids": chunk_ids,  # >>> CHANGED: Saved chunk IDs here
            "titles": titles,
            "times": times,
            "vectors": vectors,
            "texts": texts,
            "texts_short": texts_short,
            "texts_vis": texts_vis,
        }
        _cached_data_mtime = current_mtime
        print(f"Data loaded! Length: {len(titles)}, Missing times: {missing_time}")
    else:
        print("Using cached data")

    return _cached_data

# -------------------------------------------------
# Function: debug_vector_format
# -------------------------------------------------
def debug_vector_format(data):
    """
    data(List[Dict]): load_data에서 JSON으로 로드된 객체.
    각 file_obj에 대해 chunks 리스트를 순회하며 vector 형식을 디버깅 출력.
    """
    print("\n[DEBUG] ===== 벡터 형식 검사 시작 =====")
    for f_i, file_obj in enumerate(data):
        file_name = file_obj.get("file_name", f"Unknown_{f_i}")
        chunks = file_obj.get("chunks", [])
        for c_i, chunk in enumerate(chunks):
            vector_data = chunk.get("vector", None)
            if vector_data is None:
                # print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector 없음(None)")
                continue
            # 자료형, 길이, shape 등 확인
            vector_type = type(vector_data)
            # shape을 안전하게 얻기 위해 np.array 변환 시도
            try:
                arr = np.array(vector_data)
                shape = arr.shape
                # print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector_type={vector_type}, shape={shape}")
            except Exception as e:
                print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector 변환 실패: {str(e)}")
    print("[DEBUG] ===== 벡터 형식 검사 종료 =====\n")

# -------------------------------------------------
# Function: random_seed
# -------------------------------------------------
@time_tracker
def random_seed(seed):
    # Set random seed for Python's built-in random module
    random.seed(seed)

    # Set random seed for NumPy
    np.random.seed(seed)

    # Set random seed for PyTorch
    torch.manual_seed(seed)

    # Ensure the same behavior on different devices (CPU vs GPU)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # If using multi-GPU.

    # Enable deterministic algorithms
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

```


--- core/RAG.py

```python

# core/RAG.py
import torch
import re
import numpy as np
import rank_bm25
import random
import uuid
import logging
from datetime import datetime, timedelta
# from sql import generate_sql  # (구) 제거된 import
from core.SQL_NS import run_sql_unno, run_sql_bl, get_metadata  # SQL만 담당하는 함수들만 import

# Tracking
from utils.tracking import time_tracker

# Import the vLLM to use the AsyncLLMEngine
from vllm.engine.async_llm_engine import AsyncLLMEngine
# 이미지 임베딩을 별도로 계산하여 프롬프트에 포함하기
from vllm.model_executor.models.interfaces import SupportsMultiModal

# In RAG.py (at the top, add an import for prompts)
from prompt import QUERY_SORT_PROMPT, GENERATE_PROMPT_TEMPLATE, STREAM_PROMPT_TEMPLATE, SQL_EXTRACTION_PROMPT_TEMPLATE

global beep
beep = "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------"

@time_tracker
async def execute_rag(QU, KE, TA, TI, **kwargs):
    print("[SOOWAN]: execute_rag : 진입")
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    embed_model = kwargs.get("embed_model")
    embed_tokenizer = kwargs.get("embed_tokenizer")
    data = kwargs.get("data")
    config = kwargs.get("config")
    
    if TA == "yes":  # Table 이 필요하면
        print("[SOOWAN]: execute_rag : 테이블 필요 (TA == yes). SQL 생성 시작합니다.")
        try:
            # generate_sql 함수를 이 파일(RAG.py) 내부에 새로 정의했으므로, 여기서 직접 호출
            result = await generate_sql(QU, model, tokenizer, config)
        except Exception as e:
            # 1) generate_sql() 자체가 도중에 예외를 던지는 경우
            print("[ERROR] generate_sql() 도중 예외 발생:", e)
            # 멈추지 않고, 에러 형식으로 데이터를 만들어 반환
            docs = (
                "테이블 조회 시도 중 예외가 발생했습니다. "
                "해당 SQL을 실행할 수 없어서 테이블 데이터를 가져오지 못했습니다."
            )
            docs_list = []
            return docs, docs_list

        # 2) 함수가 정상 실행됐지만 결과가 None인 경우(= SQL 쿼리 결과가 없거나 오류)
        if result is None:
            print(
                "[WARNING] generate_sql()에서 None을 반환했습니다. "
                "SQL 수행 결과가 없거나 에러가 발생한 것일 수 있습니다."
            )
            docs = (
                "테이블 조회 결과가 비어 있습니다. "
                "조회할 데이터가 없거나 SQL 오류가 발생했습니다."
            )
            docs_list = []
            return docs, docs_list

        # 기존 generate_sql은 이제 6개의 값을 반환합니다.
        final_sql_query, title, explain, table_json, chart_json, detailed_result = result

        # docs : LLM 입력용 (string)
        PROMPT = (
            f"실제 사용된 SQL문: {final_sql_query}\n\n"
            f"추가 설명: {explain}\n\n"
            f"실제 SQL 추출된 데이터: {str(table_json)}\n\n"
            f"실제 선적된 B/L 데이터: {str(detailed_result)}\n\n"
        )
        # docs_list에 DG B/L 상세 정보를 추가하여 총 3개 항목으로 구성합니다.
        docs_list = [
            {"title": title, "data": table_json},
            {"title": "DG B/L 상세 정보", "data": detailed_result},
        ]
        print("[SOOWAN]: execute_rag : 테이블 부분 정상 처리 완료")

        return PROMPT, docs_list

    else:
        print("[SOOWAN]: execute_rag : 테이블 필요없음")
        # 적응형 시간 필터링으로 RAG 실행
        filtered_data = expand_time_range_if_needed(TI, data, min_docs=50)

        # 디버깅을 위해 문서 수 로깅
        print(f"[RETRIEVE] 검색에 사용되는 문서 수: {len(filtered_data.get('vectors', []))}")

        docs, docs_list = retrieve(KE, filtered_data, config.N, embed_model, embed_tokenizer)
        return docs, docs_list

@time_tracker
async def generate_answer(query, docs, **kwargs):
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    config = kwargs.get("config")

    answer = await generate(docs, query, model, tokenizer, config)
    return answer


@time_tracker
async def query_sort(params):
    max_attempts = 3
    attempt = 0
    while attempt < max_attempts:
        # params: 딕셔너리로 전달된 값들
        query = params["user_input"]
        model = params["model"]
        tokenizer = params["tokenizer"]
        embed_model = params["embed_model"]
        embed_tokenizer = params["embed_tokenizer"]
        data = params["data"]
        config = params["config"]

        # 프롬프트 생성
        PROMPT = QUERY_SORT_PROMPT.format(user_query=query)
        print("##### query_sort is starting, attempt:", attempt + 1, "#####")

        # Get Answer from LLM
        if config.use_vllm:  # use_vllm = True case
            from vllm import SamplingParams

            sampling_params = SamplingParams(
                max_tokens=config.model.max_new_tokens,
                temperature=config.model.temperature,
                top_k=config.model.top_k,
                top_p=config.model.top_p,
                repetition_penalty=config.model.repetition_penalty,
            )
            accepted_request_id = str(uuid.uuid4())
            answer = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
        else:
            input_ids = tokenizer(
                PROMPT, return_tensors="pt", truncation=True, max_length=4024
            ).to("cuda")
            token_count = input_ids["input_ids"].shape[1]
            outputs = model.generate(
                **input_ids,
                max_new_tokens=config.model.max_new_tokens,
                do_sample=config.model.do_sample,
                temperature=config.model.temperature,
                top_k=config.model.top_k,
                top_p=config.model.top_p,
                repetition_penalty=config.model.repetition_penalty,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id,
            )
            answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)

        print("[DEBUG query_sort] Generated answer:")
        print(answer)

        # Regular expressions for tags
        query_pattern = r"<query.*?>(.*?)<query.*?>"
        keyword_pattern = r"<keyword.*?>(.*?)<keyword.*?>"
        table_pattern = r"<table.*?>(.*?)<table.*?>"
        time_pattern = r"<time.*?>(.*?)<time.*?>"

        m_query = re.search(query_pattern, answer, re.DOTALL)
        m_keyword = re.search(keyword_pattern, answer, re.DOTALL)
        m_table = re.search(table_pattern, answer, re.DOTALL)
        m_time = re.search(time_pattern, answer, re.DOTALL)

        if m_query and m_keyword and m_table and m_time:
            QU = m_query.group(1).strip()
            KE = m_keyword.group(1).strip()
            TA = m_table.group(1).strip()
            TI = m_time.group(1).strip()
            if TI == "all":
                TI = "1900-01-01:2099-01-01"
            print(beep)
            print(f"구체화 질문: {QU}, 키워드 : {KE}, 테이블 필요 유무: {TA}, 시간: {TI}")
            print(beep)
            return QU, KE, TA, TI
        else:
            print("[ERROR query_sort] 필요한 태그들이 누락되었습니다. 재시도합니다.")
            attempt += 1

    # 3회 재시도 후에도 실패하면 에러 발생
    raise ValueError("LLM이 올바른 태그 형식의 답변을 생성하지 못했습니다.")


@time_tracker
async def specific_question(params):
    """
    query_sort와 동일한 로직을 수행할 수도 있으나,
    별도로 분리된 이유가 있다면 여기서 추가 처리 가능
    """
    max_attempts = 3
    attempt = 0
    while attempt < max_attempts:
        query = params["user_input"]
        model = params["model"]
        tokenizer = params["tokenizer"]
        embed_model = params["embed_model"]
        embed_tokenizer = params["embed_tokenizer"]
        data = params["data"]
        config = params["config"]

        PROMPT = QUERY_SORT_PROMPT.format(user_query=query)
        print("##### query_sort is starting, attempt:", attempt + 1, "#####")

        if config.use_vllm:
            from vllm import SamplingParams

            sampling_params = SamplingParams(
                max_tokens=config.model.max_new_tokens,
                temperature=config.model.temperature,
                top_k=config.model.top_k,
                top_p=config.model.top_p,
                repetition_penalty=config.model.repetition_penalty,
            )
            accepted_request_id = str(uuid.uuid4())
            answer = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
        else:
            input_ids = tokenizer(
                PROMPT, return_tensors="pt", truncation=True, max_length=4024
            ).to("cuda")
            token_count = input_ids["input_ids"].shape[1]
            outputs = model.generate(
                **input_ids,
                max_new_tokens=config.model.max_new_tokens,
                do_sample=config.model.do_sample,
                temperature=config.model.temperature,
                top_k=config.model.top_k,
                top_p=config.model.top_p,
                repetition_penalty=config.model.repetition_penalty,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id,
            )
            answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)

        print("[DEBUG query_sort] Generated answer:")
        print(answer)

        query_pattern = r"<query.*?>(.*?)<query.*?>"
        keyword_pattern = r"<keyword.*?>(.*?)<keyword.*?>"
        table_pattern = r"<table.*?>(.*?)<table.*?>"
        time_pattern = r"<time.*?>(.*?)<time.*?>"

        m_query = re.search(query_pattern, answer, re.DOTALL)
        m_keyword = re.search(keyword_pattern, answer, re.DOTALL)
        m_table = re.search(table_pattern, answer, re.DOTALL)
        m_time = re.search(time_pattern, answer, re.DOTALL)

        if m_query and m_keyword and m_table and m_time:
            QU = m_query.group(1).strip()
            KE = m_keyword.group(1).strip()
            TA = m_table.group(1).strip()
            TI = m_time.group(1).strip()
            if TI == "all":
                TI = "1900-01-01:2099-01-01"
            print(beep)
            print(f"구체화 질문: {QU}, 키워드 : {KE}, 테이블 필요 유무: {TA}, 시간: {TI}")
            print(beep)
            return QU, KE, TA, TI
        else:
            print("[ERROR query_sort] 필요한 태그들이 누락되었습니다. 재시도합니다.")
            attempt += 1

    raise ValueError("LLM이 올바른 태그 형식의 답변을 생성하지 못했습니다.")


@time_tracker
def sort_by_time(time_bound, data):
    """
    원본 데이터는 유지하고 필터링된 복사본을 반환하는 함수
    """
    original_count = len(data["times"])
    print(f"[시간 필터 전] 문서 수: {original_count}")

    if time_bound == "all" or time_bound == "1900-01-01:2099-01-01":
        print(f"[시간 필터] 전체 기간 사용 - 모든 문서 포함")
        return data  # 원본 그대로

    date_format = "%Y-%m-%d"
    target_date_start = datetime.strptime(time_bound.split(":")[0], date_format)
    target_date_end = datetime.strptime(time_bound.split(":")[1], date_format)

    matching_indices = [
        i
        for i, date in enumerate(data["times"])
        if (not isinstance(date, str)) and (target_date_start < date < target_date_end)
    ]

    filtered_count = len(matching_indices)
    print(f"[시간 필터 후] 문서 수: {filtered_count}, 기간: {time_bound}")

    if filtered_count < 50 and filtered_count < original_count * 0.1:
        print(f"[경고] 시간 필터로 인해 문서가 크게 줄었습니다: {original_count} → {filtered_count}")

    filtered_data = {}
    filtered_data["file_names"] = [data["file_names"][i] for i in matching_indices]
    filtered_data["titles"] = [data["titles"][i] for i in matching_indices]
    filtered_data["times"] = [data["times"][i] for i in matching_indices]
    filtered_data["chunk_ids"] = [data["chunk_ids"][i] for i in matching_indices]

    if isinstance(data["vectors"], torch.Tensor):
        filtered_data["vectors"] = data["vectors"][matching_indices]
    else:
        filtered_data["vectors"] = [data["vectors"][i] for i in matching_indices]

    filtered_data["texts"] = [data["texts"][i] for i in matching_indices]
    filtered_data["texts_short"] = [data["texts_short"][i] for i in matching_indices]
    filtered_data["texts_vis"] = [data["texts_vis"][i] for i in matching_indices]

    return filtered_data


@time_tracker
def retrieve(query, data, N, embed_model, embed_tokenizer):
    print("[SOOWAN] retrieve : 진입")
    logging.info(f"Retrieval for query: '{query}'")
    logging.info(f"Available documents: {len(data['vectors'])}")

    try:
        sim_score = cal_sim_score(query, data["vectors"], embed_model, embed_tokenizer)
        logging.info(f"Similarity score shape: {sim_score.shape}")

        bm25_score = cal_bm25_score(query, data["texts_short"], embed_tokenizer)
        logging.info(f"BM25 score shape: {bm25_score.shape}")

        scaled_sim_score = min_max_scaling(sim_score)
        scaled_bm25_score = min_max_scaling(bm25_score)

        score = scaled_sim_score * 0.4 + scaled_bm25_score * 0.6
        score_values = score[:, 0, 0]
        top_k = score[:, 0, 0].argsort()[-N:][::-1]

        logging.info(f"Top {N} document indices: {top_k}")
        logging.info(f"Top {N} document scores: {[score[:, 0, 0][i] for i in top_k]}")
        logging.info(f"Top document titles: {[data['titles'][i] for i in top_k]}")

        documents = ""
        documents_list = []
        for i, index in enumerate(top_k):
            score_str = f"{score_values[index]:.4f}"
            documents += f"{i+1}번째 검색자료 (출처:{data['file_names'][index]}) :\n{data['texts_short'][index]}, , Score: {score_str}\n"
            documents_list.append(
                {
                    "file_name": data["file_names"][index],
                    "title": data["titles"][index],
                    "contents": data["texts_vis"][index],
                    "chunk_id": data["chunk_ids"][index],
                }
            )
        print("-------------자료 검색 성공--------------")
        print("-------", documents_list, "-------")
        print("---------------------------------------")
        return documents, documents_list

    except Exception as e:
        logging.error(f"Retrieval error: {str(e)}", exc_info=True)
        return "", []


@time_tracker
def expand_time_range_if_needed(time_bound, data, min_docs=50):
    """
    시간 필터링 결과가 너무 적은 경우 자동으로 시간 범위를 확장하는 함수
    """
    if time_bound == "all" or time_bound == "1900-01-01:2099-01-01":
        print(f"[시간 범위] 전체 기간 사용")
        return data

    filtered_data = sort_by_time(time_bound, data)
    filtered_count = len(filtered_data.get("times", []))

    if filtered_count >= min_docs:
        print(f"[시간 범위] 원래 범위로 충분한 문서 확보: {filtered_count}개")
        return filtered_data

    print(f"[시간 범위 확장] 원래 범위는 {filtered_count}개 문서만 제공 (최소 필요: {min_docs}개)")

    date_format = "%Y-%m-%d"
    try:
        start_date = datetime.strptime(time_bound.split(":")[0], date_format)
        end_date = datetime.strptime(time_bound.split(":")[1], date_format)
    except Exception as e:
        print(f"[시간 범위 오류] 날짜 형식 오류: {time_bound}, 오류: {e}")
        return data

    expansions = [
        (3, "3개월"),
        (6, "6개월"),
        (12, "1년"),
        (24, "2년"),
        (60, "5년"),
    ]

    for months, label in expansions:
        new_start = start_date - timedelta(days=30 * months // 2)
        new_end = end_date + timedelta(days=30 * months // 2)

        new_range = f"{new_start.strftime(date_format)}:{new_end.strftime(date_format)}"
        print(f"[시간 범위 확장] {label} 확장 시도: {new_range}")

        expanded_data = sort_by_time(new_range, data)
        expanded_count = len(expanded_data.get("times", []))

        if expanded_count >= min_docs:
            print(f"[시간 범위 확장] {label} 확장으로 {expanded_count}개 문서 확보")
            return expanded_data

    print(f"[시간 범위 확장] 모든 확장 시도 실패, 전체 데이터셋 사용")
    return data


@time_tracker
def cal_sim_score(query, chunks, embed_model, embed_tokenizer):
    print("[SOOWAN] cal_sim_score : 진입 / query : ", query)
    query_V = embed(query, embed_model, embed_tokenizer)
    print("[SOOWAN] cal_sim_score : query_V 생산 완료")
    if len(query_V.shape) == 1:
        query_V = query_V.unsqueeze(0)
        print("[SOOWAN] cal_sim_score : query_V.shape == 1")
    score = []
    for chunk in chunks:
        if len(chunk.shape) == 1:
            chunk = chunk.unsqueeze(0)
        query_norm = query_V / query_V.norm(dim=1)[:, None]
        chunk_norm = chunk / chunk.norm(dim=1)[:, None]
        tmp = torch.mm(query_norm, chunk_norm.transpose(0, 1)) * 100
        score.append(tmp.detach())
    return np.array(score)


@time_tracker
def cal_bm25_score(query, indexes, embed_tokenizer):
    logging.info(f"Starting BM25 calculation for query: {query}")
    logging.info(f"Document count: {len(indexes)}")

    if not indexes:
        logging.warning("Empty document list provided to BM25")
        return np.zeros(0)

    tokenized_corpus = []
    for i, text in enumerate(indexes):
        try:
            tokens = embed_tokenizer(
                text,
                return_token_type_ids=False,
                return_attention_mask=False,
                return_offsets_mapping=False,
            )
            tokens = embed_tokenizer.convert_ids_to_tokens(tokens["input_ids"])
            if len(tokens) == 0:
                logging.warning(f"Document {i} tokenized to empty list")
                tokens = ["<empty>"]
            tokenized_corpus.append(tokens)
        except Exception as e:
            logging.error(f"Failed to tokenize document {i}: {str(e)}")
            tokenized_corpus.append(["<error>"])

    try:
        bm25 = rank_bm25.BM25Okapi(tokenized_corpus)
        tokenized_query = embed_tokenizer.convert_ids_to_tokens(embed_tokenizer(query)["input_ids"])
        scores = bm25.get_scores(tokenized_query)

        if np.isnan(scores).any() or np.isinf(scores).any():
            logging.warning("BM25 produced NaN/Inf scores - replacing with zeros")
            scores = np.nan_to_num(scores)

        logging.info(
            f"BM25 scores: min={scores.min():.4f}, max={scores.max():.4f}, mean={scores.mean():.4f}"
        )
        return scores
    except Exception as e:
        logging.error(f"BM25 scoring failed: {str(e)}")
        return np.zeros(len(indexes))


@time_tracker
def embed(query, embed_model, embed_tokenizer):
    print("[SOOWAN] embed: 진입")
    inputs = embed_tokenizer(query, padding=True, truncation=True, return_tensors="pt")
    embeddings, _ = embed_model(**inputs, return_dict=False)
    print("[SOOWAN] embed: 완료")
    return embeddings[0][0]


@time_tracker
def min_max_scaling(arr):
    arr_min = arr.min()
    arr_max = arr.max()
    if arr_max == arr_min:
        print("[SOOWAN] min_max_scaling: Zero range detected, returning zeros.")
        return np.zeros_like(arr)
    return (arr - arr_min) / (arr_max - arr_min)


@time_tracker
async def generate(docs, query, model, tokenizer, config):
    PROMPT = GENERATE_PROMPT_TEMPLATE.format(docs=docs, query=query)
    print("Inference steps")
    if config.use_vllm:
        from vllm import SamplingParams
        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        accepted_request_id = str(uuid.uuid4())
        answer = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
    else:
        input_ids = tokenizer(PROMPT, return_tensors="pt", truncation=True, max_length=4024).to(
            "cuda"
        )
        token_count = input_ids["input_ids"].shape[1]
        outputs = model.generate(
            **input_ids,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )
        answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)
        print(answer)
        print(">>> decode done, returning answer")
    return answer


@time_tracker
async def collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id):
    import asyncio, concurrent.futures
    print("[SOOWAN] collect_vllm_text 진입 PROMPT: ")
    outputs = []
    async for output in model.generate(PROMPT, request_id=accepted_request_id, sampling_params=sampling_params):
        outputs.append(output)
    if not outputs:
        raise RuntimeError("No outputs were generated by the model.")
    final_output = next((o for o in outputs if getattr(o, "finished", False)), outputs[-1])
    answer = "".join([getattr(comp, "text", "") for comp in getattr(final_output, "outputs", [])])
    return answer


@time_tracker
async def generate_answer_stream(query, docs, model, tokenizer, config):
    prompt = STREAM_PROMPT_TEMPLATE.format(docs=docs, query=query)
    print("최종 LLM 추론용 prompt 생성 : ", prompt)
    if config.use_vllm:
        from vllm import SamplingParams
        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        request_id = str(uuid.uuid4())
        async for partial_chunk in collect_vllm_text_stream(prompt, model, sampling_params, request_id):
            yield partial_chunk
    else:
        import torch
        from transformers import TextIteratorStreamer

        input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4024).to("cuda")
        streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
        generation_kwargs = dict(
            **input_ids,
            streamer=streamer,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        import threading

        t = threading.Thread(target=model.generate, kwargs=generation_kwargs)
        t.start()
        for new_token in streamer:
            yield new_token

@time_tracker
async def collect_vllm_text_stream(prompt, engine: AsyncLLMEngine, sampling_params, request_id) -> str:
    async for request_output in engine.generate(prompt, request_id=request_id, sampling_params=sampling_params):
        if not request_output.outputs:
            continue
        for completion in request_output.outputs:
            yield completion.text

# -------------------------------------------------------------------------
# PROCESS IMAGE QUERY (IMAGE TO TEXT)
# -------------------------------------------------------------------------



# ---------------------------
# *** 새로 추가된 generate_sql 함수 ***
# *** (기존에는 SQL_NS.py 안에 있었음) ***
# ---------------------------
@time_tracker
async def generate_sql(user_query, model, tokenizer, config):
    """
    기존 SQL_NS.py에서 VLLM을 통해 <unno>, <class>, <pol_port>, <pod_port>를 추출한 뒤
    run_sql_unno, run_sql_bl를 호출하는 로직을 RAG.py로 옮겼습니다.
    """
    # 먼저 메타데이터를 읽어옴
    metadata_location, metadata_unno = get_metadata(config)
    print("")
    # 프롬프트 생성
    PROMPT = SQL_EXTRACTION_PROMPT_TEMPLATE.format(metadata_location=metadata_location, query=user_query)

    from vllm import SamplingParams
    import uuid

    sampling_params = SamplingParams(
        max_tokens=config.model.max_new_tokens,
        temperature=config.model.temperature,
        top_k=config.model.top_k,
        top_p=config.model.top_p,
        repetition_penalty=config.model.repetition_penalty,
    )

    # 최대 3회 시도
    max_attempts = 3
    attempt = 0
    UN_number = UN_class = POL = POD = "NULL"
    unno_pattern = r'<unno.*?>(.*?)<unno.*?>'
    class_pattern = r'<class.*?>(.*?)<class.*?>'
    pol_port_pattern = r'<pol_port.*?>(.*?)<pol_port.*?>'
    pod_port_pattern = r'<pod_port.*?>(.*?)<pod_port.*?>'

    while attempt < max_attempts:
        accepted_request_id = str(uuid.uuid4())
        outputs_result = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
        print(f"[GENERATE_SQL] Attempt {attempt+1}, SQL Model Outputs: {outputs_result}")

        match_unno = re.search(unno_pattern, outputs_result, re.DOTALL)
        UN_number = match_unno.group(1).strip() if match_unno else "NULL"

        match_class = re.search(class_pattern, outputs_result, re.DOTALL)
        UN_class = match_class.group(1).strip() if match_class else "NULL"

        match_pol = re.search(pol_port_pattern, outputs_result, re.DOTALL)
        POL = match_pol.group(1).strip() if match_pol else "NULL"

        match_pod = re.search(pod_port_pattern, outputs_result, re.DOTALL)
        POD = match_pod.group(1).strip() if match_pod else "NULL"

        print(f"[GENERATE_SQL] 추출 결과 - UN_number: {UN_number}, UN_class: {UN_class}, POL: {POL}, POD: {POD}")

        # 조건: (UN_number != NULL or UN_class != NULL) and (POL != NULL) and (POD != NULL)
        if ((UN_number != "NULL" or UN_class != "NULL") and POL != "NULL" and POD != "NULL"):
            break
        attempt += 1

    print(f"[GENERATE_SQL] 최종 추출 값 - UN_number: {UN_number}, UN_class: {UN_class}, POL: {POL}, POD: {POD}")

    # run_sql_unno로 DG 가능 여부 확인
    final_sql_query, result = run_sql_unno(UN_class, UN_number, POL, POD)
    # 상세 B/L SQL
    detailed_sql_query, detailed_result = run_sql_bl(UN_class, UN_number, POL, POD)

    # Temporary: title, explain, table_json, chart_json = None
    title, explain, table_json, chart_json = (None,) * 4

    return final_sql_query, title, explain, result, chart_json, detailed_result

```


-----------------

# Base-Knowledge

 - 위 파일들은 LLM 모델을 활용한 사내 RAG 서비스의 소스 코드입니다.
 - 파일 트리와 각 파일의 내용이 코드 블록 내에 포함되어, 프로젝트의 현재 구조와 상태를 한눈에 파악할 수 있습니다.
 - vLLM과 ray를 활용하여 사용성 및 추론 성능을 개선하였습니다.
 - Langchain을 활용하여 reqeust_id별로 대화를 저장하고 활용할 수 있습니다.
 - 에러 발생 시 로깅을 통해 문제를 추적할 수 있도록 설계되었습니다.


# Answer-Rule

 1. 추후 소스 코드 개선, 구조 변경, 에러 로그 추가 등 다양한 요구사항을 반영할 수 있는 확장성을 고려합니다.
 2. 전체 코드는 한국어로 주석 및 설명이 포함되어, 이해와 유지보수가 용이하도록 작성됩니다.


# My-Requirements

 1. 현재 코드에서 보다시피, ray_utils.py에 있는 image_query를 RAG.py로 옮겨서 model generate 파트를 한군데로 통일하고 싶어.
 2. image_query에 대해서, RAG.py에 스트리밍으로 답변을 생성하는 image_streaming_query를 만들어줘.
 3. image_query와 image_streaming_query를 두 함수 모두 기존에 있는 함수들의 답변 양식과 동일해.
 4. 변경된 내역을 바탕으로 app.py, ray_utils.py에도 이를 반영해줘.
