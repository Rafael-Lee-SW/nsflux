# Project Tree of RAG for company

```
├─ app.py
├─ config.yaml
├─ core/RAG.py
├─ core/SQL_NS.py
├─ prompt/prompt_rag.py
├─ prompt/prompt_sql.py
├─ ray_deploy/langchain.py
├─ ray_deploy/ray_setup.py
├─ ray_deploy/ray_utils.py
├─ utils/utils_format.py
└─ utils/utils_load.py
```

--- config.yaml

```yaml

# config.yaml
# Server : 2x H100 (80 GB SXM5), 52 CPU cores, 483.2 GB RAM, 6 TB SSD
### Model
model_id : 'google/gemma-3-27b-it'
response_url : "http://202.20.84.16:8083/responseToUI"
# response_url : "https://eo5smhcazmp1bqe.m.pipedream.net"

### ray
ray:
  actor_count: 1                  # 총 Actor 개수(same as num_replicas)
  num_gpus: 1                     # 각 Actor(Node)가 점유하고 있는 GPU 갯수
  num_cpus: 24                    # 각 Actor(Node)가 점유하고 있는 CPU 갯수 (1 actor 시에 gpu 48개, 2 actor 시에 gpu 24개 할당)
  max_batch_size: 5               # max_concurrency(actor 최대 동시 처리량, default 1000)로 대체해도 됨
  batch_wait_timeout: 0.05        
  max_ongoing_requests: 100       # ray.serve에서 deployment setting으로 동시 요청 처리 갯수를 의미함(Batch랑 다름)

### vllm
use_vllm: True                    # vLLM 사용 여부
vllm:
  enable_prefix_caching: True
  scheduler_delay_factor: 0.1
  enable_chunked_prefill: True
  tensor_parallel_size: 1         # vLLM의 GPU 사용 갯수 (!!!! num_gpus 보다 작아야 함 !!!!)
  max_num_seqs: 128               # v1에 따른 상향
  max_num_batched_tokens: 34000   # v1에 따른 상향
  block_size: 128                 # 미적용
  gpu_memory_utilization: 0.99    # v0: 0.95 / v1: 0.99로 상향
  ### 모델 변경에 따른 추가된 설정
  max_model_len: 20000            # For the new model (Gemma2 : 8192) / Gemma3는 1xH100(SXM5)일 경우 최대 22000~23000, so that 20000으로 세팅
  ### v1에 따른 새로운 인자값
  disable_custom_all_reduce: true
  # enable_memory_defrag: True      # Gemma3 이식 작업에서 도입, 현재 미사용
  # disable_sliding_window: True    # Gemma3 이식 작업에서 도입, 현재 미사용, sliding window 비활성화 - cascade attention과 충돌이 나서 이를 비활성화
  ### Gemma3 - Multi Modal 이미지 기능에 따른 새로운 인자값
  mm_processor_kwargs:            # 이미지 처리 시 사용되는 추가 인자 정의
    do_pan_and_scan: True         # 이미지 내 객체 감지 및 영역 스캔 기능을 활성화
  disable_mm_preprocessor_cache: False # 이미지 전처리 캐시를 비활성화 할지 여부
  limit_mm_per_prompt:            # 프롬프트 당 허용되는 멀티모달(예, 이미지) 입력의 최대 개수
    image: 2                      # 최대 image 처리 개수

### model huggingface setting and sampling params
model:
  quantization_4bit : False       # Quantize 4-bit
  quantization_8bit : False       # Quantize 8-bit
  max_new_tokens : 2048           # 생성할 최대 토큰 수
  do_sample : False               # True 일때만 아래가 적용
  temperature : 1.0               # 텍스트 다양성 조정: 높을수록 창의력 향상 (1.0)
  top_k : 30                      # top-k 샘플링: 상위 k개의 후보 토큰 중 하나를 선택 (50)
  top_p : 1.0                     # top-p 샘플링: 누적 확률을 기준으로 후보 토큰을 선택 (1.0 보다 낮을수록 창의력 증가)
  repetition_penalty : 1.0        # 같은 단어를 반복해서 출력하지 않도록 패널티를 부여 (1.0 보다 클수록 페널티 증가)
embed_model_id : 'BM-K/KoSimCSE-roberta-multitask'
# cache_dir : "D:/huggingface" # Windows Local
# cache_dir : "/media/user/7340afbb-e4ce-4a38-8210-c6362e85eae7/RAG/RAG_application/huggingface" # Local
cache_dir : "/workspace/huggingface"  # Docker

### Data
data_path : '/workspace/data/0228_DB_.json'     # VectorDB Path - New one (계약서 데이터 포함)
# data_path : 'data/1104_NS_DB_old.json' # VectorDB Path - Old one
metadata_path : '/workspace/data/Metadata.json' # Metadata.json Path
metadata_unno : '/workspace/data/METADATA_OPRAIMDG.json'
sql_data_path : '/workspace/data/poc.db'        # SQLite 데이터베이스 Path

### Retrieve
N : 5 # Retrieve top N chunks

### Others
beep : '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------'
seed : 4734                     # Radom Seed
k : 15                          # SQL Max Rows (None=MAX)

```


--- app.py

```python

import os
# Setting environment variable
# os.environ["TRANSFORMERS_CACHE"] = "/workspace/huggingface"
os.environ["HF_HOME"] = "/workspace/huggingface"
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# For the Huggingface Token setting
os.environ["HF_TOKEN_PATH"] = "/root/.cache/huggingface/token"
# Change to GNU to using OpenMP. Because this is more friendly with CUDA(NVIDIA),
# and Some library(Pytorch, Numpy, vLLM etc) use the OpenMP so that set the GNU is better.
# OpenMP: Open-Multi-Processing API
os.environ["MKL_THREADING_LAYER"] = "GNU"
# Increase download timeout (in seconds)
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "60"
# Use the vLLM as v1 version
os.environ["VLLM_USE_V1"] = "1"
os.environ["VLLM_STANDBY_MEM"] = "0"
os.environ["VLLM_METRICS_LEVEL"] = "1"
os.environ["VLLM_PROFILE_MEMORY"]= "1"
# GPU 단독 사용(박상제 연구원님이랑 분기점 - 연구원님 0번 GPU, 수완 1번 GPU - 기본 안정화 세팅은 0번 GPU)
os.environ["CUDA_VISIBLE_DEVICES"] = "0"

# 토크나이저 병렬 처리 명시적 비활성화
os.environ["TOKENIZERS_PARALLELISM"] = "false"

print("[[TEST]]")

from flask import (
    Flask,
    request,
    Response,
    render_template,
    jsonify,
    g,
    stream_with_context,
)
import json
from datetime import datetime

# Import the Ray modules
from ray_deploy import init_ray, InferenceService, SSEQueueManager
# Import utils
from utils import random_seed, error_format, send_data_to_server, process_format_to_response

# ------ checking process of the thread level
import logging
import threading

# 로깅 설정: 요청 처리 시간과 현재 스레드 이름을 기록
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s %(levelname)s [%(threadName)s] %(message)s'
)

import ray
from ray import serve
import uuid
import asyncio
import time

# Configuration
import yaml
from box import Box
with open("./config.yaml", "r") as f:
    config_yaml = yaml.load(f, Loader=yaml.FullLoader)
    config = Box(config_yaml)
random_seed(config.seed)

########## Ray Dashboard 8265 port ##########
init_ray()  # Initialize the Ray
sse_manager = SSEQueueManager.options(name="SSEQueueManager").remote()
serve.start(detached=True)

#### Ray-Actor 다중 ####
inference_service = InferenceService.options(num_replicas=config.ray.actor_count).bind(config)
serve.run(inference_service)
inference_handle = serve.get_deployment_handle("inference", app_name="default")

#### Ray-Actor 단독 ####
# inference_actor = InferenceActor.options(num_cpus=config.ray.num_cpus, num_gpus=config.ray.num_gpus).remote(config)

########## FLASK APP setting ##########
app = Flask(__name__)
content_type = "application/json; charset=utf-8"

# 기본 페이지를 불러오는 라우트
@app.route("/")
def index():
    return render_template("index.html")  # index.html을 렌더링

# Test 페이지를 불러오는 라우트
@app.route("/test")
def test_page():
    return render_template("index_test.html")

# chatroomPage 페이지를 불러오는 라우트
@app.route("/chat")
def chat_page():
    return render_template("chatroom.html")

# data 관리
from data_control.data_control import data_control_bp
app.register_blueprint(data_control_bp, url_prefix="/data")

# --------------------- Non Streaming part ----------------------------
@app.route("/query", methods=["POST"])
async def query():
    try:
        # Log when the query is received
        receive_time = datetime.now().isoformat()
        print(f"[APP] Received /query request at {receive_time}")
        
        # Optionally, attach the client time if desired:
        http_query = request.json  # 클라이언트로부터 JSON 요청 수신
        
        http_query["server_receive_time"] = receive_time
        
        # Ray Serve 배포된 서비스를 통해 추론 요청 (자동으로 로드밸런싱됨)
        # result = await inference_actor.process_query.remote(http_query) # 단일
        result = await inference_handle.query.remote(http_query) # 다중
        if isinstance(result, dict):
            result = json.dumps(result, ensure_ascii=False)
        print("APP.py - 결과: ", result)
        return Response(result, content_type=content_type)
    except Exception as e:
        error_resp = error_format(f"서버 처리 중 오류 발생: {str(e)}", 500)
        return Response(error_resp, content_type=content_type)

# --------------------- Streaming part ----------------------------
@app.route("/query_stream", methods=["POST"])
def query_stream():
    """
    POST 방식 SSE 스트리밍 엔드포인트.
    클라이언트가 아래 필드들을 포함한 JSON을 보내면:
      - qry_id, user_id, page_id, auth_class, qry_contents, qry_time
    auth_class는 내부적으로 'admin'으로 통일합니다.
    """
    body = request.json or {}
    # 새로운 필드 추출
    qry_id = body.get("qry_id")
    user_id = body.get("user_id")
    page_id = body.get("page_id")
    auth_class = "admin"  # 어떤 값이 와도 'admin'으로 통일
    qry_contents = body.get("qry_contents", "")
    qry_time = body.get("qry_time")  # 클라이언트 측 타임스탬프
    image_data = body.get("image_data") # Image Data
    # RAG 여부
    use_rag = body.get("rag")
    print(f"[DEBUG] RAG using = ", use_rag)
    
    print(f"[DEBUG] /query_stream called with qry_id='{qry_id}', user_id='{user_id}', page_id='{page_id}', qry_contents='{qry_contents}', qry_time='{qry_time}'")
    
    
    # 새로운 http_query 생성 – 내부 로직에서는 page_id를 채팅방 id로 사용
    http_query = {
        "qry_id": qry_id,
        "user_id": user_id,
        "page_id": page_id if page_id else str(uuid.uuid4()),
        "auth_class": auth_class,
        "qry_contents": qry_contents,
        "qry_time": qry_time,
        "use_rag" : use_rag,
    }
    
    # image_data가 존재하면 http_query에 추가하고, 길이(또는 타입)만 간략하게 출력
    if image_data is not None:
        http_query["image_data"] = image_data
        # image_data가 문자열이나 시퀀스 타입이면 길이를, 아니면 타입을 출력합니다.
        if hasattr(image_data, "__len__"):
            print(f"[DEBUG] image_data received: length={len(image_data)}")
        else:
            print(f"[DEBUG] image_data received: type={type(image_data)}")

    # http_query 전체를 출력할 때 image_data 내용은 생략(요약 정보만 출력)
    http_query_print = http_query.copy()
    if "image_data" in http_query_print:
        http_query_print["image_data"] = "<omitted>"
    print(f"[DEBUG] Built http_query: {http_query_print}")
    
    # Ray Serve를 통한 streaming 호출 (변경 없음, 내부 인자는 수정된 http_query)
    response = inference_handle.process_query_stream.remote(http_query)
    obj_ref = response._to_object_ref_sync()
    chat_id = ray.get(obj_ref)  # chat_id는 page_id
    print(f"[DEBUG] streaming chat_id={chat_id}")
    
    def sse_generator():
        try:
            while True:
                # SSEQueueManager에서 토큰을 가져옴 (chat_id 사용)
                token = ray.get(sse_manager.get_token.remote(chat_id, 120))
                if token is None or token == "[[STREAM_DONE]]":
                    break
                yield f"data: {token}\n\n"
        except Exception as e:
            error_token = json.dumps({"type": "error", "message": str(e)})
            yield f"data: {error_token}\n\n"
        finally:
            try:
                obj_ref = inference_handle.close_sse_queue.remote(chat_id)._to_object_ref_sync()
                ray.get(obj_ref)
            except Exception as ex:
                print(f"[DEBUG] Error closing SSE queue for {chat_id}: {str(ex)}")
            print("[DEBUG] SSE closed.")

    return Response(sse_generator(), mimetype="text/event-stream")

# --------------------- CLT Streaming part ----------------------------
@app.route("/queryToSLLM", methods=["POST"])
def query_stream_to_clt():
    """
    POST 방식 SSE 스트리밍 엔드포인트.
    클라이언트가 {"qry_id": "...", "user_id": "...", "page_id": "...", "qry_contents": "...", "qry_time": "..." }
    형태의 JSON을 보내면, 내부 Ray Serve SSE 스트림을 통해 처리한 후 지정된 response_url로 SSE 청크를 전송합니다.
    """
    # POST 요청 파라미터 파싱
    body = request.json or {}
    qry_id = body.get("qry_id", "")
    user_id = body.get("user_id", "")
    page_id = body.get("page_id", "")
    auth_class = "admin"  # 모든 요청을 'admin'으로 처리
    user_input = body.get("qry_contents", "")
    qry_time = body.get("qry_time", "")
    
    response_url = config.response_url

    print(f"[DEBUG] /queryToSLLM called with qry_id='{qry_id}', user_id='{user_id}', "
          f"page_id='{page_id}', qry_contents='{user_input}', qry_time='{qry_time}', url={response_url}")
    
    # 내부 로직에서는 page_id를 채팅방 ID(또는 request_id)로 사용합니다.
    http_query = {
        "qry_id": qry_id,
        "user_id": user_id,
        "page_id": page_id if page_id else str(uuid.uuid4()),
        "auth_class": auth_class,
        "qry_contents": user_input,
        "qry_time": qry_time,
        "response_url": response_url
    }
    print(f"[DEBUG] Built http_query={http_query}")

    # Ray Serve에 SSE 스트리밍 요청 보내기
    response = inference_handle.process_query_stream.remote(http_query)
    obj_ref = response._to_object_ref_sync()
    request_id = ray.get(obj_ref)
    print(f"[DEBUG] streaming request_id={request_id}")
    
    def sse_generator(request_id, response_url):
        token_buffer = []  # To collect tokens (for answer tokens only)
        last_sent_time = time.time()  # To track the last time data was sent
        answer_counter = 1  # 답변 업데이트 순번
        try:
            while True:
                token = ray.get(sse_manager.get_token.remote(request_id, 120))
                if token is None:
                    print("[DEBUG] 토큰이 None 반환됨. 종료합니다.")
                    break

                if isinstance(token, str):
                    token = token.strip()
                if token == "[[STREAM_DONE]]":
                    print("[DEBUG] 종료 토큰([[STREAM_DONE]]) 수신됨. 스트림 종료.")
                    break

                try:
                    token_dict = json.loads(token) if isinstance(token, str) else token
                except Exception as e:
                    print(f"[ERROR] JSON 파싱 실패: {e}. 원시 토큰: '{token}'")
                    continue

                # If token is a reference token, send it immediately
                if token_dict.get("type") == "reference":
                    print(f"[DEBUG] Reference token details: {token_dict}")
                    ref_format = process_format_to_response([token_dict], qry_id, continue_="C", update_index=answer_counter)
                    print(f"[DEBUG] Sending reference data: {json.dumps(ref_format, ensure_ascii=False, indent=2)}")
                    send_data_to_server(ref_format, response_url)
                    continue

                # Otherwise, accumulate answer tokens
                token_buffer.append(token_dict)
                current_time = time.time()
                # If 1 second has passed, flush the accumulated answer tokens
                if current_time - last_sent_time >= 1:
                    if len(token_buffer) > 0:
                        # Check if any token in the buffer signals termination.
                        final_continue = "E" if any(t.get("continue") == "E" for t in token_buffer) else "C"
                        print(f"[DEBUG] Flushing {len(token_buffer)} tokens with continue flag: {final_continue}")
                        buffer_format = process_format_to_response(token_buffer, qry_id, continue_=final_continue, update_index=answer_counter)
                        send_data_to_server(buffer_format, response_url)
                        token_buffer = []  # Reset the buffer
                        last_sent_time = current_time  # Update the last sent time
                        answer_counter += 1
                if token_dict.get("continue") == "E":
                    # Immediately flush the buffer with termination flag if needed
                    if len(token_buffer) > 0:
                        print(f"[DEBUG] Immediate flush due to termination flag in buffer (size {len(token_buffer)}).")
                        buffer_format = process_format_to_response(token_buffer, qry_id, continue_="E", update_index=answer_counter)
                        send_data_to_server(buffer_format, response_url)
                        token_buffer = []
                    break
            # After loop: if tokens remain, flush them with termination flag
            if len(token_buffer) > 0:
                print(f"[DEBUG] Final flush of remaining {len(token_buffer)} tokens with end flag.")
                buffer_format = process_format_to_response(token_buffer, qry_id, continue_="E", update_index=answer_counter)
                send_data_to_server(buffer_format, response_url)
        except Exception as e:
            print(f"[ERROR] sse_generator encountered an error: {e}")
        finally:
            try:
                obj_ref = inference_handle.close_sse_queue.remote(request_id)._to_object_ref_sync()
                ray.get(obj_ref)
            except Exception as ex:
                print(f"[DEBUG] Error closing SSE queue for {request_id}: {str(ex)}")
            print("[DEBUG] SSE closed.")

    
    # 별도의 스레드에서 SSE generator 실행
    job = threading.Thread(target=sse_generator, args=(request_id, response_url), daemon=False)
    job.start()

    # 클라이언트에는 즉시 "수신양호" 메시지를 JSON 형식으로 응답
    return Response(error_format("수신양호", 200, qry_id), content_type="application/json")


# --------------------- History & Reference part ----------------------------

# 새로 추가1: request_id로 대화 기록을 조회하는 API 엔드포인트
@app.route("/history", methods=["GET"])
def conversation_history():
    request_id = request.args.get("request_id", "")
    last_index = request.args.get("last_index")
    if not request_id:
        error_resp = error_format("request_id 파라미터가 필요합니다.", 400)
        return Response(error_resp, content_type="application/json; charset=utf-8")
    
    try:
        last_index = int(last_index) if last_index is not None else None
        response = inference_handle.get_history.remote(request_id, last_index=last_index)
        # DeploymentResponse를 ObjectRef로 변환
        obj_ref = response._to_object_ref_sync()
        history_data = ray.get(obj_ref)
        return jsonify(history_data)
    except Exception as e:
        print(f"[ERROR /history] {e}")
        error_resp = error_format(f"대화 기록 조회 오류: {str(e)}", 500)
        return Response(error_resp, content_type="application/json; charset=utf-8")

# 새로 추가2: request_id로 해당 답변의 참고자료를 볼 수 있는 API
@app.route("/reference", methods=["GET"])
def get_reference():
    request_id = request.args.get("request_id", "")
    msg_index_str = request.args.get("msg_index", "")
    if not request_id or not msg_index_str:
        error_resp = error_format("request_id와 msg_index 파라미터가 필요합니다.", 400)
        return Response(error_resp, content_type="application/json; charset=utf-8")
    
    try:
        msg_index = int(msg_index_str)
        # 먼저 history를 가져옴
        response = inference_handle.get_history.remote(request_id)
        obj_ref = response._to_object_ref_sync()
        history_data = ray.get(obj_ref)
        
        history_list = history_data.get("history", [])
        if msg_index < 0 or msg_index >= len(history_list):
            return jsonify({"error": "유효하지 않은 메시지 인덱스"}), 400
        
        message = history_list[msg_index]
        if message.get("role") != "ai":
            return jsonify({"error": "해당 메시지는 AI 응답이 아닙니다."}), 400
        
        chunk_ids = message.get("references", [])
        if not chunk_ids:
            return jsonify({"references": []})
        
        # chunk_ids에 해당하는 실제 참조 데이터 조회
        ref_response = inference_handle.get_reference_data.remote(chunk_ids)
        ref_obj_ref = ref_response._to_object_ref_sync()
        references = ray.get(ref_obj_ref)
        return jsonify({"references": references})
    except Exception as e:
        print(f"[ERROR /reference] {e}")
        error_resp = error_format(f"참조 조회 오류: {str(e)}", 500)
        return Response(error_resp, content_type="application/json; charset=utf-8")

# Flask app 실행
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=False)

```


--- ray_deploy/ray_setup.py

```python

# ray_deploy/ray_setup.py
import ray
from ray import serve

########## Starting Banner ############
from colorama import init, Fore, Style
init(autoreset=True)

BANNER = Fore.GREEN + r"""
'########:'##:::::::'##::::'##:'##::::'##::::::::::'##::: ##::'######::
 ##.....:: ##::::::: ##:::: ##:. ##::'##::::::::::: ###:: ##:'##... ##:
 ##::::::: ##::::::: ##:::: ##::. ##'##:::::::::::: ####: ##: ##:::..::
 ######::: ##::::::: ##:::: ##:::. ###::::::::::::: ## ## ##:. ######::
 ##...:::: ##::::::: ##:::: ##::: ## ##:::::::::::: ##. ####::..... ##:
 ##::::::: ##::::::: ##:::: ##:: ##:. ##::::::::::: ##:. ###:'##::: ##:
 ##::::::: ########:. #######:: ##:::. ##:'#######: ##::. ##:. ######::
..::::::::........:::.......:::..:::::..::.......::..::::..:::......:::
"""

def init_ray():
    print(BANNER)
    # Ray-Dashboard - GPU 상태, 사용 통계 등을 제공하는 모니터링 툴, host 0.0.0.0로 외부 접속을 허용하고, Default 포트인 8265으로 설정
    ray.init(
        include_dashboard=True,
        dashboard_host="0.0.0.0" # External IP accessable
        # dashboard_port=8265
    )
    print("Ray initialized. DashBoard running at http://192.222.54.254:8265") # New Server(2xH100)

```


--- ray_deploy/ray_utils.py

```python


# ray_deploy/ray_utils.py
import ray  # Ray library
from ray import serve
import json
import asyncio  # async I/O process module
from concurrent.futures import ProcessPoolExecutor  # 스레드 컨트롤
import uuid  # --- NEW OR MODIFIED ---
import time
from typing import Dict, Optional  # --- NEW OR MODIFIED ---
import threading  # To find out the usage of thread
import datetime

from core.RAG import (
    query_sort,
    specific_question,
    execute_rag,
    generate_answer,
    generate_answer_stream,
    image_query,
)  # hypothetically
from utils import (
    load_model,
    load_data,
    process_format_to_response,
    process_to_format,
    error_format,
)
# from summarizer import summarize_conversation
from utils.summarizer import summarize_conversation
from utils.debug_tracking import log_batch_info, log_system_info
# Langchain Memory system
from ray_deploy.langchain import CustomConversationBufferMemory, serialize_message

# Configuration
import yaml
from box import Box
with open("./config.yaml", "r") as f:
    config_yaml = yaml.load(f, Loader=yaml.FullLoader)
    config = Box(config_yaml)

@ray.remote  # From Decorator, Each Actor is allocated 1 GPU
class InferenceActor:
    async def __init__(self, config):
        self.config = config
        # 액터 내부에서 모델 및 토크나이저를 새로 로드 (GPU에 한 번만 로드)
        self.model, self.tokenizer, self.embed_model, self.embed_tokenizer = load_model(
            config
        )
        # 데이터는 캐시 파일을 통해 로드
        self.data = load_data(config.data_path)
        
        # 비동기 큐와 배치 처리 설정
        self.request_queue = asyncio.Queue()
        self.max_batch_size = config.ray.max_batch_size  # 최대 배치 수
        self.batch_wait_timeout = config.ray.batch_wait_timeout  # 배치당 처리 시간 - 이제 사용하지 아니함(연속 배치이기에 배치당 처리 시간이 무의미)

        # Actor 내부에서 ProcessPoolExecutor 생성 (직렬화 문제 회피)
        max_workers = int(min(config.ray.num_cpus * 0.8, (26*config.ray.actor_count)-4))
        self.process_pool = ProcessPoolExecutor(max_workers)

        # --- SSE Queue Manager --- 
        # Key = request_id, Value = an asyncio.Queue of partial token strings
        # A dictionary to store SSE queues for streaming requests
        self.queue_manager = ray.get_actor("SSEQueueManager")
        self.active_sse_queues: Dict[str, asyncio.Queue] = {}

        self.batch_counter = 0  # New counter to track batches

        self.memory_map = {}
        
        # 활성 작업 추적을 위한 변수 추가
        self.active_tasks = set()
        self.max_concurrent_tasks = config.ray.max_batch_size
        
        # 연속 배치 처리기 시작 (Continuous batch)
        asyncio.create_task(self.continuous_batch_processor())
        
    # -------------------------------------------------------------------------
    # GET MEMORY FOR SESSION
    # -------------------------------------------------------------------------
    def get_memory_for_session(self, request_id: str) -> CustomConversationBufferMemory:
        """
        세션별 Memory를 안전하게 가져오는 헬퍼 메서드.
        만약 memory_map에 request_id가 없으면 새로 생성해 저장 후 반환.
        """
        if request_id not in self.memory_map:
            print(f"[DEBUG] Creating new CustomConversationBufferMemory for session={request_id}")
            self.memory_map[request_id] = CustomConversationBufferMemory(return_messages=True)
        return self.memory_map[request_id]
    # -------------------------------------------------------------------------
    # CONTINUOUOS BATCH PROCESSOR
    # -------------------------------------------------------------------------
    async def continuous_batch_processor(self):
        """
        연속적인 배치 처리 - 최대 max_batch_size개의 요청이 항상 동시에 처리되도록 함
        """
        while True:
            # 사용 가능한 슬롯 확인
            available_slots = self.max_concurrent_tasks - len(self.active_tasks)
            
            if available_slots > 0:
                try:
                    # 새 요청 받기 (짧은 타임아웃으로 non-blocking 유지)
                    request_tuple = await asyncio.wait_for(
                        self.request_queue.get(), 
                        timeout=0.01
                    )
                    
                    # 비동기 처리 작업 생성
                    request_obj, fut, sse_queue = request_tuple
                    task = asyncio.create_task(
                        self._process_single_query(request_obj, fut, sse_queue)
                    )
                    
                    # 작업 완료 시 활성 목록에서 제거하는 콜백 설정
                    task.add_done_callback(
                        lambda t, task_ref=task: self.active_tasks.discard(task_ref)
                    )
                    
                    # 활성 작업 목록에 추가
                    self.active_tasks.add(task)
                    
                    print(f"[Continuous Batching] +1 request => active tasks now {len(self.active_tasks)}/{self.max_concurrent_tasks}")
                
                except asyncio.TimeoutError:
                    # 새 요청이 없으면 잠시 대기
                    await asyncio.sleep(0.01)
            else:
                # 모든 슬롯이 사용 중이면 작업 완료될 때까지 대기
                if self.active_tasks:
                    # 작업 중 하나라도 완료될 때까지 대기
                    done, pending = await asyncio.wait(
                        self.active_tasks, 
                        return_when=asyncio.FIRST_COMPLETED
                    )
                    
                    # 완료된 작업 확인
                    for task in done:
                        try:
                            await task
                        except Exception as e:
                            print(f"[ERROR] Task failed: {e}")
                    
                    print(f"[Continuous Batching] Tasks completed: {len(done)} => active tasks now {len(self.active_tasks)}/{self.max_concurrent_tasks}")
                else:
                    await asyncio.sleep(0.01)

    # -------------------------------------------------------------------------
    # PROCESS SINGLE QUERY (TEXT TO TEXT)
    # -------------------------------------------------------------------------
    async def _process_single_query(self, http_query_or_stream_dict, future, sse_queue):
        """
        Process a single query from the micro-batch. If 'sse_queue' is given,
        we do partial-token streaming. Otherwise, normal final result.
        """
        body = http_query_or_stream_dict["http_query"]
        request_check = body.get("qry_contents", "")
        print(
            f"[DEBUG] _process_single_query 시작: {time.strftime('%H:%M:%S')}, 요청 내용: {request_check}, 현재 스레드: {threading.current_thread().name}"
        )
        
        # RAG on/off using boolean
        use_rag = body.get("use_rag")
        
        if use_rag is False:
            print(f"[NOT-USING-RAG] RAG is FALSE")
            
            # Determine) === Is streaming or not ===
            request_id = None
            # For streaming, the dict contains a "request_id"
            if isinstance(http_query_or_stream_dict, dict) and "request_id" in http_query_or_stream_dict:
                request_id = http_query_or_stream_dict["request_id"]
                http_query = http_query_or_stream_dict["http_query"]
                is_streaming = True
                print(f"[STREAM] _process_single_query: request_id={request_id}")
            else:
                request_id = None
                http_query = http_query_or_stream_dict
                is_streaming = False
                print("[NORMAL] _process_single_query started...")
                
            # 1) bring the user's input query
            user_input = http_query.get("qry_contents", "")
            print("[PROCESS_SINGLE_QUERY] user input : ", user_input)
            
            # Determine) === Make a description of image ===
            image_data = http_query.get("image_data")
                
            # 2) Memory 객체 정보 가져오기 (없으면 새로 생성)
            page_id = http_query.get("page_id", request_id)
            memory = self.get_memory_for_session(page_id)
            
            # 4) LangChain Memory에서 이전 대화 이력(history) 추출
            past_context = memory.load_memory_variables({}).get("history", [])
            # history가 리스트 형식인 경우 (각 메시지가 별도 항목으로 저장되어 있다면)
            if isinstance(past_context, list):
                recent_messages = [msg if isinstance(msg, str) else msg.content for msg in past_context[-5:]]
                past_context = "\n\n".join(recent_messages)
            else:
                # 문자열인 경우, 메시지 구분자를 "\n\n"으로 가정하여 분리
                messages = str(past_context).split("\n\n")
                recent_messages = messages[-5:]
                past_context = "\n\n".join(recent_messages)
                
            docs = None
            retrieval = None
            chart = None
            
            await self._stream_partial_answer(user_input, docs, retrieval, chart, request_id, future, user_input, http_query)
            
        else:
            try:
                # Determine) === Is streaming or not ===
                request_id = None
                # For streaming, the dict contains a "request_id"
                if isinstance(http_query_or_stream_dict, dict) and "request_id" in http_query_or_stream_dict:
                    request_id = http_query_or_stream_dict["request_id"]
                    http_query = http_query_or_stream_dict["http_query"]
                    is_streaming = True
                    print(f"[STREAM] _process_single_query: request_id={request_id}")
                else:
                    request_id = None
                    http_query = http_query_or_stream_dict
                    is_streaming = False
                    print("[NORMAL] _process_single_query started...")
                    
                # 1) bring the user's input query
                user_input = http_query.get("qry_contents", "")
                print("[PROCESS_SINGLE_QUERY] user input : ", user_input)
                
                # Determine) === Make a description of image ===
                image_data = http_query.get("image_data")
                image_description = {"is_structured": False, "description": "이미지는 입력되지 않았습니다."}
                if image_data is not None:
                    print("[DEBUG] _process_single_query: image_data detected, initiating image_sorting process.")
                    image_description = await image_query(http_query, self.model, config) 
                    
                # 2) Memory 객체 정보 가져오기 (없으면 새로 생성)
                page_id = http_query.get("page_id", request_id)
                memory = self.get_memory_for_session(page_id)
                
                # 4) LangChain Memory에서 이전 대화 이력(history) 추출
                past_context = memory.load_memory_variables({}).get("history", [])
                # history가 리스트 형식인 경우 (각 메시지가 별도 항목으로 저장되어 있다면)
                if isinstance(past_context, list):
                    recent_messages = [msg if isinstance(msg, str) else msg.content for msg in past_context[-5:]]
                    past_context = "\n\n".join(recent_messages)
                else:
                    # 문자열인 경우, 메시지 구분자를 "\n\n"으로 가정하여 분리
                    messages = str(past_context).split("\n\n")
                    recent_messages = messages[-5:]
                    past_context = "\n\n".join(recent_messages)
                
                # ★ 토큰 수 계산 코드 추가 ★
                past_tokens = self.tokenizer.tokenize(str(past_context))
                query_tokens = self.tokenizer.tokenize(str(user_input))
                total_tokens = len(past_tokens) + len(query_tokens)
                print(f"[DEBUG] Token counts - 이전 대화: {len(past_tokens)}, 사용자 입력 질문: {len(query_tokens)}, 총합: {total_tokens}")
                
                # 5) 필요하다면 RAG 데이터를 다시 로드(1.16version 유지)
                self.data = load_data(self.config.data_path)  # if you want always-latest, else skip
                
                # 6) “대화 이력 + 현재 사용자 질문”을 Prompt에 합쳐서 RAG 수행
                params = {
                    "user_input": f"사용자 질문: {user_input} [이미지 설명: {image_description.get('description')}]",
                    "model": self.model,
                    "tokenizer": self.tokenizer,
                    "embed_model": self.embed_model,
                    "embed_tokenizer": self.embed_tokenizer,
                    "data": self.data,
                    "config": self.config,
                }
                print("[PROCESS_SINGLE_QUERY]... calling query_sort() ...")
                QU, KE, TA, TI = await query_sort(params)
                print(f"   ... query_sort => QU={QU}, KE={KE}, TA={TA}, TI={TI}")

                # 4) RAG
                if TA == "yes":
                    try:
                        print("[SOOWAN] config 설정 : ", self.config)
                        docs, docs_list = await execute_rag(
                            QU,
                            KE,
                            TA,
                            TI,
                            model=self.model,
                            tokenizer=self.tokenizer,
                            embed_model=self.embed_model,
                            embed_tokenizer=self.embed_tokenizer,
                            data=self.data,
                            config=self.config,
                        )
                        try:
                                                    # 기존 방식
                            retrieval, chart = process_to_format(docs_list, type="SQL")
                            # 수정된 방식 - Talbe,Chart 없이 Answer Part에 SQL 결과 전송.
                            # retrieval_sql = process_to_format(docs, type="Answer")
                            # await self.queue_manager.put_token.remote(request_id, retrieval_sql)
                        except Exception as e:
                            print("[ERROR] process_to_format (SQL) failed:", str(e))
                            retrieval, chart = [], None

                        # If streaming => partial tokens
                        if is_streaming:
                            print(
                                f"[STREAM] Starting partial generation for request_id={request_id}"
                            )
                            await self._stream_partial_answer(
                                QU, docs, retrieval, chart, request_id, future, user_input, http_query
                            )
                        else:
                            # normal final result
                            output = await generate_answer(
                                QU,
                                docs,
                                model=self.model,
                                tokenizer=self.tokenizer,
                                config=self.config,
                            )
                            answer = process_to_format([output, chart], type="Answer")
                            final_data = [retrieval, answer]
                            outputs = process_format_to_response(final_data, qry_id=None, continue_="C")
                            
                            # >>> Record used chunk IDs
                            # 변경 후: retrieval 결과에서 추출
                            chunk_ids_used = []
                            print("---------------- chunk_id 찾기 : ", retrieval.get("rsp_data", []))
                            for doc in retrieval.get("rsp_data", []):
                                if "chunk_id" in doc:
                                    chunk_ids_used.append(doc["chunk_id"])
                                                            
                            # 메모리에 저장
                            try:
                                memory.save_context(
                                    {
                                        "qry_contents": user_input,
                                        "qry_id": http_query.get("qry_id"),
                                        "user_id": http_query.get("user_id"),
                                        "auth_class": http_query.get("auth_class"),
                                        "qry_time": http_query.get("qry_time")
                                    },
                                    {
                                        "output": output,
                                        "chunk_ids": chunk_ids_used
                                    }
                                )
                            except Exception as e:
                                print(f"[ERROR memory.save_context] {e}")
                            # >>> CHANGED -----------------------------------------------------
                            future.set_result(outputs)

                    except Exception as e:
                        outputs = error_format("내부 Excel 에 해당 자료가 없습니다.", 551)
                        future.set_result(outputs)

                else:
                    try:
                        print("[SOOWAN] TA is No, before make a retrieval")
                        QU, KE, TA, TI = await specific_question(params) # TA == no, so that have to remake the question based on history
                        
                        docs, docs_list = await execute_rag(
                            QU,
                            KE,
                            TA,
                            TI,
                            model=self.model,
                            tokenizer=self.tokenizer,
                            embed_model=self.embed_model,
                            embed_tokenizer=self.embed_tokenizer,
                            data=self.data,
                            config=self.config,
                        )
                        retrieval = process_to_format(docs_list, type="Retrieval")
                        print("[SOOWAN] TA is No, and make a retrieval is successed")
                        if is_streaming:
                            print(
                                f"[STREAM] Starting partial generation for request_id={request_id}"
                            )
                            await self._stream_partial_answer(
                                QU, docs, retrieval, None, request_id, future, user_input, http_query
                            )
                        else:
                            output = await generate_answer(
                                QU,
                                docs,
                                model=self.model,
                                tokenizer=self.tokenizer,
                                config=self.config,
                            )
                            print("process_to_format 이후에 OUTPUT 생성 완료")
                            answer = process_to_format([output], type="Answer")
                            print("process_to_format 이후에 ANSWER까지 생성 완료")
                            final_data = [retrieval, answer]
                            outputs = process_format_to_response(final_data, qry_id=None, continue_="C")
                            
                            # >>> CHANGED: Record used chunk ID
                            chunk_ids_used = []
                            print("---------------- chunk_id 찾기 : ", retrieval.get("rsp_data", []))
                            for doc in retrieval.get("rsp_data", []):
                                if "chunk_id" in doc:
                                    chunk_ids_used.append(doc["chunk_id"])
                                    
                            # 메모리 저장
                            try:
                                memory.save_context(
                                    {
                                        "qry_contents": user_input,
                                        "qry_id": http_query.get("qry_id"),
                                        "user_id": http_query.get("user_id"),
                                        "auth_class": http_query.get("auth_class"),
                                        "qry_time": http_query.get("qry_time")
                                    },
                                    {
                                        "output": output,
                                        "chunk_ids": chunk_ids_used
                                    }
                                )
                            except Exception as e:
                                print(f"[ERROR memory.save_context] {e}")
                            # --------------------------------------------------------------------
                            
                            future.set_result(outputs)

                    except Exception as e:
                        # ====== 이 부분에서 SSE를 즉시 닫고 스트리밍 종료 ======
                        err_msg = f"[ERROR] 처리 중 오류 발생: {str(e)}"
                        print(err_msg)

                        # SSE 전송 (error 이벤트)
                        if request_id:
                            try:
                                error_token = json.dumps({"type": "error", "message": err_msg}, ensure_ascii=False)
                                await self.queue_manager.put_token.remote(request_id, error_token)
                                # 스트리밍 종료
                                await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
                            except Exception as e2:
                                print(f"[ERROR] SSE 전송 중 추가 예외 발생: {str(e2)}")
                            finally:
                                # SSEQueue 정리
                                await self.close_sse_queue(request_id)

                        # Future 응답도 에러로
                        future.set_result(error_format(str(e), 500))
                        return
                    
            except Exception as e:
                err_msg = f"[ERROR] 처리 중 오류 발생: {str(e)}"
                print("[ERROR]", err_msg)
                # SSE 스트리밍인 경우 error 토큰과 종료 토큰 전송
                if request_id:
                    try:
                        error_token = json.dumps({"type": "error", "message": err_msg}, ensure_ascii=False)
                        await self.queue_manager.put_token.remote(request_id, error_token)
                    except Exception as e2:
                        print(f"[ERROR] SSE 전송 중 추가 예외 발생: {str(e2)}")
                future.set_result(error_format(err_msg, 500))
            finally:
                # 스트리밍 요청인 경우 반드시 SSE 큐에 종료 토큰을 넣고 큐를 정리한다.
                if request_id:
                    try:
                        await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
                    except Exception as ex:
                        print(f"[DEBUG] Error putting STREAM_DONE: {str(ex)}")
                    await self.close_sse_queue(request_id)
                
    # ------------------------------------------------------------
    # HELPER FOR STREAMING PARTIAL ANSWERS (Modified to send reference)
    # ------------------------------------------------------------
    async def _stream_partial_answer(
        self, QU, docs, retrieval, chart, request_id, future, user_input, http_query
    ):
        """
        Instead of returning a final string, we generate partial tokens
        and push them to the SSE queue in real time.
        We'll do a "delta" approach so each chunk is only what's newly added.
        """
        print(
            f"[STREAM] _stream_partial_answer => request_id={request_id}, chart={chart}"
        )
        
        # 먼저, 참조 데이터 전송: type을 "reference"로 명시
        reference_json = json.dumps({
            "type": "reference",
            "status_code": 200,
            "result": "OK",
            "detail": "Reference data",
            "evt_time": datetime.datetime.now().isoformat(),
            "data_list": [retrieval]
        }, ensure_ascii=False)
        # Debug: print the reference JSON before sending
        print(f"[DEBUG] Prepared reference data: {reference_json}")
        await self.queue_manager.put_token.remote(request_id, reference_json)
        
        print(f"[STREAM] Sent reference data for request_id={request_id}")
        
        # 1) 메모리 가져오기 (없으면 생성)
        try:
            memory = self.get_memory_for_session(request_id)
        except Exception as e:
            msg = f"[STREAM] Error retrieving memory for {request_id}: {str(e)}"
            print(msg)
            # 에러 응답을 SSE로 전송하고 종료
            error_token = json.dumps({"type":"error","message":msg}, ensure_ascii=False)
            await self.queue_manager.put_token.remote(request_id, error_token)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            future.set_result(error_format(msg, 500))
            return
        
        # 2) 과거 대화 이력 로드
        try:
            past_context = memory.load_memory_variables({})["history"]
            # history가 리스트 형식인 경우 (각 메시지가 별도 항목으로 저장되어 있다면)
            if isinstance(past_context, list):
                recent_messages = [msg if isinstance(msg, str) else msg.content for msg in past_context[-5:]]
                past_context = "\n\n".join(recent_messages)
            else:
                # 문자열인 경우, 메시지 구분자를 "\n\n"으로 가정하여 분리
                messages = str(past_context).split("\n\n")
                recent_messages = messages[-5:]
                past_context = "\n\n".join(recent_messages)
            
        except KeyError:
            # 만약 "history" 키가 없으면 빈 문자열로 처리
            print(f"[STREAM] No 'history' in memory for {request_id}, using empty.")
            past_context = ""
        except Exception as e:
            msg = f"[STREAM] load_memory_variables error for {request_id}: {str(e)}"
            print(msg)
            error_token = json.dumps({"type":"error","message":msg}, ensure_ascii=False)
            await self.queue_manager.put_token.remote(request_id, error_token)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            future.set_result(error_format(msg, 500))
            return

        # 3) 최종 프롬프트 구성
        final_query = f"{past_context}\n\n[사용자 질문]\n{QU}"
        print(f"[STREAM] final_query = \n{final_query}")
        
        # ★ 토큰 수 계산 코드 추가 ★
        # retrieval 자료는 dict나 리스트일 수 있으므로 문자열로 변환하여 토큰화합니다.
        retrieval_str = str(retrieval)
        # 각 입력값을 명시적으로 str()로 변환합니다.
        past_tokens = self.tokenizer.tokenize(str(past_context))
        query_tokens = self.tokenizer.tokenize(str(QU))
        retrieval_tokens = self.tokenizer.tokenize(retrieval_str)
        total_tokens = len(self.tokenizer.tokenize(str(final_query))) + len(retrieval_tokens)
        print(f"[DEBUG] Token counts - 이전 대화: {len(past_tokens)}, RAG 검색 자료: {len(retrieval_tokens)}, 사용자 구체화 질문: {len(query_tokens)}, 총합: {total_tokens}")
        
        partial_accumulator = ""

        try:
            print(
                f"[STREAM] SSE: calling generate_answer_stream for request_id={request_id}"
            )
            async for partial_text in generate_answer_stream(
                final_query, docs, self.model, self.tokenizer, self.config, http_query
            ):
                # print(f"[STREAM] Received partial_text: {partial_text}")
                new_text = partial_text[len(partial_accumulator) :]
                partial_accumulator = partial_text
                
                # 수정: new_text가 완전히 빈 문자열("")인 경우에만 건너뛰기
                if new_text == "":
                    continue
                
                # Wrap answer tokens in a JSON object with type "answer"
                answer_json = json.dumps({
                    "type": "answer",
                    "answer": new_text
                }, ensure_ascii=False)
                # Use the central SSEQueueManager to put tokens
                # print(f"[STREAM] Sending token: {answer_json}")
                await self.queue_manager.put_token.remote(request_id, answer_json)
            final_text = partial_accumulator
                
            # >>> CHANGED: Update conversation summary in streaming branch as well
            chunk_ids_used = []
            print("---------------- chunk_id 찾기 : ", retrieval.get("rsp_data", []))
            for doc in retrieval.get("rsp_data", []):
                if "chunk_id" in doc:
                    chunk_ids_used.append(doc["chunk_id"])
                    
            # 메모리 저장
            try:
                memory.save_context(
                    {
                        "qry_contents": user_input,
                        "qry_id": "",  # 필요한 경우 http_query에 있는 값을 넣음
                    },
                    {
                        "output": final_text,
                        "chunk_ids": chunk_ids_used
                    }
                )
            except Exception as e:
                print(f"[ERROR memory.save_context in stream] {e}")
            
            print("메시지 저장 직후 chunk_id 확인 : ", memory)

            # 최종 응답 구조
            if chart is not None:
                ans = process_to_format([final_text, chart], type="Answer")
                final_res = process_format_to_response(retrieval, ans)
            else:
                ans = process_to_format([final_text], type="Answer")
                final_res = process_format_to_response(retrieval, ans)
                
            # 담아서 보내기
            future.set_result(final_res)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            print(
                f"[STREAM] done => placed [[STREAM_DONE]] for request_id={request_id}"
            )
        except Exception as e:
            msg = f"[STREAM] error in partial streaming => {str(e)}"
            print(msg)
            future.set_result(error_format(msg, 500))
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")

    # --------------------------------------------------------
    # EXISTING METHODS FOR NORMAL QUERIES (unchanged)
    # --------------------------------------------------------
    async def process_query(self, http_query):
        """
        Existing synchronous method. Returns final string/dict once done.
        """
        loop = asyncio.get_event_loop()
        future = loop.create_future()
        # There's no SSE queue for normal queries
        sse_queue = None
        await self.request_queue.put((http_query, future, sse_queue))
        # print("self.request_queue : ", self.request_queue)
        return await future
    # ----------------------
    # 1) Streaming Entrypoint
    # ----------------------
    async def process_query_stream(self, http_query: dict) -> str:
        """
        /query_stream 호출 시 page_id(채팅방 id)를 기반으로 SSE queue 생성하고,
        대화 저장에 활용할 수 있도록 합니다.
        """
        # page_id를 채팅방 id로 사용 (없으면 생성)
        chat_id = http_query.get("page_id")
        if not chat_id:
            chat_id = str(uuid.uuid4())
        http_query["page_id"] = chat_id  # 강제 할당
        await self.queue_manager.create_queue.remote(chat_id)
        print(f"[STREAM] process_query_stream => chat_id={chat_id}")
        
        # http_query 전체를 출력할 때 image_data 내용은 생략(요약 정보만 출력)
        http_query_print = http_query.copy()
        if "image_data" in http_query_print:
            http_query_print["image_data"] = "<omitted>"
        print(f"[DEBUG] Built http_query: {http_query_print}")

        loop = asyncio.get_event_loop()
        final_future = loop.create_future()

        sse_queue = asyncio.Queue()
        self.active_sse_queues[chat_id] = sse_queue
        print(f"[STREAM] Created SSE queue for chat_id={chat_id}")

        # 기존과 동일하게 micro-batch queue에 푸시 (http_query에 새 필드들이 포함됨)
        queued_item = {
            "request_id": chat_id,   # 내부적으로 page_id를 request_id처럼 사용
            "http_query": http_query,
        }

        print(f"[STREAM] Putting item into request_queue for chat_id={chat_id}")
        await self.request_queue.put((queued_item, final_future, sse_queue))
        print(f"[STREAM] Done putting item in queue => chat_id={chat_id}")

        return chat_id

    # ----------------------
    # 2) SSE token popping
    # ----------------------
    async def pop_sse_token(self, request_id: str) -> Optional[str]:
        """
        The SSE route calls this repeatedly to get partial tokens.
        If no token is available, we block up to 120s, else return None.
        """
        if request_id not in self.active_sse_queues:
            print(
                f"[STREAM] pop_sse_token => no SSE queue found for request_id={request_id}"
            )
            return None

        queue = self.active_sse_queues[request_id]
        try:
            token = await asyncio.wait_for(queue.get(), timeout=120.0)
            # print(f"[STREAM] pop_sse_token => got token from queue: {token}")
            return token
        except asyncio.TimeoutError:
            print(
                f"[STREAM] pop_sse_token => timed out waiting for token, request_id={request_id}"
            )
            return None

    # ----------------------
    # 3) SSE queue cleanup
    # ----------------------
    async def close_sse_queue(self, request_id: str):
        """
        Called by the SSE route after finishing.
        Remove the queue from memory.
        """
        if request_id in self.active_sse_queues:
            print(
                f"[STREAM] close_sse_queue => removing SSE queue for request_id={request_id}"
            )
            del self.active_sse_queues[request_id]
        else:
            print(f"[STREAM] close_sse_queue => no SSE queue found for {request_id}")
    
    # ----------------------
    # /history | 대화 기록 가져오기
    # ----------------------
    async def get_conversation_history(self, request_id: str) -> dict:
        """
        Returns the conversation history for the given request_id.
        The messages are serialized into a JSON-friendly format.
        """
        try:
            if request_id in self.memory_map:
                memory = self.memory_map[request_id]
                history_obj = memory.load_memory_variables({})
                if "history" in history_obj and isinstance(history_obj["history"], list):
                    # 직렬화
                    serialized = [serialize_message(msg) for msg in history_obj["history"]]
                    print("[HISTORY] 대화 기록 반환(직렬화) : ", serialized)
                    return {"history": serialized}
                else:
                    print("[HISTORY] 대화 기록 반환(직렬화X) : ", history_obj)
                    return {"history": []}
            else:
                return {"history": []}
        except Exception as e:
            print(f"[ERROR get_conversation_history] {e}")
            return {"history": []}
        
    # ----------------------
    # /reference | 해당 답변의 출처 가져오기
    # ----------------------
    async def get_reference_data(self, chunk_ids: list):
        try:
            result = []
            data = self.data
            for cid in chunk_ids:
                if cid in data["chunk_ids"]:
                    idx = data["chunk_ids"].index(cid)
                    record = {
                        "file_name": data["file_names"][idx],
                        "title": data["titles"][idx],
                        "text": data["texts_vis"][idx],
                        "date": str(data["times"][idx])
                    }
                    result.append(record)
            return result
        except Exception as e:
            print(f"[ERROR get_reference_data] {e}")
            return []

# Ray Serve를 통한 배포
@serve.deployment(name="inference", max_ongoing_requests=100)
class InferenceService:
    def __init__(self, config):
        self.config = config
        self.actor = InferenceActor.options(
            num_gpus=config.ray.num_gpus, 
            num_cpus=config.ray.num_cpus
        ).remote(config)

    # Text
    async def query(self, http_query: dict):
        result = await self.actor.process_query.remote(http_query)
        return result
    # Text Stream
    async def process_query_stream(self, http_query: dict) -> str:
        req_id = await self.actor.process_query_stream.remote(http_query)
        return req_id
    
    async def pop_sse_token(self, req_id: str) -> str:
        token = await self.actor.pop_sse_token.remote(req_id)
        return token

    async def close_sse_queue(self, req_id: str) -> str:
        await self.actor.close_sse_queue.remote(req_id)
        return "closed"
    
    # /history
    async def get_history(self, request_id: str, last_index: int = None):
        result = await self.actor.get_conversation_history.remote(request_id)
        if last_index is not None and isinstance(result.get("history"), list):
            result["history"] = result["history"][last_index+1:]
        return result

    # /reference
    async def get_reference_data(self, chunk_ids: list):
        result = await self.actor.get_reference_data.remote(chunk_ids)
        return result

```


--- ray_deploy/langchain.py

```python

# ray_deploy/langchain.py

# 랭체인 도입
from langchain.memory import ConversationBufferMemory
from langchain.schema import HumanMessage, AIMessage

# =============================================================================
# Custom Conversation Memory to store extra metadata (e.g., chunk_ids)
# =============================================================================
class CustomConversationBufferMemory(ConversationBufferMemory):
    """대화 저장 시 추가 메타데이터를 함께 기록"""
    def save_context(self, inputs: dict, outputs: dict) -> None:
        """
        inputs, outputs 예시:
            inputs = {
                "qry_contents": "사용자 질문",
                "qry_id": "...",
                "user_id": "...",
                "auth_class": "...",
                "qry_time": "..."
            }
            outputs = {
                "output": "AI의 최종 답변",
                "chunk_ids": [...참조 chunk_id 리스트...]
            }
        """
        try:
            user_content = inputs.get("qry_contents", "")
            human_msg = HumanMessage(
                content=user_content,
                additional_kwargs={
                    "qry_id": inputs.get("qry_id"),
                    "user_id": inputs.get("user_id"),
                    "auth_class": inputs.get("auth_class"),
                    "qry_time": inputs.get("qry_time")
                }
            )
            ai_content = outputs.get("output", "")
            ai_msg = AIMessage(
                content=ai_content,
                additional_kwargs={
                    "chunk_ids": outputs.get("chunk_ids", []),
                    "qry_id": inputs.get("qry_id"),
                    "user_id": inputs.get("user_id"),
                    "auth_class": inputs.get("auth_class"),
                    "qry_time": inputs.get("qry_time")
                }
            )

            self.chat_memory.messages.append(human_msg)
            self.chat_memory.messages.append(ai_msg)
        except Exception as e:
            print(f"[ERROR in save_context] {e}")
        
    def load_memory_variables(self, inputs: dict) -> dict:
        """
        랭체인 규약에 의해 {"history": [메시지 리스트]} 형태 리턴
        """
        try:
            return {"history": self.chat_memory.messages}
        except Exception as e:
            print(f"[ERROR in load_memory_variables] {e}")
            return {"history": []}

# Serialization function for messages
def serialize_message(msg):
    """
    HumanMessage -> {"role": "human", "content": ...}
    AIMessage    -> {"role": "ai", "content": ..., "references": [...]}
    """
    try:
        if isinstance(msg, HumanMessage):
            return {"role": "human", "content": msg.content}
        elif isinstance(msg, AIMessage):
            refs = msg.additional_kwargs.get("chunk_ids", [])
            # 디버그 출력
            print(f"[DEBUG serialize_message] AI refs: {refs}")
            return {"role": "ai", "content": msg.content, "references": refs}
        else:
            return {
                "role": "unknown",
                "content": getattr(msg, "content", str(msg))
            }
    except Exception as e:
        print(f"[ERROR in serialize_message] {e}")
        return {"role": "error", "content": str(e)}
    
# =============================================================================
# =============================================================================

```


--- utils/utils_format.py

```python

# utils/utils_format.py
import json
from datetime import datetime, timedelta

import requests

# Define the minimum valid file size (e.g., 10MB)
MIN_WEIGHT_SIZE = 10 * 1024 * 1024

# For tracking execution time of functions
from utils.tracking import time_tracker

# -------------------------------------------------
# Function: process_to_format
# -------------------------------------------------
@time_tracker
def process_to_format(qry_contents, type):
    # 여기서 RAG 시스템을 호출하거나 답변을 생성하도록 구현하세요.
    # 예제 응답 형식
    ### rsp_type : RA(Retrieval All), RT(Retrieval Text), RB(Retrieval taBle), AT(Answer Text), AB(Answer taBle) ###
    print("[SOOWAN] process_to_format 진입")
    if type == "Retrieval":
        print("[SOOWAN] 타입 : 리트리버")
        tmp_format = {"rsp_type": "R", "rsp_tit": "남성 내부 데이터", "rsp_data": []}
        for i, form in enumerate(qry_contents):
            tmp_format_ = {
                "rsp_tit": f"{i+1}번째 검색데이터: {form['title']} (출처:{form['file_name']})",
                "rsp_data": form["contents"],
                "chunk_id": form.get("chunk_id"),
            }
            tmp_format["rsp_data"].append(tmp_format_)
        return tmp_format

    elif type == "SQL":
        print("[SOOWAN] 타입 : SQL")
        tmp_format = {
            "rsp_type": "R",
            "rsp_tit": "남성 내부 데이터",
            "rsp_data": [{"rsp_tit": "SQL Query 결과표", "rsp_data": []}],
        }
        tmp_format_sql = {
            "rsp_type": "TB",
            "rsp_tit": qry_contents[0]["title"],
            "rsp_data": qry_contents[0]["data"],
        }
        tmp_format_chart = {
            "rsp_type": "CT",
            "rsp_tit": qry_contents[1]["title"],
            "rsp_data": {"chart_tp": "BAR", "chart_data": qry_contents[1]["data"]},
        }
        tmp_format["rsp_data"][0]["rsp_data"].append(tmp_format_sql)
        # tmp_format['rsp_data'].append(tmp_format_chart)
        return tmp_format, tmp_format_chart

    elif type == "Answer":
        print("[SOOWAN] 타입 : 대답")
        tmp_format = {"rsp_type": "A", "rsp_tit": "답변", "rsp_data": []}
        # for i, form in enumerate(qry_contents):
            # if i == 0:
        tmp_format_ = {"rsp_type": "TT", "rsp_data": qry_contents}
        tmp_format["rsp_data"].append(tmp_format_)
            # elif i == 1:
            #     tmp_format["rsp_data"].append(form)
            # else:
            #     None

        return tmp_format

    else:
        print("Error! Type Not supported!")
        return None

# @time_tracker
# def process_format_to_response(formats, qry_id, continue_="C", update_index=1):
#     # Get multiple formats to tuple

#     ans_format = {
#         "status_code": 200,
#         "result": "OK",
#         "detail": "",
#         "continue":continue_,
#         "qry_id": qry_id,
#         "rsp_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
#         "data_list": [],
#     }

#     # 누적된 토큰을 하나의 문자열로 결합합니다.
#     aggregated_answer = "".join(token.get("answer", "") for token in formats)
#     ans_format["data_list"].append({
#         "rsp_type": "A",
#         "rsp_tit": f"답변{update_index}",
#         "rsp_data": [
#             {
#                 "rsp_type": "TT",
#                 "rsp_data": aggregated_answer
#             }
#         ]
#     })
    
#     # Validate JSON before returning
#     try:
#         json.dumps(ans_format, ensure_ascii=False)  # Test JSON validity
#     except Exception as e:
#         print(f"[ERROR] Invalid JSON structure: {str(e)}")
#         ans_format["status_code"] = 500
#         ans_format["result"] = "ERROR"
#         ans_format["detail"] = f"JSON Error: {str(e)}"

#     # for format in formats:
#     #     ans_format["data_list"].append(format)

#     # return json.dumps(ans_format, ensure_ascii=False)
#     return ans_format

@time_tracker
def process_format_to_response(formats, qry_id, continue_="C", update_index=1):
    # If there are any reference tokens, return only them.
    reference_tokens = [token for token in formats if token.get("type") == "reference"]
    if reference_tokens:
        # For this example, we'll use the first reference token.
        ref = reference_tokens[0]
        # Add the extra keys.
        ref["qry_id"] = qry_id
        ref["continue"] = continue_
        ref["rsp_time"] = datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f")
        # Ensure that a "rsp_tit" key exists to satisfy downstream requirements.
        if "rsp_tit" not in ref:
            ref["rsp_tit"] = "Reference"
        return ref

    # Otherwise, aggregate the normal answer tokens.
    normal_tokens = [token.get("answer", "") for token in formats if token.get("type") != "reference"]
    aggregated_answer = "".join(normal_tokens)
    
    ans_format = {
        "status_code": 200,
        "result": "OK",
        "detail": "",
        "continue": continue_,
        "qry_id": qry_id,
        "rsp_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
        "data_list": [{
            "rsp_type": "A",
            "rsp_tit": f"답변{update_index}",
            "rsp_data": [{
                "rsp_type": "TT",
                "rsp_data": aggregated_answer
            }]
        }]
    }
    
    # Validate JSON structure before returning.
    try:
        json.dumps(ans_format, ensure_ascii=False)
    except Exception as e:
        print(f"[ERROR] Invalid JSON structure: {str(e)}")
        ans_format["status_code"] = 500
        ans_format["result"] = "ERROR"
        ans_format["detail"] = f"JSON Error: {str(e)}"
    
    return ans_format



# @time_tracker
# def process_format_to_response(formats, qry_id, continue_="C", update_index=1):
#     # 누적된 토큰들을 하나의 문자열로 결합합니다.
#     aggregated_answer = "".join(token.get("answer", "") for token in formats)
    
#     # retrieval과 동일한 구조를 위해, 답변 데이터는 내부 data_list가 딕셔너리 형태로 구성됩니다.
#     answer = {
#         "rsp_type": "A",                # Answer
#         "rsp_tit": f"답변{update_index}",
#         "rsp_data": [                    # 바로 텍스트 응답 리스트를 구성
#             {
#                 "rsp_tit": f"답변{update_index}",
#                 "rsp_data": [
#                     {
#                         'rsp_type': 'TT',
#                         'rsp_tit': '',
#                         'rsp_data': aggregated_answer,
#                     }
#                 ]
                
#             }
#         ]
#     }
    
#     # 최종 응답 구조: 최상위에 data_list는 리스트이고, 내부에 딕셔너리로 답변 데이터를 포함합니다.
#     ans_format = {
#         "status_code": 200,
#         "result": "OK",
#         "detail": "Answer",
#         "continue": continue_,
#         "qry_id": qry_id,
#         "rsp_time": datetime.now().isoformat(),
#         "data_list": [
#             {
#                 "type": "answer",               # 응답 타입 answer
#                 "status_code": 200,
#                 "result": "OK",
#                 "detail": "Answer",
#                 "evt_time": datetime.now().isoformat(),
#                 "data_list": answer              # retrieval의 data_list와 동일하게 딕셔너리 형태
#             }
#         ]
#     }
#     return ans_format

@time_tracker
def error_format(message, status, qry_id=""):
    ans_format = {
        "status_code": status,
        "result": message,
        "qry_id": qry_id,  # 추가: qry_id 포함
        "detail": "",
        "evt_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
    }
    return json.dumps(ans_format)

# @time_tracker
# def send_data_to_server(data, url):
#     headers = {
#         "Content-Type": "application/json; charset=utf-8"
#     }
#     try:
#         # 다른 서버로 데이터를 전송 (POST 요청)
#         response = requests.post(url, json=data, headers=headers)
#         if response.status_code == 200:
#             print(f"Data sent successfully: {data}")
#         else:
#             print(f"Failed to send data: {response.status_code}")
#             print(f"Failed data: {data}")
#     except requests.exceptions.RequestException as e:
#         print(f"Error sending data: {e}")
@time_tracker     
def send_data_to_server(data, url):
    try:
        if not data or "data_list" not in data:
            print("[ERROR] Empty or Invalid data structure")
            return
        # Log reference data if present
        for item in data["data_list"]:
            if item.get("rsp_type") == "A" and "references" in str(item):
                print(f"[DEBUG] Sending reference data: {json.dumps(data, ensure_ascii=False, indent=2)}")
        response = requests.post(url, json=data, timeout=10)
        
        if response.status_code != 200:
            print(f"[ERROR] Failed to send data: {response.status_code}, {response.text}")
        
        return response

    except Exception as e:
        print(f"[ERROR] send_data_to_server encountered an error: {str(e)}")

```


--- utils/utils_load.py

```python

# utils/utils_load.py
import json
import numpy as np
import torch
import random
import shutil
from datetime import datetime, timedelta
from transformers import (
    AutoModel,
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    AutoConfig,
)

import os

# 전역 캐시 변수 - 데이터의 변화를 감지하기 위한
_cached_data = None
_cached_data_mtime = 0

# Import vLLM utilities
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

# Define the minimum valid file size (e.g., 10MB)
MIN_WEIGHT_SIZE = 10 * 1024 * 1024

# For tracking execution time of functions
from utils.tracking import time_tracker

# Logging
import logging
# -------------------------------------------------
# Function: find_weight_directory - 허깅페이스 권한 문제 해결 후에 잘 사용되지 아니함
# -------------------------------------------------
# Recursively searches for weight files (safetensors or pytorch_model.bin) in a given base path.
# This method Find the files searching the whole directory
# Because, vLLM not automatically find out the model files.
# -------------------------------------------------
@time_tracker
def find_weight_directory(base_path):
    # ---- Recursively searches for weight files in a given base path ----
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if ".safetensors" in file or "pytorch_model.bin" in file:
                file_path = os.path.join(root, file)
                try:
                    if os.path.getsize(file_path) >= MIN_WEIGHT_SIZE:
                        return root, "safetensors" if ".safetensors" in file else "pt"
                    else:
                        logging.debug(
                            f"파일 {file_path}의 크기가 너무 작음: {os.path.getsize(file_path)} bytes"
                        )
                except Exception as ex:
                    logging.debug(f"파일 크기 확인 실패: {file_path} - {ex}")
    return None, None

# -------------------------------------------------
# Function: load_model
# -------------------------------------------------
@time_tracker
def load_model(config):
    # Loads the embedding model and the main LLM model (using vLLM if specified in the config).
    
    # Get the HF token from the environment variable.
    logging.info("Starting model loading...")
    token = os.getenv("HF_TOKEN_PATH")
    # Check if token is likely a file path.
    if token is not None and not token.startswith("hf_"):
        if os.path.exists(token) and os.path.isfile(token):
            try:
                with open(token, "r") as f:
                    token = f.read().strip()
            except Exception as e:
                print("DEBUG: Exception while reading token file:", e)
                logging.warning("Failed to read token from file: %s", e)
                token = None
        else:
            logging.warning("The HF_TOKEN path does not exist: %s", token)
            token = None
    else:
        print("DEBUG: HF_TOKEN appears to be a token string; using it directly:")

    if token is None or token == "":
        logging.warning("HF_TOKEN is not set. Access to gated models may fail.")
        token = None

    # -------------------------------
    # Load the embedding model and tokenizer.
    # -------------------------------
    print("Loading embedding model")
    try:
        embed_model = AutoModel.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,  # using 'token' parameter
        )
    except Exception as e:
        raise e
    try:
        embed_tokenizer = AutoTokenizer.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,
        )
    except Exception as e:
        raise e
    print(":Embedding tokenizer loaded successfully.")
    embed_model.eval()
    embed_tokenizer.model_max_length = 4096

    # -------------------------------
    # Load the main LLM model via vLLM.
    # -------------------------------
    if config.use_vllm:
        print("vLLM mode enabled. Starting to load main LLM model via vLLM.")
        if config.model.quantization_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
            print("Using 4-bit quantization.")
        elif config.model.quantization_8bit:
            bnb_config = BitsAndBytesConfig(load_in_8bit=True)
            print("Using 8-bit quantization.")
        else:
            bnb_config = None
            print("Using pure option of Model(No quantization)")

        local_model_path = os.path.join(
            config.cache_dir, "models--" + config.model_id.replace("/", "--")
        )
        local_model_path = os.path.abspath(local_model_path)

        config_file = os.path.join(local_model_path, "config.json")
        need_patch = False

        if not os.path.exists(config_file):
            os.makedirs(local_model_path, exist_ok=True)
            try:
                hf_config = AutoConfig.from_pretrained(
                    config.model_id,
                    cache_dir=config.cache_dir,
                    trust_remote_code=True,
                    token=token,
                )
            except Exception as e:
                raise e
            # 패치: vocab_size 속성이 없으면 embed_tokenizer의 값을 사용하여 추가
            if not hasattr(hf_config, "vocab_size"):
                print("[MODEL-LOADING] 'vocab_size' 속성이 없어서 기본값으로 추가합니다.")
                hf_config.vocab_size = getattr(embed_tokenizer, "vocab_size", 32000)
            config_dict = hf_config.to_dict()
            # 패치: architectures 속성이 없으면 Gemma2로 기본 설정
            if not config_dict.get("architectures"):
                print("[MODEL-LOADING] Config file의 architectures 정보 없음, Default Gemma2 아키텍처 설정")
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
            # 확인
            print(f"[DEBUG] => Saving HF Config with architectures={config_dict['architectures']}")
                
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config_dict, f)
        else:
            # 이미 config_file이 존재하는 경우
            with open(config_file, "r", encoding="utf-8") as f:
                config_dict = json.load(f)
            
            # 패치: vocab_size 속성이 없으면 embed_tokenizer의 값을 사용하여 추가
            if "vocab_size" not in config_dict:
                # embed_tokenizer의 vocab_size가 존재하면 사용하고, 없으면 기본값 30522로 설정
                config_dict["vocab_size"] = getattr(embed_tokenizer, "vocab_size", 30522)
                print("[MODEL-LOADING] 'vocab_size' 속성이 없어서 기본값으로 추가합니다:", config_dict["vocab_size"])
            # 패치: architectures 속성이 없으면 Gemma2로 기본 설정
            if not config_dict.get("architectures"):
                print("[MODEL-LOADING] Config file의 architectures 정보 없음, Default Gemma2 아키텍처 설정")
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
            # 확인
            print(f"[DEBUG] => Saving HF Config with architectures={config_dict['architectures']}")

            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config_dict, f)

        weight_dir, weight_format = find_weight_directory(local_model_path)
        if weight_dir is None:
            print("DEBUG: No model weights found. Attempting to download model snapshot.")
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    print(f"DEBUG: Snapshot download attempt {attempt+1}...")
                    # Attempt to download the model snapshot using the Hugging Face hub function.
                    from huggingface_hub import snapshot_download
                    snapshot_download(config.model_id, cache_dir=config.cache_dir, token=token)
                    break  # If download succeeds, break out of the loop.
                except Exception as e:
                    print(f"DEBUG: Snapshot download attempt {attempt+1} failed:", e)
                    if attempt < max_retries - 1:
                        print("DEBUG: Retrying snapshot download...")
                    else:
                        raise RuntimeError(f"Snapshot download failed after {max_retries} attempts: {e}")
            # After download, try to find the weights again.
            weight_dir, weight_format = find_weight_directory(local_model_path)
            if weight_dir is None:
                raise RuntimeError(f"Unable to find model weights even after snapshot download in {local_model_path}.")

        snapshot_config = os.path.join(weight_dir, "config.json")
        if not os.path.exists(snapshot_config):
            shutil.copy(config_file, snapshot_config)
        engine_args = AsyncEngineArgs(
            model=weight_dir,
            tokenizer=config.model_id,
            download_dir=config.cache_dir,
            trust_remote_code=True,
            config_format="hf",
            load_format=weight_format,
        )
        
        vllm_conf = config.get("vllm", {})
        
        # --- 기존 엔진 설정들
        engine_args.enable_prefix_caching = True
        # engine_args.scheduler_delay_factor = vllm_conf.get("scheduler_delay_factor", 0.1)
        engine_args.enable_chunked_prefill = True
        engine_args.tensor_parallel_size = vllm_conf.get("tensor_parallel_size", 1) # How many use the parllel Multi-GPU
        engine_args.max_num_seqs = vllm_conf.get("max_num_seqs")
        engine_args.max_num_batched_tokens = vllm_conf.get("max_num_batched_tokens", 8192)
        # engine_args.block_size = vllm_conf.get("block_size", 128)
        engine_args.gpu_memory_utilization = vllm_conf.get("gpu_memory_utilization")
        
        # --- 다중 GPU 사용 관련 설정 ---
        if vllm_conf.get("disable_custom_all_reduce", False):
            engine_args.disable_custom_all_reduce = True # For Fixing the Multi GPU problem
        
        # --- 모델의 Context Length ---
        engine_args.max_model_len = vllm_conf.get("max_model_len")
        
        # --- 멀티모달 (Image) 관련 설정 ---
        engine_args.mm_processor_kwargs = vllm_conf.get("mm_processor_kwargs", {"do_pan_and_scan": True})
        engine_args.disable_mm_preprocessor_cache = vllm_conf.get("disable_mm_preprocessor_cache", False)
        engine_args.limit_mm_per_prompt = vllm_conf.get("limit_mm_per_prompt", {"image": 2})
        
        print("EngineArgs setting be finished")
        
        try:
            # --- v1 구동 해결책: 현재 스레드가 메인 스레드가 아니면 signal 함수를 임시 패치 ---
            import threading, signal
            if threading.current_thread() is not threading.main_thread():
                original_signal = signal.signal
                signal.signal = lambda s, h: None  # signal 설정 무시
                print("비메인 스레드에서 signal.signal을 monkey-patch 하였습니다.")
            # --- v1 구동 해결책: ------------------------------------------------------ ---
            engine = AsyncLLMEngine.from_engine_args(engine_args) # Original
            # v1 구동 해결책: 엔진 생성 후 원래 signal.signal으로 복원 (필요 시) ----------------- ---
            if threading.current_thread() is not threading.main_thread():
                signal.signal = original_signal
            # --- v1 구동 해결책: ------------------------------------------------------ ---
            print("DEBUG: vLLM engine successfully created.") # Original
            
        except Exception as e:
            print("DEBUG: Exception during engine creation:", e)
            if "HeaderTooSmall" in str(e):
                print("DEBUG: Falling back to PyTorch weights.")
                fallback_dir = None
                for root, dirs, files in os.walk(local_model_path):
                    for file in files:
                        if (
                            "pytorch_model.bin" in file
                            and os.path.getsize(os.path.join(root, file))
                            >= MIN_WEIGHT_SIZE
                        ):
                            fallback_dir = root
                            break
                    if fallback_dir:
                        break
                if fallback_dir is None:
                    logging.error(
                        "DEBUG: No PyTorch weight file found in", local_model_path
                    )
                    raise e
                engine_args.load_format = "pt"
                engine_args.model = fallback_dir
                print("DEBUG: New EngineArgs for fallback:", engine_args)
                engine = AsyncLLMEngine.from_engine_args(engine_args)
                print("DEBUG: vLLM engine created with PyTorch fallback.")
            else:
                logging.error("DEBUG: Engine creation failed:", e)
                raise e

        engine.is_vllm = True

        print("DEBUG: Loading main LLM tokenizer with token authentication.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
                local_files_only=True  # Force loading from local cache to avoid hub requests
            )
        except Exception as e:
            print("DEBUG: Exception loading main tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        return engine, tokenizer, embed_model, embed_tokenizer

    else:
        print("DEBUG: vLLM is not used. Loading model via standard HF method.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        try:
            model = AutoModelForCausalLM.from_pretrained(
                config.model_id,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                cache_dir=config.cache_dir,
                # quantization_config=bnb_config,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading model:", e)
            raise e
        model.eval()
        return model, tokenizer, embed_model, embed_tokenizer

# -------------------------------------------------
# Function: load_data
# -------------------------------------------------
@time_tracker
def load_data(data_path):
    global _cached_data, _cached_data_mtime
    try:
        current_mtime = os.path.getmtime(data_path)
    except Exception as e:
        print("파일 수정 시간 확인 실패:", e)
        return None

    # 캐시가 비어있거나 파일 수정 시간이 변경된 경우 데이터 재로드
    if _cached_data is None or current_mtime != _cached_data_mtime:
        with open(data_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # --- 디버그 함수: 벡터 포맷 검사 ---
        debug_vector_format(data)

        # 데이터 전처리 (예: 리스트 변환 및 numpy, torch 변환)
        file_names = []
        chunk_ids = []  # >>> CHANGED: Added to record each chunk's ID
        titles = []
        times = []
        vectors = []
        texts = []
        texts_short = []
        texts_vis = []
        missing_time = 0

        for file_obj in data:
            for chunk in file_obj["chunks"]:
                file_names.append(file_obj["file_name"])
                chunk_ids.append(chunk.get("chunk_id", 0))  # >>> CHANGED: Record chunk_id
                try:
                    arr = np.array(chunk["vector"])
                    vectors.append(arr)
                except Exception as e:
                    logging.warning(f"[load_data] 벡터 변환 오류: {e} → 빈 벡터로 대체")
                    vectors.append(np.zeros((1, 768), dtype=np.float32))  # 임의로 1x768 형식
                
                titles.append(chunk["title"])
                
                # 날짜 파싱
                if chunk["date"]:
                    try:
                        times.append(datetime.strptime(chunk["date"], "%Y-%m-%d"))
                    except ValueError:
                        logging.warning(f"잘못된 날짜 형식: {chunk['date']} → 기본 날짜로 대체")
                        times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))
                        missing_time += 1
                else:
                    missing_time += 1
                    times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))

                texts.append(chunk["text"])
                texts_short.append(chunk["text_short"])
                texts_vis.append(chunk["text_vis"])

        # 실제 텐서로 변환
        try:
            vectors = np.array(vectors)
            vectors = torch.from_numpy(vectors).to(torch.float32)
        except Exception as e:
            logging.error(f"[load_data] 최종 벡터 텐서 변환 오류: {str(e)}")
            # 필요 시 추가 처리

        _cached_data = {
            "file_names": file_names,
            "chunk_ids": chunk_ids,  # >>> CHANGED: Saved chunk IDs here
            "titles": titles,
            "times": times,
            "vectors": vectors,
            "texts": texts,
            "texts_short": texts_short,
            "texts_vis": texts_vis,
        }
        _cached_data_mtime = current_mtime
        print(f"Data loaded! Length: {len(titles)}, Missing times: {missing_time}")
    else:
        print("Using cached data")

    return _cached_data

# -------------------------------------------------
# Function: debug_vector_format
# -------------------------------------------------
def debug_vector_format(data):
    """
    data(List[Dict]): load_data에서 JSON으로 로드된 객체.
    각 file_obj에 대해 chunks 리스트를 순회하며 vector 형식을 디버깅 출력.
    """
    print("\n[DEBUG] ===== 벡터 형식 검사 시작 =====")
    for f_i, file_obj in enumerate(data):
        file_name = file_obj.get("file_name", f"Unknown_{f_i}")
        chunks = file_obj.get("chunks", [])
        for c_i, chunk in enumerate(chunks):
            vector_data = chunk.get("vector", None)
            if vector_data is None:
                # print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector 없음(None)")
                continue
            # 자료형, 길이, shape 등 확인
            vector_type = type(vector_data)
            # shape을 안전하게 얻기 위해 np.array 변환 시도
            try:
                arr = np.array(vector_data)
                shape = arr.shape
                # print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector_type={vector_type}, shape={shape}")
            except Exception as e:
                print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector 변환 실패: {str(e)}")
    print("[DEBUG] ===== 벡터 형식 검사 종료 =====\n")

# -------------------------------------------------
# Function: random_seed
# -------------------------------------------------
@time_tracker
def random_seed(seed):
    # Set random seed for Python's built-in random module
    random.seed(seed)

    # Set random seed for NumPy
    np.random.seed(seed)

    # Set random seed for PyTorch
    torch.manual_seed(seed)

    # Ensure the same behavior on different devices (CPU vs GPU)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # If using multi-GPU.

    # Enable deterministic algorithms
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False

```


--- core/RAG.py

```python

# core/RAG.py
import torch
import re
import numpy as np
import rank_bm25
import random
import uuid
import logging
from datetime import datetime, timedelta
# from sql import generate_sql  # (구) 제거된 import

# Tracking
from utils.tracking import time_tracker

# Import the vLLM to use the AsyncLLMEngine
from vllm.engine.async_llm_engine import AsyncLLMEngine
# 이미지 임베딩을 별도로 계산하여 프롬프트에 포함하기
from vllm.model_executor.models.interfaces import SupportsMultiModal

# Prompt 템플릿 불러오기
from prompt import QUERY_SORT_PROMPT, GENERATE_PROMPT_TEMPLATE, STREAM_PROMPT_TEMPLATE, SQL_EXTRACTION_PROMPT_TEMPLATE, IMAGE_DESCRIPTION_PROMPT
# SQL만 담당하는 함수들만 import
from core.SQL_NS import run_sql_unno, run_sql_bl, get_metadata  

global beep
beep = "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------"

@time_tracker
async def execute_rag(QU, KE, TA, TI, **kwargs):
    print("[SOOWAN]: execute_rag : 진입")
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    embed_model = kwargs.get("embed_model")
    embed_tokenizer = kwargs.get("embed_tokenizer")
    data = kwargs.get("data")
    config = kwargs.get("config")
    
    if TA == "yes":  # Table 이 필요하면
        print("[SOOWAN]: execute_rag : 테이블 필요 (TA == yes). SQL 생성 시작합니다.")
        try:
            # generate_sql 함수를 이 파일(RAG.py) 내부에 새로 정의했으므로, 여기서 직접 호출
            result = await generate_sql(QU, model, tokenizer, config)
        except Exception as e:
            # 1) generate_sql() 자체가 도중에 예외를 던지는 경우
            print("[ERROR] generate_sql() 도중 예외 발생:", e)
            # 멈추지 않고, 에러 형식으로 데이터를 만들어 반환
            docs = (
                "테이블 조회 시도 중 예외가 발생했습니다. "
                "해당 SQL을 실행할 수 없어서 테이블 데이터를 가져오지 못했습니다."
            )
            docs_list = []
            return docs, docs_list

        # 2) 함수가 정상 실행됐지만 결과가 None인 경우(= SQL 쿼리 결과가 없거나 오류)
        if result is None:
            print(
                "[WARNING] generate_sql()에서 None을 반환했습니다. "
                "SQL 수행 결과가 없거나 에러가 발생한 것일 수 있습니다."
            )
            docs = (
                "테이블 조회 결과가 비어 있습니다. "
                "조회할 데이터가 없거나 SQL 오류가 발생했습니다."
            )
            docs_list = []
            return docs, docs_list

        # 기존 generate_sql은 이제 6개의 값을 반환합니다.
        final_sql_query, title, explain, table_json, chart_json, detailed_result = result

        # docs : LLM 입력용 (string)
        PROMPT = (
            f"실제 사용된 SQL문: {final_sql_query}\n\n"
            f"추가 설명: {explain}\n\n"
            f"실제 SQL 추출된 데이터: {str(table_json)}\n\n"
            f"실제 선적된 B/L 데이터: {str(detailed_result)}\n\n"
        )
        # docs_list에 DG B/L 상세 정보를 추가하여 총 3개 항목으로 구성합니다.
        docs_list = [
            {"title": title, "data": table_json},
            {"title": "DG B/L 상세 정보", "data": detailed_result},
        ]
        print("[SOOWAN]: execute_rag : 테이블 부분 정상 처리 완료")

        return PROMPT, docs_list

    else:
        print("[SOOWAN]: execute_rag : 테이블 필요없음")
        # 적응형 시간 필터링으로 RAG 실행
        filtered_data = expand_time_range_if_needed(TI, data, min_docs=50)

        # 디버깅을 위해 문서 수 로깅
        print(f"[RETRIEVE] 검색에 사용되는 문서 수: {len(filtered_data.get('vectors', []))}")

        docs, docs_list = retrieve(KE, filtered_data, config.N, embed_model, embed_tokenizer)
        return docs, docs_list

@time_tracker
async def generate_answer(query, docs, **kwargs):
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    config = kwargs.get("config")

    answer = await generate(docs, query, model, tokenizer, config)
    return answer


@time_tracker
async def query_sort(params):
    max_attempts = 3
    attempt = 0
    while attempt < max_attempts:
        # params: 딕셔너리로 전달된 값들
        query = params["user_input"]
        model = params["model"]
        tokenizer = params["tokenizer"]
        embed_model = params["embed_model"]
        embed_tokenizer = params["embed_tokenizer"]
        data = params["data"]
        config = params["config"]

        # 프롬프트 생성
        PROMPT = QUERY_SORT_PROMPT.format(user_query=query)
        print("##### query_sort is starting, attempt:", attempt + 1, "#####")

        # Get Answer from LLM
        if config.use_vllm:  # use_vllm = True case
            from vllm import SamplingParams

            sampling_params = SamplingParams(
                max_tokens=config.model.max_new_tokens,
                temperature=config.model.temperature,
                top_k=config.model.top_k,
                top_p=config.model.top_p,
                repetition_penalty=config.model.repetition_penalty,
            )
            accepted_request_id = str(uuid.uuid4())
            answer = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
        else:
            input_ids = tokenizer(
                PROMPT, return_tensors="pt", truncation=True, max_length=4024
            ).to("cuda")
            token_count = input_ids["input_ids"].shape[1]
            outputs = model.generate(
                **input_ids,
                max_new_tokens=config.model.max_new_tokens,
                do_sample=config.model.do_sample,
                temperature=config.model.temperature,
                top_k=config.model.top_k,
                top_p=config.model.top_p,
                repetition_penalty=config.model.repetition_penalty,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id,
            )
            answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)

        print("[DEBUG query_sort] Generated answer:")
        print(answer)

        # Regular expressions for tags
        query_pattern = r"<query.*?>(.*?)<query.*?>"
        keyword_pattern = r"<keyword.*?>(.*?)<keyword.*?>"
        table_pattern = r"<table.*?>(.*?)<table.*?>"
        time_pattern = r"<time.*?>(.*?)<time.*?>"

        m_query = re.search(query_pattern, answer, re.DOTALL)
        m_keyword = re.search(keyword_pattern, answer, re.DOTALL)
        m_table = re.search(table_pattern, answer, re.DOTALL)
        m_time = re.search(time_pattern, answer, re.DOTALL)

        if m_query and m_keyword and m_table and m_time:
            QU = m_query.group(1).strip()
            KE = m_keyword.group(1).strip()
            TA = m_table.group(1).strip()
            TI = m_time.group(1).strip()
            if TI == "all":
                TI = "1900-01-01:2099-01-01"
            print(beep)
            print(f"구체화 질문: {QU}, 키워드 : {KE}, 테이블 필요 유무: {TA}, 시간: {TI}")
            print(beep)
            return QU, KE, TA, TI
        else:
            print("[ERROR query_sort] 필요한 태그들이 누락되었습니다. 재시도합니다.")
            attempt += 1

    # 3회 재시도 후에도 실패하면 에러 발생
    raise ValueError("LLM이 올바른 태그 형식의 답변을 생성하지 못했습니다.")


@time_tracker
async def specific_question(params):
    """
    query_sort와 동일한 로직을 수행할 수도 있으나,
    별도로 분리된 이유가 있다면 여기서 추가 처리 가능
    """
    max_attempts = 3
    attempt = 0
    while attempt < max_attempts:
        query = params["user_input"]
        model = params["model"]
        tokenizer = params["tokenizer"]
        embed_model = params["embed_model"]
        embed_tokenizer = params["embed_tokenizer"]
        data = params["data"]
        config = params["config"]

        PROMPT = QUERY_SORT_PROMPT.format(user_query=query)
        print("##### query_sort is starting, attempt:", attempt + 1, "#####")

        if config.use_vllm:
            from vllm import SamplingParams

            sampling_params = SamplingParams(
                max_tokens=config.model.max_new_tokens,
                temperature=config.model.temperature,
                top_k=config.model.top_k,
                top_p=config.model.top_p,
                repetition_penalty=config.model.repetition_penalty,
            )
            accepted_request_id = str(uuid.uuid4())
            answer = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
        else:
            input_ids = tokenizer(
                PROMPT, return_tensors="pt", truncation=True, max_length=4024
            ).to("cuda")
            token_count = input_ids["input_ids"].shape[1]
            outputs = model.generate(
                **input_ids,
                max_new_tokens=config.model.max_new_tokens,
                do_sample=config.model.do_sample,
                temperature=config.model.temperature,
                top_k=config.model.top_k,
                top_p=config.model.top_p,
                repetition_penalty=config.model.repetition_penalty,
                eos_token_id=tokenizer.eos_token_id,
                pad_token_id=tokenizer.eos_token_id,
            )
            answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)

        print("[DEBUG query_sort] Generated answer:")
        print(answer)

        query_pattern = r"<query.*?>(.*?)<query.*?>"
        keyword_pattern = r"<keyword.*?>(.*?)<keyword.*?>"
        table_pattern = r"<table.*?>(.*?)<table.*?>"
        time_pattern = r"<time.*?>(.*?)<time.*?>"

        m_query = re.search(query_pattern, answer, re.DOTALL)
        m_keyword = re.search(keyword_pattern, answer, re.DOTALL)
        m_table = re.search(table_pattern, answer, re.DOTALL)
        m_time = re.search(time_pattern, answer, re.DOTALL)

        if m_query and m_keyword and m_table and m_time:
            QU = m_query.group(1).strip()
            KE = m_keyword.group(1).strip()
            TA = m_table.group(1).strip()
            TI = m_time.group(1).strip()
            if TI == "all":
                TI = "1900-01-01:2099-01-01"
            print(beep)
            print(f"구체화 질문: {QU}, 키워드 : {KE}, 테이블 필요 유무: {TA}, 시간: {TI}")
            print(beep)
            return QU, KE, TA, TI
        else:
            print("[ERROR query_sort] 필요한 태그들이 누락되었습니다. 재시도합니다.")
            attempt += 1

    raise ValueError("LLM이 올바른 태그 형식의 답변을 생성하지 못했습니다.")


@time_tracker
def sort_by_time(time_bound, data):
    """
    원본 데이터는 유지하고 필터링된 복사본을 반환하는 함수
    """
    original_count = len(data["times"])
    print(f"[시간 필터 전] 문서 수: {original_count}")

    if time_bound == "all" or time_bound == "1900-01-01:2099-01-01":
        print(f"[시간 필터] 전체 기간 사용 - 모든 문서 포함")
        return data  # 원본 그대로

    date_format = "%Y-%m-%d"
    target_date_start = datetime.strptime(time_bound.split(":")[0], date_format)
    target_date_end = datetime.strptime(time_bound.split(":")[1], date_format)

    matching_indices = [
        i
        for i, date in enumerate(data["times"])
        if (not isinstance(date, str)) and (target_date_start < date < target_date_end)
    ]

    filtered_count = len(matching_indices)
    print(f"[시간 필터 후] 문서 수: {filtered_count}, 기간: {time_bound}")

    if filtered_count < 50 and filtered_count < original_count * 0.1:
        print(f"[경고] 시간 필터로 인해 문서가 크게 줄었습니다: {original_count} → {filtered_count}")

    filtered_data = {}
    filtered_data["file_names"] = [data["file_names"][i] for i in matching_indices]
    filtered_data["titles"] = [data["titles"][i] for i in matching_indices]
    filtered_data["times"] = [data["times"][i] for i in matching_indices]
    filtered_data["chunk_ids"] = [data["chunk_ids"][i] for i in matching_indices]

    if isinstance(data["vectors"], torch.Tensor):
        filtered_data["vectors"] = data["vectors"][matching_indices]
    else:
        filtered_data["vectors"] = [data["vectors"][i] for i in matching_indices]

    filtered_data["texts"] = [data["texts"][i] for i in matching_indices]
    filtered_data["texts_short"] = [data["texts_short"][i] for i in matching_indices]
    filtered_data["texts_vis"] = [data["texts_vis"][i] for i in matching_indices]

    return filtered_data


@time_tracker
def retrieve(query, data, N, embed_model, embed_tokenizer):
    print("[SOOWAN] retrieve : 진입")
    logging.info(f"Retrieval for query: '{query}'")
    logging.info(f"Available documents: {len(data['vectors'])}")

    try:
        sim_score = cal_sim_score(query, data["vectors"], embed_model, embed_tokenizer)
        logging.info(f"Similarity score shape: {sim_score.shape}")

        bm25_score = cal_bm25_score(query, data["texts_short"], embed_tokenizer)
        logging.info(f"BM25 score shape: {bm25_score.shape}")

        scaled_sim_score = min_max_scaling(sim_score)
        scaled_bm25_score = min_max_scaling(bm25_score)

        score = scaled_sim_score * 0.4 + scaled_bm25_score * 0.6
        score_values = score[:, 0, 0]
        top_k = score[:, 0, 0].argsort()[-N:][::-1]

        logging.info(f"Top {N} document indices: {top_k}")
        logging.info(f"Top {N} document scores: {[score[:, 0, 0][i] for i in top_k]}")
        logging.info(f"Top document titles: {[data['titles'][i] for i in top_k]}")

        documents = ""
        documents_list = []
        for i, index in enumerate(top_k):
            score_str = f"{score_values[index]:.4f}"
            documents += f"{i+1}번째 검색자료 (출처:{data['file_names'][index]}) :\n{data['texts_short'][index]}, , Score: {score_str}\n"
            documents_list.append(
                {
                    "file_name": data["file_names"][index],
                    "title": data["titles"][index],
                    "contents": data["texts_vis"][index],
                    "chunk_id": data["chunk_ids"][index],
                }
            )
        print("-------------자료 검색 성공--------------")
        print("-------", documents_list, "-------")
        print("---------------------------------------")
        return documents, documents_list

    except Exception as e:
        logging.error(f"Retrieval error: {str(e)}", exc_info=True)
        return "", []


@time_tracker
def expand_time_range_if_needed(time_bound, data, min_docs=50):
    """
    시간 필터링 결과가 너무 적은 경우 자동으로 시간 범위를 확장하는 함수
    """
    if time_bound == "all" or time_bound == "1900-01-01:2099-01-01":
        print(f"[시간 범위] 전체 기간 사용")
        return data

    filtered_data = sort_by_time(time_bound, data)
    filtered_count = len(filtered_data.get("times", []))

    if filtered_count >= min_docs:
        print(f"[시간 범위] 원래 범위로 충분한 문서 확보: {filtered_count}개")
        return filtered_data

    print(f"[시간 범위 확장] 원래 범위는 {filtered_count}개 문서만 제공 (최소 필요: {min_docs}개)")

    date_format = "%Y-%m-%d"
    try:
        start_date = datetime.strptime(time_bound.split(":")[0], date_format)
        end_date = datetime.strptime(time_bound.split(":")[1], date_format)
    except Exception as e:
        print(f"[시간 범위 오류] 날짜 형식 오류: {time_bound}, 오류: {e}")
        return data

    expansions = [
        (3, "3개월"),
        (6, "6개월"),
        (12, "1년"),
        (24, "2년"),
        (60, "5년"),
    ]

    for months, label in expansions:
        new_start = start_date - timedelta(days=30 * months // 2)
        new_end = end_date + timedelta(days=30 * months // 2)

        new_range = f"{new_start.strftime(date_format)}:{new_end.strftime(date_format)}"
        print(f"[시간 범위 확장] {label} 확장 시도: {new_range}")

        expanded_data = sort_by_time(new_range, data)
        expanded_count = len(expanded_data.get("times", []))

        if expanded_count >= min_docs:
            print(f"[시간 범위 확장] {label} 확장으로 {expanded_count}개 문서 확보")
            return expanded_data

    print(f"[시간 범위 확장] 모든 확장 시도 실패, 전체 데이터셋 사용")
    return data


@time_tracker
def cal_sim_score(query, chunks, embed_model, embed_tokenizer):
    print("[SOOWAN] cal_sim_score : 진입 / query : ", query)
    query_V = embed(query, embed_model, embed_tokenizer)
    print("[SOOWAN] cal_sim_score : query_V 생산 완료")
    if len(query_V.shape) == 1:
        query_V = query_V.unsqueeze(0)
        print("[SOOWAN] cal_sim_score : query_V.shape == 1")
    score = []
    for chunk in chunks:
        if len(chunk.shape) == 1:
            chunk = chunk.unsqueeze(0)
        query_norm = query_V / query_V.norm(dim=1)[:, None]
        chunk_norm = chunk / chunk.norm(dim=1)[:, None]
        tmp = torch.mm(query_norm, chunk_norm.transpose(0, 1)) * 100
        score.append(tmp.detach())
    return np.array(score)


@time_tracker
def cal_bm25_score(query, indexes, embed_tokenizer):
    logging.info(f"Starting BM25 calculation for query: {query}")
    logging.info(f"Document count: {len(indexes)}")

    if not indexes:
        logging.warning("Empty document list provided to BM25")
        return np.zeros(0)

    tokenized_corpus = []
    for i, text in enumerate(indexes):
        try:
            tokens = embed_tokenizer(
                text,
                return_token_type_ids=False,
                return_attention_mask=False,
                return_offsets_mapping=False,
            )
            tokens = embed_tokenizer.convert_ids_to_tokens(tokens["input_ids"])
            if len(tokens) == 0:
                logging.warning(f"Document {i} tokenized to empty list")
                tokens = ["<empty>"]
            tokenized_corpus.append(tokens)
        except Exception as e:
            logging.error(f"Failed to tokenize document {i}: {str(e)}")
            tokenized_corpus.append(["<error>"])

    try:
        bm25 = rank_bm25.BM25Okapi(tokenized_corpus)
        tokenized_query = embed_tokenizer.convert_ids_to_tokens(embed_tokenizer(query)["input_ids"])
        scores = bm25.get_scores(tokenized_query)

        if np.isnan(scores).any() or np.isinf(scores).any():
            logging.warning("BM25 produced NaN/Inf scores - replacing with zeros")
            scores = np.nan_to_num(scores)

        logging.info(
            f"BM25 scores: min={scores.min():.4f}, max={scores.max():.4f}, mean={scores.mean():.4f}"
        )
        return scores
    except Exception as e:
        logging.error(f"BM25 scoring failed: {str(e)}")
        return np.zeros(len(indexes))


@time_tracker
def embed(query, embed_model, embed_tokenizer):
    print("[SOOWAN] embed: 진입")
    inputs = embed_tokenizer(query, padding=True, truncation=True, return_tensors="pt")
    embeddings, _ = embed_model(**inputs, return_dict=False)
    print("[SOOWAN] embed: 완료")
    return embeddings[0][0]


@time_tracker
def min_max_scaling(arr):
    arr_min = arr.min()
    arr_max = arr.max()
    if arr_max == arr_min:
        print("[SOOWAN] min_max_scaling: Zero range detected, returning zeros.")
        return np.zeros_like(arr)
    return (arr - arr_min) / (arr_max - arr_min)


@time_tracker
async def generate(docs, query, model, tokenizer, config):
    PROMPT = GENERATE_PROMPT_TEMPLATE.format(docs=docs, query=query)
    print("Inference steps")
    if config.use_vllm:
        from vllm import SamplingParams
        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        accepted_request_id = str(uuid.uuid4())
        answer = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
    else:
        input_ids = tokenizer(PROMPT, return_tensors="pt", truncation=True, max_length=4024).to(
            "cuda"
        )
        token_count = input_ids["input_ids"].shape[1]
        outputs = model.generate(
            **input_ids,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )
        answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)
        print(answer)
        print(">>> decode done, returning answer")
    return answer


@time_tracker
async def collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id):
    import asyncio, concurrent.futures
    print("[SOOWAN] collect_vllm_text 진입 PROMPT: ")
    outputs = []
    async for output in model.generate(PROMPT, request_id=accepted_request_id, sampling_params=sampling_params):
        outputs.append(output)
    if not outputs:
        raise RuntimeError("No outputs were generated by the model.")
    final_output = next((o for o in outputs if getattr(o, "finished", False)), outputs[-1])
    answer = "".join([getattr(comp, "text", "") for comp in getattr(final_output, "outputs", [])])
    return answer


@time_tracker
async def generate_answer_stream(query, docs, model, tokenizer, config, http_query):
    
    prompt = STREAM_PROMPT_TEMPLATE.format(docs=docs, query=query)
    print("최종 LLM 추론용 prompt 생성 : ", prompt)
    
    import uuid
    import base64
    import io
    from PIL import Image
    from transformers import AutoProcessor
    from vllm import SamplingParams
    from vllm.multimodal.utils import fetch_image
    
    print("[IMAGE-STREAMING-QUERY] Image_query 진입")
    
    image_data = http_query.get("image_data")
    
    # 2) Image -> PIL (image_data가 있을 경우에만 처리)
    pil_image = None
    if image_data:
        try:
            print("[DEBUG] Step2 - Converting image data to PIL...")
            if isinstance(image_data, str) and (image_data.startswith("http://") or image_data.startswith("https://")):
                print("[DEBUG]   => image_data is a URL; using fetch_image()")
                pil_image = fetch_image(image_data)
            else:
                print("[DEBUG]   => image_data is presumably base64")
                if isinstance(image_data, str) and image_data.startswith("data:image/"):
                    print("[DEBUG]   => detected 'data:image/' prefix => splitting off base64 header")
                    image_data = image_data.split(",", 1)[-1]
                decoded = base64.b64decode(image_data)
                print(f"[DEBUG]   => decoded base64 length={len(decoded)} bytes")
                pil_image = Image.open(io.BytesIO(decoded)).convert("RGB")
            print("[DEBUG] Step2 - PIL image loaded successfully:", pil_image.size)
        except Exception as e:
            err_msg = f"[ERROR-step2] Failed to load image: {str(e)}"
            print(err_msg)
    else:
        print("[DEBUG] No image_data provided; proceeding with text-only query.")
    
    # 3) HF Processor 로드
    try:
        print(f"[DEBUG] Step3 - Loading processor from '{config.model_id}' ... (use_fast=False)")
        processor = AutoProcessor.from_pretrained(
            config.model_id,
            use_fast=False  # 모델이 fast processor를 지원한다면 True로 시도 가능
        )
        print("[DEBUG]   => processor loaded:", type(processor).__name__)
    except Exception as e:
        err_msg = f"[ERROR-step3] Failed to load processor: {str(e)}"
        print(err_msg)
    
    # 4) Chat Template 적용 (tokenize=False => 최종 prompt string만 얻음)
    print("[DEBUG] Step4 - Constructing messages & applying chat template (tokenize=False)...")
    messages = [
        {
            "role": "user",
            "content": []
        }
    ]
    if pil_image is not None:
        messages[0]["content"].append({"type": "image", "url": pil_image})
    messages[0]["content"].append({"type": "text",  "text": prompt})
    
    try:
        # tokenize=False => prompt를 'raw string' 형태로 얻음
        prompt_string = processor.apply_chat_template(
            messages,
            tokenize=False,           # 핵심!
            add_generation_prompt=True,
        )
        print("[DEBUG]   => prompt_string : ", prompt_string)
    except Exception as e:
        err_msg = f"[ERROR-step4] Error in processor.apply_chat_template: {str(e)}"
        print(err_msg)
    
    generate_request = {
        "prompt": prompt_string,
        "multi_modal_data": {}
    }
    if pil_image is not None:
        generate_request["multi_modal_data"]["image"] = [pil_image]
    
    if config.use_vllm:
        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        request_id = str(uuid.uuid4())
        async for partial_chunk in collect_vllm_text_stream(generate_request, model, sampling_params, request_id):
            yield partial_chunk
    else:
        import torch
        from transformers import TextIteratorStreamer

        input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4024).to("cuda")
        streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
        generation_kwargs = dict(
            **input_ids,
            streamer=streamer,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        import threading

        t = threading.Thread(target=model.generate, kwargs=generation_kwargs)
        t.start()
        for new_token in streamer:
            yield new_token


@time_tracker
async def collect_vllm_text_stream(prompt, engine: AsyncLLMEngine, sampling_params, request_id) -> str:
    async for request_output in engine.generate(prompt, request_id=request_id, sampling_params=sampling_params):
        if not request_output.outputs:
            continue
        for completion in request_output.outputs:
            yield completion.text

# -------------------------------------------------------------------------
# PROCESS IMAGE QUERY (IMAGE TO TEXT) - Legacy
# -------------------------------------------------------------------------
@time_tracker
async def image_query(http_query, model, config):
    """
    기존 ray_utils.py의 process_image_query 로직을 옮겨온 함수.
    이미지 입력을 받아 최종 한 번에 답변을 생성(스트리밍 없음).
    """
    import uuid
    import base64
    import io
    from PIL import Image
    from transformers import AutoProcessor
    from vllm import SamplingParams
    from vllm.multimodal.utils import fetch_image
    
    print("[IMAGE-NONSTREAMING-QUERY] Image_query 진입")
    
    # 1) 파라미터 파싱
    request_id = http_query.get("request_id", str(uuid.uuid4()))
    image_input = http_query.get("image_data")
    user_query = http_query.get("qry_contents", "이 이미지를 한국어로 잘 설명해주세요.")
    print(f"[DEBUG] Step1 - user_query='{user_query}', image_input type={type(image_input)}")
    
    # 2) Image -> PIL
    pil_image = None
    try:
        print("[DEBUG] Step2 - Converting image data to PIL...")
        if isinstance(image_input, str) and (
            image_input.startswith("http://") or image_input.startswith("https://")
        ):
            print("[DEBUG]   => image_input is a URL; using fetch_image()")
            pil_image = fetch_image(image_input)
        else:
            print("[DEBUG]   => image_input is presumably base64")
            if isinstance(image_input, str) and image_input.startswith("data:image/"):
                print("[DEBUG]   => detected 'data:image/' prefix => splitting off base64 header")
                image_input = image_input.split(",", 1)[-1]
            decoded = base64.b64decode(image_input)
            print(f"[DEBUG]   => decoded base64 length={len(decoded)} bytes")
            pil_image = Image.open(io.BytesIO(decoded)).convert("RGB")
        print("[DEBUG] Step2 - PIL image loaded successfully:", pil_image.size)
    except Exception as e:
        err_msg = f"[ERROR-step2] Failed to load image: {str(e)}"
        print(err_msg)
        return {"type": "error", "message": err_msg}
    
    # 3) HF Processor 로드
    try:
        print(f"[DEBUG] Step3 - Loading processor from '{config.model_id}' ... (use_fast=False)")
        processor = AutoProcessor.from_pretrained(
            config.model_id,
            use_fast=False  # 모델이 fast processor를 지원한다면 True로 시도 가능
        )
        print("[DEBUG]   => processor loaded:", type(processor).__name__)
    except Exception as e:
        err_msg = f"[ERROR-step3] Failed to load processor: {str(e)}"
        print(err_msg)
        return {"type": "error", "message": err_msg}
    
    print("[DEBUG] step 3.5 이미지에 대한 간단한 정보 및 프롬프트화 ")
    # 이미지에 대한 간단한 정보(실제 서비스에서는 이미지 메타데이터 추출 등을 수행할 수 있음)
    image_info = "Image data provided"  # 현재는 간단한 문자열 사용
    
    prompt_image_sorting = IMAGE_DESCRIPTION_PROMPT.format(image_info=image_info, user_query=user_query)
    
    # 4) Chat Template 적용 (tokenize=False => 최종 prompt string만 얻음)
    print("[DEBUG] Step4 - Constructing messages & applying chat template (tokenize=False)...")
    messages = [
        {
            "role": "system",
            "content": [{"type": "text", "text": prompt_image_sorting}]
        },
        {
            "role": "user",
            "content": [
                {"type": "image", "url": pil_image},
                {"type": "text",  "text": user_query},
            ],
        }
    ]
    
    try:
        # tokenize=False => prompt를 'raw string' 형태로 얻음
        prompt_string = processor.apply_chat_template(
            messages,
            tokenize=False,           # 핵심!
            add_generation_prompt=True,
        )
        print("[DEBUG]   => prompt_string full version:", prompt_string)
    except Exception as e:
        err_msg = f"[ERROR-step4] Error in processor.apply_chat_template: {str(e)}"
        print(err_msg)
        return {"type": "error", "message": err_msg}
    
    # 5) Sampling Params
    print("[DEBUG] Step5 - Setting sampling params...")
    sampling_params = SamplingParams(
        max_tokens=config.model.max_new_tokens,
        temperature=config.model.temperature,
        top_k=config.model.top_k,
        top_p=config.model.top_p,
        repetition_penalty=config.model.repetition_penalty,
    )
    print("[DEBUG]   => sampling_params =", sampling_params)
    
    # 6) Generate 호출
    print("[DEBUG] Step6 - Starting vLLM generate(...) using multi_modal_data")
    result_chunks = []
    try:
        # vLLM에 prompt와 함께 multi_modal_data를 넘김
        # => vLLM 내부에서 Gemma3MultiModalProcessor가 image 임베딩 및 token placement 처리
        generate_request = {
            "prompt": prompt_string,
            "multi_modal_data": {
                "image": [pil_image]  # 여러 장이라면 list에 더 추가
            }
        }

        async for out in model.generate(
            prompt=generate_request,
            sampling_params=sampling_params,
            request_id=request_id,
        ):
            result_chunks.append(out)
        print("[DEBUG] Step6 - All chunks retrieved: total:", len(result_chunks))

    except Exception as e:
        err_msg = f"[ERROR-step6] Error in model.generate: {str(e)}"
        print(err_msg)
        return {"type": "error", "message": err_msg}

    if not result_chunks:
        print("[DEBUG] Step6 - No output from model => returning error")
        return {"type": "error", "message": "No output from model."}

    final_output = next((c for c in result_chunks if getattr(c, "finished", False)), result_chunks[-1])
    answer_text = "".join(piece.text for piece in final_output.outputs)
    print(f"[DEBUG] Final answer: {answer_text}")

    # 완료
    print(f"[DEBUG] [process_image_query] DONE => request_id={request_id}")
    
    import json
    try:
        # 강제적으로 JSON 파싱 – 추가 문장이 있을 경우 추출
        # 예를 들어, JSON 블록만 추출
        match = re.search(r'\{.*\}', answer_text, re.DOTALL)
        if match:
            result = json.loads(match.group(0))
        else:
            raise ValueError("JSON 응답 형식 미확인")
        if "is_structured" not in result or "description" not in result:
            raise ValueError("응답에 필요한 키가 누락됨")
    except Exception as e:
        # 파싱 실패 시 기본 처리: 추가 RAG가 필요하도록 설정
        result = {"is_structured": False, "description": http_query.get("qry_contents", "")}
    return result


@time_tracker
async def image_streaming_query(http_query, model, tokenizer, config):
    """
    새로 추가된 이미지 스트리밍 함수:
    - 이미지 입력 + 사용자 텍스트를 받아서, 부분 토큰을 SSE로 전달할 수 있도록 yield 함.
    - use_vllm=True일 때는 collect_vllm_text_stream와 동일한 방식으로 partial chunk를 yield
    - HF standard는 TextIteratorStreamer를 이용한 방식으로 partial chunk를 yield
    """
    import uuid
    import base64
    import io
    from PIL import Image
    from transformers import AutoProcessor
    from vllm import SamplingParams
    from vllm.multimodal.utils import fetch_image

    print("[IMAGE-STREAMING-QUERY] Image_streaming_query 진입")

    # 1) 파라미터 파싱
    request_id = http_query.get("request_id", str(uuid.uuid4()))
    image_input = http_query.get("image_data")
    user_query = http_query.get("qry_contents", "이 이미지를 설명해주세요.")
    print(f"[DEBUG] Step1 - user_query='{user_query}', image_input type={type(image_input)}")
    
    # 2) Image -> PIL
    try:
        print("[DEBUG] Step2 - Converting image data to PIL...")
        if isinstance(image_input, str) and (image_input.startswith("http://") or image_input.startswith("https://")):
            print("[DEBUG]   => image_input is a URL; using fetch_image()")
            pil_image = fetch_image(image_input)
        else:
            print("[DEBUG]   => image_input is presumably base64")
            if isinstance(image_input, str) and image_input.startswith("data:image/"):
                print("[DEBUG]   => detected 'data:image/' prefix => splitting off base64 header")
                image_input = image_input.split(",", 1)[-1]
            decoded = base64.b64decode(image_input)
            print(f"[DEBUG]   => decoded base64 length={len(decoded)} bytes")
            pil_image = Image.open(io.BytesIO(decoded)).convert("RGB")
        print("[DEBUG] Step2 - PIL image loaded successfully:", pil_image.size)
    except Exception as e:
        err_msg = f"[ERROR-step2] Failed to load image: {str(e)}"
        print(err_msg)
        yield {"type": "error", "message": err_msg}
        return  # No value, simply return

    # 3) HF Processor 로드
    try:
        print(f"[DEBUG] Step3 - Loading processor from '{config.model_id}' ... (use_fast=False)")
        processor = AutoProcessor.from_pretrained(config.model_id, use_fast=False)
        print("[DEBUG]   => processor loaded:", type(processor).__name__)
    except Exception as e:
        err_msg = f"[ERROR-step3] Failed to load processor: {str(e)}"
        print(err_msg)
        yield {"type": "error", "message": err_msg}
        return

    # 4) Chat Template 적용 (tokenize=False => 최종 prompt string만 얻음)
    print("[DEBUG] Step4 - Constructing messages & applying chat template (tokenize=False)...")
    messages = [
        {
            "role": "user",
            "content": [
                {"type": "image", "url": pil_image},
                {"type": "text",  "text": user_query},
            ],
        }
    ]
    try:
        prompt_string = processor.apply_chat_template(
            messages,
            tokenize=False,           # 핵심!
            add_generation_prompt=True,
        )
        print("[DEBUG]   => prompt_string :", prompt_string
              )
    except Exception as e:
        err_msg = f"[ERROR-step4] Error in processor.apply_chat_template: {str(e)}"
        print(err_msg)
        yield {"type": "error", "message": err_msg}
        return

    # 5) Sampling Params
    print("[DEBUG] Step5 - Setting sampling params...")
    sampling_params = SamplingParams(
        max_tokens=config.model.max_new_tokens,
        temperature=config.model.temperature,
        top_k=config.model.top_k,
        top_p=config.model.top_p,
        repetition_penalty=config.model.repetition_penalty,
    )
    print("[DEBUG]   => sampling_params =", sampling_params)
    
    # 6) Generate 호출
    print("[DEBUG] Step6 - Starting vLLM generate(...) using multi_modal_data")
    generate_request = {
        "prompt": prompt_string,
        "multi_modal_data": {
            "image": [pil_image]
        }
    }
    async for partial_chunk in collect_vllm_text_stream(generate_request, model, sampling_params, request_id):
        yield partial_chunk

# ---------------------------
# *** 새로 추가된 generate_sql 함수 ***
# *** (기존에는 SQL_NS.py 안에 있었음) ***
# ---------------------------
@time_tracker
async def generate_sql(user_query, model, tokenizer, config):
    """
    기존 SQL_NS.py에서 VLLM을 통해 <unno>, <class>, <pol_port>, <pod_port>를 추출한 뒤
    run_sql_unno, run_sql_bl를 호출하는 로직을 RAG.py로 옮겼습니다.
    """
    # 먼저 메타데이터를 읽어옴
    metadata_location, metadata_unno = get_metadata(config)
    print("")
    # 프롬프트 생성
    PROMPT = SQL_EXTRACTION_PROMPT_TEMPLATE.format(metadata_location=metadata_location, query=user_query)

    from vllm import SamplingParams
    import uuid

    sampling_params = SamplingParams(
        max_tokens=config.model.max_new_tokens,
        temperature=config.model.temperature,
        top_k=config.model.top_k,
        top_p=config.model.top_p,
        repetition_penalty=config.model.repetition_penalty,
    )

    # 최대 3회 시도
    max_attempts = 3
    attempt = 0
    UN_number = UN_class = POL = POD = "NULL"
    unno_pattern = r'<unno.*?>(.*?)<unno.*?>'
    class_pattern = r'<class.*?>(.*?)<class.*?>'
    pol_port_pattern = r'<pol_port.*?>(.*?)<pol_port.*?>'
    pod_port_pattern = r'<pod_port.*?>(.*?)<pod_port.*?>'

    while attempt < max_attempts:
        accepted_request_id = str(uuid.uuid4())
        outputs_result = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
        print(f"[GENERATE_SQL] Attempt {attempt+1}, SQL Model Outputs: {outputs_result}")

        match_unno = re.search(unno_pattern, outputs_result, re.DOTALL)
        UN_number = match_unno.group(1).strip() if match_unno else "NULL"

        match_class = re.search(class_pattern, outputs_result, re.DOTALL)
        UN_class = match_class.group(1).strip() if match_class else "NULL"

        match_pol = re.search(pol_port_pattern, outputs_result, re.DOTALL)
        POL = match_pol.group(1).strip() if match_pol else "NULL"

        match_pod = re.search(pod_port_pattern, outputs_result, re.DOTALL)
        POD = match_pod.group(1).strip() if match_pod else "NULL"

        print(f"[GENERATE_SQL] 추출 결과 - UN_number: {UN_number}, UN_class: {UN_class}, POL: {POL}, POD: {POD}")

        # 조건: (UN_number != NULL or UN_class != NULL) and (POL != NULL) and (POD != NULL)
        if ((UN_number != "NULL" or UN_class != "NULL") and POL != "NULL" and POD != "NULL"):
            break
        attempt += 1

    print(f"[GENERATE_SQL] 최종 추출 값 - UN_number: {UN_number}, UN_class: {UN_class}, POL: {POL}, POD: {POD}")

    # run_sql_unno로 DG 가능 여부 확인
    final_sql_query, result = run_sql_unno(UN_class, UN_number, POL, POD)
    # 상세 B/L SQL
    detailed_sql_query, detailed_result = run_sql_bl(UN_class, UN_number, POL, POD)

    # Temporary: title, explain, table_json, chart_json = None
    title, explain, table_json, chart_json = (None,) * 4

    return final_sql_query, title, explain, result, chart_json, detailed_result

```


--- core/SQL_NS.py

```python

# core/SQL_NS.py

import os
import subprocess
from utils.tracking import time_tracker
import json
import yaml
from box import Box
import re

# ÌôòÍ≤Ω Î≥ÄÏàò ÏÑ§Ï†ï
os.environ['ORACLE_HOME'] = '/workspace/oracle/instantclient_23_7'
os.environ['LD_LIBRARY_PATH'] = os.environ['ORACLE_HOME'] + ':' + os.environ.get('LD_LIBRARY_PATH', '')
os.environ['PATH'] = os.environ['ORACLE_HOME'] + ':' + os.environ.get('PATH', '')

# Config Î∂àÎü¨Ïò§Í∏∞ (DB Ï†ëÏÜç Îì± Í∏∞ÌÉÄ Ï†ïÎ≥¥Î•º config.yamlÏóêÏÑú Í∞ÄÏ†∏Ïò®Îã§Í≥† Í∞ÄÏ†ï)
with open("./config.yaml", "r") as f:
    config_yaml = yaml.load(f, Loader=yaml.FullLoader)
    config = Box(config_yaml)

# Í∏∞Î≥∏ SQL Ï†ëÏÜçÏΩîÎìú (‚Äª Ïã§Ï†ú DB Ï†ëÏÜç Í≥ÑÏ†ï/Ï£ºÏÜåÎäî Î≥∏Ïù∏ ÌôòÍ≤ΩÏóê ÎßûÍ≤å ÏàòÏ†ï ÌïÑÏöî)
sqlplus_command = [
    "sqlplus", "-S", "LLM/L9SD2TT9XJ0H@//210.113.16.230:1521/ORA11GDR"
]

'''
### ORACLE DB Ï†ïÎ≥¥ ###
TABLE : ai_dg_check
    COLUMNS : CLS (ÏúÑÌóòÎ¨º ÌÅ¥ÎûòÏä§)
              UNNO (ÏúÑÌóòÎ¨º UN Î≤àÌò∏)
              PORT (Ìè¨Ìä∏ Î≤àÌò∏)
              ALLOW_YN (Ï∑®Í∏â Í∞ÄÎä• Ïó¨Î∂Ä)
'''

@time_tracker
def check_sqlplus():
    """
    sqlplus Î≤ÑÏ†Ñ Ï†ïÎ≥¥ ÌôïÏù∏
    """
    try:
        result = subprocess.run(['sqlplus', '-version'], capture_output=True, text=True, check=True)
        print(" SQL*Plus is working!")
        print("Version info:\n", result.stdout)
    except subprocess.CalledProcessError as e:
        print(f"Error: {e.stderr}")


@time_tracker
def check_db_connection():
    """
    DB Ïó∞Í≤∞Ïù¥ Ï†ïÏÉÅÏ†ÅÏù∏ÏßÄ ÌÖåÏä§Ìä∏
    """
    try:
        sql_query = "SELECT 1 FROM dual;\nEXIT;\n"
        result = subprocess.run(
            sqlplus_command,
            input=sql_query,
            capture_output=True,
            text=True
        )

        if "1" in result.stdout:
            print("  Successfully connected to the Namsung database!")
        else:
            print(" Connection to the database failed!")

    except subprocess.CalledProcessError as e:
        print(f" Error: {e.stderr}")


@time_tracker
def get_all_schema_tables():
    """
    Î™®Îì† Ïä§ÌÇ§Îßà, ÌÖåÏù¥Î∏î Î™©Î°ù Ï°∞Ìöå
    """
    try:
        sqlplus_cmd = [
            'sqlplus', '-S', 'LLM/L9SD2TT9XJ0H@//210.113.16.230:1521/ORA11GDR'
        ]
        sql_query = """SET PAGESIZE 0 FEEDBACK OFF VERIFY OFF HEADING OFF ECHO OFF;
        SELECT OWNER, TABLE_NAME FROM ALL_TABLES ORDER BY OWNER, TABLE_NAME;
        EXIT;"""

        result = subprocess.run(
            sqlplus_cmd,
            input=sql_query,
            capture_output=True,
            text=True
        )

        schema_tables = {}
        for line in result.stdout.splitlines():
            line = line.strip()
            if line:
                parts = line.split()
                if len(parts) >= 2:
                    schema, table = parts[0], parts[1]
                    if schema not in schema_tables:
                        schema_tables[schema] = []
                    schema_tables[schema].append(table)

        if schema_tables:
            print("  Ïä§ÌÇ§ÎßàÎ≥Ñ ÌÖåÏù¥Î∏î Î™©Î°ù:")
            for schema, tables in schema_tables.items():
                print(f"\nüîπ Ïä§ÌÇ§Îßà: {schema}")
                for t in tables:
                    print(f"  - {t}")
        else:
            print(" ÌÖåÏù¥Î∏îÏù¥ Ï°¥Ïû¨ÌïòÏßÄ ÏïäÏäµÎãàÎã§.")

        return schema_tables

    except subprocess.CalledProcessError as e:
        print(f" Error: {e.stderr}")
        return {}


def make_metadata_from_table(schema_name="ICON", table_name="OPRAIMDG"):
    """
    ÏòàÏãú Ìï®Ïàò: OPRAIMDG ÌÖåÏù¥Î∏îÎ°úÎ∂ÄÌÑ∞ UN, CLASS, DESCRIPTION Ï†ïÎ≥¥Î•º JSONÏúºÎ°ú ÎßåÎìúÎäî Ìï®Ïàò
    """
    sql_query = f"""
    SET LINESIZE 2000;
    SET PAGESIZE 0;
    SET TRIMSPOOL ON;
    COL IMDCOM FORMAT A200;
    SELECT IMDUNM, IMDCLS, REPLACE(REPLACE(IMDCOM, CHR(10), ' '), CHR(13), ' ') AS IMDCOM 
    FROM {schema_name}.{table_name};
    EXIT;
    """

    try:
        result = subprocess.run(sqlplus_command, input=sql_query, capture_output=True, text=True)
        print(f"  RESULT: \n{str(result)[:1000]}")
        output = result.stdout
        print(f"  OUTPUT: \n{str(output)[:1000]}")

        lines = output.strip().split("\n")
        print(f"  LINE: \n{str(lines)[:1000]}")
        metadata = []

        for line in lines[:-1]:
            values = line.split(None, 2)
            if len(values) == 3:
                imdunm = values[0].strip()
                imdcls = values[1].strip()
                imdcom = values[2].strip()
                metadata.append({
                    "UNNO": imdunm,
                    "Class": imdcls,
                    "Description": imdcom
                })

        json_filename = "/workspace/data/METADATA_OPRAIMDG.json"
        with open(json_filename, "w", encoding="utf-8") as json_file:
            json.dump(metadata, json_file, indent=4, ensure_ascii=False)

        print(f"  Metadata saved to {json_filename}")

    except subprocess.CalledProcessError as e:
        print(f" SQL Execution Error: {e.stderr}")


@time_tracker
def run_sql_unno(cls=None, unno=None, pol_port='KR%', pod_port='JP%'):
    """
    ai_dg_check ÌÖåÏù¥Î∏îÏóêÏÑú CLS, UNNO, PORTÏóê ÎåÄÌïú DG ÏÑ†Ï†Å Í∞ÄÎä• Ïó¨Î∂Ä Ï°∞Ìöå
    """
    cls_val = "NULL" if (cls is None or cls == "NULL") else f"'{cls}'"
    unno_val = "NULL" if (unno is None or unno == "NULL") else f"'{unno}'"

    sql_query = f"""
    SET LINESIZE 150;
    SET PAGESIZE 1000;
    SET TRIMSPOOL ON;

    SELECT 
        p.cls  AS CLS,
        p.unno AS UNNO,
        p.port AS POL_PORT,
        d.port AS POD_PORT,
        DECODE(p.allow_yn,'Y','OK','N','Forbidden','Need to contact PIC of POL') AS Landing_STATUS,
        DECODE(d.allow_yn,'Y','OK','N','Forbidden','Need to contact PIC of POL') AS Departure_STATUS
    FROM icon.ai_dg_check p
    JOIN icon.ai_dg_check d 
        ON p.unno = d.unno 
        AND p.cls = d.cls
    WHERE (p.cls={cls_val} OR {cls_val} IS NULL) AND (p.unno={unno_val} OR {unno_val} IS NULL) 
      AND p.port LIKE '{pol_port}'
      AND (p.cls={cls_val} OR {cls_val} IS NULL) AND (d.unno={unno_val} OR {unno_val} IS NULL) 
      AND d.port LIKE '{pod_port}';
    EXIT;
    """

    try:
        result = subprocess.run(sqlplus_command, input=sql_query, capture_output=True, text=True)
        print("  SQL Query Results:\n", result.stdout)
    except subprocess.CalledProcessError as e:
        print(f" Error: {e.stderr}")

    return sql_query, result.stdout


@time_tracker
def run_sql_bl(cls=None, unno=None, pol_port='KR%', pod_port='JP%'):
    """
    B/L ÏÉÅÏÑ∏ Ï°∞Ìöå
    """
    cls_val = "NULL" if (cls is None or cls == "NULL") else f"'{cls}'"
    unno_val = "NULL" if (unno is None or unno == "NULL") else f"'{unno}'"

    sql_query = f"""
    SELECT *
    FROM (
        SELECT
            MST.FRTBNO AS "B/L No",
            MST.FRTOBD AS onBoard_Date,
            MST.FRTPOL AS POL,
            MST.FRTPOD AS POD,
            MST.FRTSBM AS ship_back,
            CNT.KCTUNN AS UNNO,
            CNT.KCTCLS AS CLASS,
            COUNT(*) AS "DG_Container_Count"
        FROM ICON.WSDAMST MST
        JOIN ICON.WSDACNT CNT ON CNT.KCTBNO = MST.FRTBNO
        WHERE MST.BUKRS = '1000'
        AND CNT.BUKRS = '1000'
        AND MST.FRTOBD BETWEEN TO_CHAR(SYSDATE-1095,'YYYYMMDD')+1 AND TO_CHAR(SYSDATE+1,'YYYYMMDD')
        AND CNT.KCTUNN = {unno_val}
        AND CNT.KCTCLS = {cls_val}
        AND MST.FRTPOL = '{pol_port}'
        AND MST.FRTPOD = '{pod_port}'
        GROUP BY
            MST.FRTBNO,
            MST.FRTOBD,
            MST.FRTPOL,
            MST.FRTPOD,
            MST.FRTSBM,
            CNT.KCTUNN,
            CNT.KCTCLS
    )
    WHERE ROWNUM <= 5;
    EXIT;
    """

    try:
        result = subprocess.run(sqlplus_command, input=sql_query, capture_output=True, text=True)
        print("[SQL_NS] SQL Query run_sql_bl Results:\n", result.stdout)
    except subprocess.CalledProcessError as e:
        print(f"[SQL_NS] run_sql_bl Error: {e.stderr}")

    return sql_query, result.stdout


def get_metadata(config):
    """
    metadata_path (port Îç∞Ïù¥ÌÑ∞), metadata_unno (UN Î≤àÌò∏ Î¶¨Ïä§Ìä∏)Ïóê ÏûàÎäî ÎÇ¥Ïö©ÏùÑ Î∂àÎü¨ÏôÄ Î∞òÌôò
    """
    print("[SOOWAN] get_metadata ÏßÑÏûÖ")
    if not config or not hasattr(config, "metadata_unno"):
        raise ValueError("Config Í∞ùÏ≤¥Ïóê 'metadata_unno' ÏÜçÏÑ±Ïù¥ ÏóÜÏäµÎãàÎã§.")

    unno_path = config.metadata_unno
    port_path = config.metadata_path

    with open(port_path, "r", encoding="utf-8") as f:
        port_data = json.load(f)
    location_codes = json.dumps(port_data.get("location_code"), ensure_ascii=False)

    with open(unno_path, "r", encoding="utf-8") as f:
        unno_data = json.load(f)
    unno_list_as_string = json.dumps(unno_data, ensure_ascii=False)

    return location_codes, unno_list_as_string


if __name__ == "__main__":
    # ÏïÑÎûòÎäî ÌÖåÏä§Ìä∏/ÎîîÎ≤ÑÍπÖÏö© ÏΩîÎìú
    # ÌïÑÏöîÌïú Í≤ΩÏö∞ÏóêÎßå ÏÇ¨Ïö© Í∞ÄÎä•. Ïã§Ï†ú Ïö¥ÏòÅ ÏãúÏóî Ï†úÍ±∞Ìï† ÏàòÎèÑ ÏûàÏùå.

    check_sqlplus()             # sqlplusÍ∞Ä Ïûò ÎèôÏûëÌïòÎäîÏßÄ ÌôïÏù∏
    check_db_connection()       # Îç∞Ïù¥ÌÑ∞Î≤†Ïù¥Ïä§ Ï†ëÏÜç Ïó¨Î∂Ä ÌôïÏù∏
    schema_info = get_all_schema_tables()
    print("Schema info:", schema_info)
    # make_metadata_from_table()  # ÌäπÏ†ï ÌÖåÏù¥Î∏îÎ°úÎ∂ÄÌÑ∞ Î©îÌÉÄÎç∞Ïù¥ÌÑ∞ ÏÉùÏÑ±ÌïòÎäî ÏòàÏãú
    # ÏòàÏãú SQL Ïã§Ìñâ
    sql_q, sql_res = run_sql_unno(cls=4.1, unno=1033, pol_port="KRPUS", pod_port="JPKOB")
    print("[TEST] run_sql_unno result:", sql_q, sql_res)
    sql_q2, sql_res2 = run_sql_bl(cls=4.1, unno=1033, pol_port="KRPUS", pod_port="JPKOB")
    print("[TEST] run_sql_bl result:", sql_q2, sql_res2)

```


--- prompt/prompt_rag.py

```python

# prompt/prompt_rag.py
from datetime import datetime

# Get today's date (this is computed when the module is loaded)
TODAY = datetime.today()

### 수정 : --- 3개 질문 유형에 대해서만 SQL로 넘어감 ---
# New Prompt for query sorting
QUERY_SORT_PROMPT = f"""
<bos><start_of_turn>user
너는 질문의 유형을 파악하고 분류하는 역할이야. 질문에 대해 질문자의 의도를 파악하고, 내가 지시하는 대로 답변형태를 맞춰서 해줘. 
query는 질문을 구체화 하는 거야, 그리고 만약 질문에 오타가 있다면 고쳐줘. 
keyword는 질문의 키워드를 뽑는거야. 
table은 질문에 대한 답을 할때 정형 데이터가 필요한지 여부야, 현재는 선적 가능성 관련 질문만 가능하니 이때만 yes로 답해줘.
time은 질문에 답하기 위해 필요한 데이터의 날짜 범위야(오늘 날짜는 {TODAY.year}년 {TODAY.month}월 {TODAY.day}일). 
시간의 길이는 최소 3개월로 설정해야하고, 날짜는 1일로 설정해. (예시:2024년 10월에 대한 질문은 2024-08-01:2024-11-01) 
또한, '최근'이라는 말이 들어가면 2024-06-01:{TODAY.year}-{TODAY.month}-{TODAY.day}로 설정해줘.

내가 먼저 예시를 줄게

질문: UN 번호 1689 DG화물, 부산에서 미즈시마 선적 가능 여부를 확인해 주세요.
답변:
<query/>UN 번호 1689 화물의 부산항에서 미즈시마항 선적 가능 여부를 확인해 주세요.<query>
<keyword/>UN번호 1689, 부산항, 미즈시마항, 선적가능성<keyword>
<table/>yes<table>
<time/>2024-08-01:2024-{TODAY.month}-{TODAY.day}<time>

질문: 올해 3월에 중국 시장 전망에 대해 조사했던 내용을 정리해줘
답변:
<query/>2024년 3월 중국시장 전망에 대한 조사내용을 알려주고 정리해줘<query>
<keyword/>2024년 3월 중국시장 전망<keyword>
<table/>no<table>
<time/>2024-02-01:2024-05-01<time>

질문: 부산발 인도네시아착 경쟁사 서비스 및 항차수를 알려줘
답변:
<query/>부산 출발 인도네시아 도착 경쟁사 서비스 및 항차수<query>
<keyword/>부산발 인도네시아착 경쟁사 서비스 항차수<keyword>
<table/>no<table>
<time/>all<time>

질문: 남성해운의 인도 대리점 선정 과정은 어떻게 돼?
답변:
<query/>인도 대리점 선정과정을 보기 좋게 정리해줘<query>
<keyword/>인도 대리점 선정과정<keyword>
<table/>no<table>
<time/>all<time>

### 아래 구분자를 추가하여 실제 사용자 질문을 명확히 구분합니다.
### 새로운 질문: {{user_query}}<end_of_turn>
<start_of_turn>model
답변:
"""

# Prompt for query sorting
QUERY_SORT_PROMPT_OLD = f"""
<bos><start_of_turn>user
너는 질문의 유형을 파악하고 분류하는 역할이야. 질문에 대해 질문자의 의도를 파악하고, 내가 지시하는 대로 답변형태를 맞춰서 해줘. 
query는 질문을 구체화 하는 거야, 그리고 만약 질문에 오타가 있다면 고쳐줘. 
keyword는 질문의 키워드를 뽑는거야. 
table은 질문에 대한 답을 할때 표형식 데이터가 필요한지 여부야, 현재는 매출액 관련 질문만 대응 가능하니 이때만 yes로 답해줘.
time은 질문에 답하기 위해 필요한 데이터의 날짜 범위야(오늘 날짜는 {TODAY.year}년 {TODAY.month}월 {TODAY.day}일). 
시간의 길이는 최소 3개월로 설정해야하고, 날짜는 1일로 설정해. (예시:2024년 10월에 대한 질문은 2024-08-01:2024-11-01) 
또한, '최근'이라는 말이 들어가면 2024-06-01:{TODAY.year}-{TODAY.month}-{TODAY.day}로 설정해줘.

내가 먼저 예시를 줄게

질문: 최근 일본발 베트남착 매출면에서 우리사에 기여도가 높은 화주(고객)은 어떻게 돼?
답변:
<query/>최근 일본발 베트남착 매출면에서 우리사에 기여도가 높은 화주(고객)은 어떻게 돼?<query>
<keyword/>일본발 베트남착 매출 기여도 화주 고객<keyword>
<table/>yes<table>
<time/>2024-08-01:2024-{TODAY.month}-{TODAY.day}<time>

질문: 올해 3월에 중국 시장 전망에 대해 조사했던 내용을 정리해줘
답변:
<query/>2024년 3월 중국시장 전망에 대한 조사내용을 알려주고 정리해줘<query>
<keyword/>2024년 3월 중국시장 전망<keyword>
<table/>no<table>
<time/>2024-02-01:2024-05-01<time>

질문: 부산발 인도네시아착 경쟁사 서비스 및 항차수를 알려줘
답변:
<query/>부산 출발 인도네시아 도착 경쟁사 서비스 및 항차수<query>
<keyword/>부산발 인도네시아착 경쟁사 서비스 항차수<keyword>
<table/>no<table>
<time/>all<time>

질문: 남성해운의 인도 대리점 선정 과정은 어떻게 돼?
답변:
<query/>인도 대리점 선정과정을 보기 좋게 정리해줘<query>
<keyword/>인도 대리점 선정과정<keyword>
<table/>no<table>
<time/>all<time>

### 아래 구분자를 추가하여 실제 사용자 질문을 명확히 구분합니다.
### 새로운 질문: {{user_query}}<end_of_turn>
<start_of_turn>model
답변:
"""

# Template for generating an answer based on internal documents
GENERATE_PROMPT_TEMPLATE = """
<bos><start_of_turn>user
너는 남성해운의 도움을 주는 데이터 분석가야.

주어진 **내부 자료**를 바탕으로 **내 질문**에 **상세하고 논리정연한** 답변을 작성해줘.
**답변은 반드시 아래 규칙에 맞춰 Markdown 형식**으로 작성해줘.

1. **문단/제목:** `#`, `##`, `###` 등의 헤더를 사용하고, 문단 사이에 **한 줄 이상의 공백**을 넣어줘.
2. **강조**: 강조할 단어나 문구는 `**굵게**`(이중별표) 또는 `*기울임*`(단일별표)를 사용해줘.
3. **목록**: 필요하다면 `-` 또는 `*` 기호를 사용해 **목록**을 만들어줘.
4. **표**: SQL 표 결과가 답변에 포함될 경우, 반드시 표의 시작 부분에 "<<<TABLE>>>"와 끝 부분에 "<<<END_TABLE>>>"라는 구분자를 추가해서 출력해줘.
5. **코드 블록**: 예시 코드나 특수한 데이터는 ``` ``` 언어명 ... ``` ``` 을 사용해 표시해줘.
6. **출처**: 내부 자료를 참조했다면 **어디서 어떤 내용을 사용했는지**를 마지막에 간단히 표기해줘.
7. **각주(footnotes)**: 답변에 각주가 필요한 경우, 본문 중에 `[^1]`, `[^2]`와 같은 형태로 표시하고, 마크다운 형식과 같게 답변 마지막에 해당 각주 내용을 작성해줘.

만약 **주어진 자료**에 질문에 해당하는 내용이 **없다면**, `"내부 자료에 해당 자료 없음"`이라고만 답변해줘.

**주의**: 
- 너무 짧거나 단답형이 아닌, 충분히 **길고 자세한 보고서 형태**로 답변해줘.
- 가능하면 **논리적 근거**와 **예시**를 들어 설명해줘.
- **문단**을 명확히 구분하여 **읽기 편하게** 작성해줘.

아래는 **내부 자료**와 **질문**이 주어질 것이니, 꼭 이 지침을 따라 답변해줘.

내부 자료: {docs}

질문: {query}<end_of_turn>
<start_of_turn>model
답변:
"""

# Template for the streaming version of answer generation
STREAM_PROMPT_TEMPLATE = """
<bos><start_of_turn>user
너는 남성해운의 도움을 주는 데이터 분석가야.

주어진 **내부 자료**를 바탕으로 **내 질문**에 **상세하고 논리정연한** 답변을 작성해줘.
**답변은 반드시 아래 규칙에 맞춰 Markdown 형식**으로 작성해줘.

1. **문단/제목:** `#`, `##`, `###` 등의 헤더를 사용하고, 문단 사이에 **한 줄 이상의 공백**을 넣어줘.
2. **강조**: 강조할 단어나 문구는 `**굵게**`(이중별표) 또는 `*기울임*`(단일별표)를 사용해줘.
3. **목록**: 필요하다면 `-` 또는 `*` 기호를 사용해 **목록**을 만들어줘.
4. **표**: SQL 표 결과가 답변에 포함될 경우, 반드시 표의 시작 부분에 "<<<TABLE>>>"와 끝 부분에 "<<<END_TABLE>>>"라는 구분자를 추가해서 출력해줘.
5. **코드 블록**: 예시 코드나 특수한 데이터는 ``` ``` 언어명 ... ``` ``` 을 사용해 표시해줘.
6. **출처**: 내부 자료를 참조했다면 **어디서 어떤 내용을 사용했는지**를 마지막에 간단히 표기해줘.
7. **각주(footnotes)**: 답변에 각주가 필요한 경우, 본문 중에 `[^1]`, `[^2]`와 같은 형태로 표시하고, 마크다운 형식과 같게 답변 마지막에 해당 각주 내용을 작성해줘.

만약 **주어진 자료**에 질문에 해당하는 내용이 **없다면**, `"내부 자료에 해당 자료 없음"`이라고만 답변해줘.

**주의**: 
- 너무 짧거나 단답형이 아닌, 충분히 **길고 자세한 보고서 형태**로 답변해줘.
- 가능하면 **논리적 근거**와 **예시**를 들어 설명해줘.
- **문단**을 명확히 구분하여 **읽기 편하게** 작성해줘.

아래는 **내부 자료**와 **질문**이 주어질 것이니, 꼭 이 지침을 따라 답변해줘.

내부 자료: {docs}

질문: {query}<end_of_turn>
<start_of_turn>model
답변:
"""

IMAGE_DESCRIPTION_PROMPT = """
<bos><start_of_turn>user
너는 이미지 OCR 및 데이터 정제 전문가이다.
주어진 이미지를 분석하여, 만약 이미지에 표, 그래프, 또는 기타 구조화된 데이터가 포함되어 있다면,
모든 데이터를 누락 없이 OCR 처리하여 정돈된 전체 데이터를 출력하라.
그렇지 않다면, 이미지의 주요 내용을 중요 정보가 누락되지 않도록 최대한 자세하게 요약하라.
출력은 반드시 오직 JSON 형식으로만 출력해야 하며, 추가적인 설명이나 부가 문장은 없어야 한다.
출력 형식:
{{"is_structured": True, "description": "<이미지의 전체 OCR 결과>"}} 또는 {{"is_structured": False, "description": "<이미지 요약 내용>"}}
아래 조건을 반드시 준수하라.

이미지 정보: {image_info}<end_of_turn>
<start_of_turn>model
답변:
"""

```


--- prompt/prompt_sql.py

```python

# prompt/prompt_sql.py

"""
Prompt templates for SQL extraction tasks.
"""

SQL_EXTRACTION_PROMPT_TEMPLATE = """
<bos>
<system>
"YourRole": "질문으로 부터 조건을 추출하는 역할",
"YourJob": "아래 요구 사항에 맞추어 'unno', 'class', 'pol_port', 'pod_port' 정보를 추출하여, 예시처럼 답변을 구성해야 합니다.",
"Requirements": [
    unno: UNNO Number는 4개의 숫자로 이루어진 위험물 번호 코드야.
    class : UN Class는 2.1, 6.0,,, 의 숫자로 이루어진 코드야.
    pol_port, pod_port: 항구 코드는 5개의 알파벳 또는, 나라는 2개의 알파벳과 %로 이루어져 있어. 다음은 항구 코드에 대한 메타데이터야 {metadata_location}. 여기에서 매칭되는 코드만을 사용해야 해. 항구는 항구코드, 나라는 2개의 나라코드와 %를 사용해.
    unknown : 질문에서 찾을 수 없는 정보는 NULL을 출력해줘.
]
"Examples": [
    "질문": "UN 번호 1689 화물의 부산에서 미즈시마로의 선적 가능 여부를 확인해 주세요.",
    "답변": "<unno/>1689<unno>\\n<class/>NULL<class>\\n<pol_port/>KRPUS<pol_port>\\n<pod_port/>JPMIZ<pod_port>",
    "질문": "UN 클래스 2.1 화물의 한국에서 일본으로의 선적 가능 여부를 확인해 주세요.",
    "답변": "<unno/>NULL<unno>\\n<class/>2.1<class>\\n<pol_port/>KR%<pol_port>\\n<pod_port/>JP%<pod_port>"
]
- 최종 출력은 반드시 다음 4가지 항목을 포함해야 합니다:
    <unno/>...<unno>
    <class/>...<class>
    <pol_port/>...<pol_port>
    <pod_port/>...<pod_port>
</system>

<user>
질문: "{query}"
</user>

<assistant>
답변:
</assistant>
"""

```


-----------------

# Base-Knowledge

 - 위 파일들은 LLM 모델을 활용한 사내 RAG 서비스의 소스 코드입니다.
 - 파일 트리와 각 파일의 내용이 코드 블록 내에 포함되어, 프로젝트의 현재 구조와 상태를 한눈에 파악할 수 있습니다.
 - vLLM과 ray를 활용하여 사용성 및 추론 성능을 개선하였습니다.
 - Langchain을 활용하여 reqeust_id별로 대화를 저장하고 활용할 수 있습니다.
 - 에러 발생 시 로깅을 통해 문제를 추적할 수 있도록 설계되었습니다.


# Answer-Rule

 1. 추후 소스 코드 개선, 구조 변경, 에러 로그 추가 등 다양한 요구사항을 반영할 수 있는 확장성을 고려합니다.
 2. 전체 코드는 한국어로 주석 및 설명이 포함되어, 이해와 유지보수가 용이하도록 작성됩니다.


# My-Requirements

 1. User requirements.
 2. My requirements.
