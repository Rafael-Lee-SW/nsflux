# Project Tree of RAG for company

```
├─ .dockerignore
├─ Dockerfile
├─ RAG.py
├─ app.py
├─ config.yaml
├─ prompt_rag.py
├─ ray_setup.py
├─ ray_utils.py
├─ requirements.txt
├─ sql.py
├─ templates/chatroom.html
├─ tracking.py
└─ utils.py
```

--- .dockerignore

```python

huggingface
YeoJun

# Ignore git files and folders
.git
.gitignore

# Ignore Python cache files
__pycache__
*.pyc
*.pyo
*.pyd

# Ignore virtual environments or local build folders
venv
env
*.env
.env.*
build/
dist/

# Ignore logs and temporary files
*.log
*.tmp

# If you have any large data or model files that are not needed in build context, ignore them
data/
models/
logs/

```


--- config.yaml

```python

# config.yaml
# Server : 2x H100 (80 GB SXM5), 52 CPU cores, 483.2 GB RAM, 6 TB SSD
### Model
model_id : 'google/gemma-2-27b-it'

ray:
  actor_count: 1                  # 총 Actor 개수(same as num_replicas)
  num_gpus: 2                     # 각 Actor(Node)가 점유하고 있는 GPU 갯수
  num_cpus: 48                    # 각 Actor(Node)가 점유하고 있는 CPU 갯수 (1 actor 시에 gpu 48개, 2 actor 시에 gpu 24개 할당)
  max_batch_size: 10              # max_concurrency(actor 최대 동시 처리량, default 1000)로 대체해도 됨
  batch_wait_timeout: 0.05        
  max_ongoing_requests: 100        # ray.serve에서 deployment setting으로 동시 요청 처리 갯수를 의미함(Batch랑 다름)

use_vllm: True # vLLM 사용 여부
vllm:
  enable_prefix_caching: True
  scheduler_delay_factor: 0.1
  enable_chunked_prefill: True
  tensor_parallel_size: 2         # vLLM의 GPU 사용 갯수 (!!!! num_gpus 보다 작아야 함 !!!!)
  max_num_seqs: 192               # v1에 따른 상향
  max_num_batched_tokens: 24576   # v1에 따른 상향
  block_size: 128 # 미적용
  gpu_memory_utilization: 0.99    # v0: 0.95 / v1: 0.99로 상향
  disable_custom_all_reduce: true
  enable_memory_defrag: True      # v1 신규 기능 활성화

model:
  quantization_4bit : False # Quantize 4-bit
  quantization_8bit : False # Quantize 8-bit
  max_new_tokens : 2048      # 생성할 최대 토큰 수

  do_sample : True # True 일때만 아래가 적용
  temperature : 1.0          # 텍스트 다양성 조정: 높을수록 창의력 향상 (1.0)
  top_k : 30                 # top-k 샘플링: 상위 k개의 후보 토큰 중 하나를 선택 (50)
  top_p : 1.0                # top-p 샘플링: 누적 확률을 기준으로 후보 토큰을 선택 (1.0 보다 낮을수록 창의력 증가)
  repetition_penalty : 1.0   # 같은 단어를 반복해서 출력하지 않도록 패널티를 부여 (1.0 보다 클수록 페널티 증가)
embed_model_id : 'BM-K/KoSimCSE-roberta-multitask'
# cache_dir : "D:/huggingface" # Windows Local
# cache_dir : "/media/user/7340afbb-e4ce-4a38-8210-c6362e85eae7/RAG/RAG_application/huggingface" # Local
cache_dir : "/workspace/huggingface"  # Docker

### Data
data_path : 'data/1104_NS_DB_old.json' # VectorDB Path
metadata_path : 'data/Metadata.json' # Metadata.json Path
sql_data_path : 'data/poc.db'        # SQLite 데이터베이스 Path

### Retrieve
N : 5 # Retrieve top N chunks

### Others
beep : '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------'
seed : 4734                     # Radom Seed
k : 15                        # SQL Max Rows (None=MAX)

```


--- Dockerfile

```python

# 베이스 이미지 선택
FROM globeai/flux_ns:1.24

# 작업 디렉토리 설정
WORKDIR /workspace

# requirements.txt만 먼저 복사해서 종속성 설치 (캐시 활용)
COPY requirements.txt .

# pip 캐시 사용 안 함으로 설치 (임시 파일 최소화)
RUN pip install --no-cache-dir -r requirements.txt

# Solve the C compier
RUN apt-get update && apt-get install build-essential -y

# 현재 디렉토리의 모든 파일을 컨테이너의 /app 폴더로 복사
COPY . /workspace

# Flask 앱이 실행될 포트를 열어둠
EXPOSE 5000

# Ray Dashboard 포트 (8265)와 vLLM 관련 포트 필요 시 추가
EXPOSE 8265
# Expose port for the vLLM
EXPOSE 8000

# Flask 앱 실행 명령어
CMD ["python", "app.py"]

```


--- requirements.txt

```python

# 파일 읽기 오류: 'utf-8' codec can't decode byte 0xff in position 0: invalid start byte

```


--- app.py

```python

# app.py
import os
# Setting environment variable
# os.environ["TRANSFORMERS_CACHE"] = "/workspace/huggingface"
os.environ["HF_HOME"] = "/workspace/huggingface"
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# For the Huggingface Token setting
os.environ["HF_TOKEN_PATH"] = "/root/.cache/huggingface/token"
# Change to GNU to using OpenMP. Because this is more friendly with CUDA(NVIDIA),
# and Some library(Pytorch, Numpy, vLLM etc) use the OpenMP so that set the GNU is better.
# OpenMP: Open-Multi-Processing API
os.environ["MKL_THREADING_LAYER"] = "GNU"
# Increase download timeout (in seconds)
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "60"
# Use the vLLM as v1 version
os.environ["VLLM_USE_V1"] = "1"
os.environ["VLLM_STANDBY_MEM"] = "0"
os.environ["VLLM_METRICS_LEVEL"] = "1"
os.environ["VLLM_PROFILE_MEMORY"]= "1"

from flask import (
    Flask,
    request,
    Response,
    render_template,
    jsonify,
    g,
    stream_with_context,
)
import json
import yaml
from box import Box
from utils import random_seed, error_format
from datetime import datetime

# Import the Ray modules
from ray_setup import init_ray
from ray import serve
from ray_utils import InferenceActor
from ray_utils import InferenceService, SSEQueueManager

# ------ checking process of the thread level
import logging

# 로깅 설정: 요청 처리 시간과 현재 스레드 이름을 기록
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s %(levelname)s [%(threadName)s] %(message)s'
)

# --------------------- Streaming part ----------------------------
import ray
import uuid
import asyncio

# --------------------- Streaming part ----------------------------

# Configuration
with open("./config.yaml", "r") as f:
    config_yaml = yaml.load(f, Loader=yaml.FullLoader)
    config = Box(config_yaml)
random_seed(config.seed)

########## Ray Dashboard 8265 port ##########
init_ray()  # Initialize the Ray
sse_manager = SSEQueueManager.options(name="SSEQueueManager").remote()
serve.start(detached=True)

#### Ray-Actor 다중 ####
inference_service = InferenceService.options(num_replicas=config.ray.actor_count).bind(config)
serve.run(inference_service)
inference_handle = serve.get_deployment_handle("inference", app_name="default")

#### Ray-Actor 단독 ####
# inference_actor = InferenceActor.options(num_cpus=config.ray.num_cpus, num_gpus=config.ray.num_gpus).remote(config)

########## FLASK APP setting ##########
app = Flask(__name__)
content_type = "application/json; charset=utf-8"


# 기본 페이지를 불러오는 라우트
@app.route("/")
def index():
    return render_template("index.html")  # index.html을 렌더링

# Test 페이지를 불러오는 라우트
@app.route("/test")
def test_page():
    return render_template("index_test_streaming.html")

# chatroomPage 페이지를 불러오는 라우트
@app.route("/chat")
def chat_page():
    return render_template("chatroom.html")

# Query Endpoint (Non-streaming)
@app.route("/query", methods=["POST"])
async def query():
    try:
        
        # Log when the query is received
        receive_time = datetime.now().isoformat()
        print(f"[APP] Received /query request at {receive_time}")
        
        # Optionally, attach the client time if desired:
        http_query = request.json  # 클라이언트로부터 JSON 요청 수신
        
        http_query["server_receive_time"] = receive_time
        
        # Ray Serve 배포된 서비스를 통해 추론 요청 (자동으로 로드밸런싱됨)
        # result = await inference_actor.process_query.remote(http_query) # 단일
        result = await inference_handle.query.remote(http_query) # 다중
        if isinstance(result, dict):
            result = json.dumps(result, ensure_ascii=False)
        # print("APP.py - 결과: ", result)
        return Response(result, content_type=content_type)
    except Exception as e:
        error_resp = error_format(f"서버 처리 중 오류 발생: {str(e)}", 500)
        return Response(error_resp, content_type=content_type)

# --------------------- Streaming part ----------------------------

# Streaming Endpoint (POST 방식 SSE) → 동기식 뷰 함수로 변경
@app.route("/query_stream", methods=["POST"])
def query_stream():
    """
    POST 방식 SSE 스트리밍 엔드포인트.
    클라이언트가 {"input": "..."} 형태의 JSON을 보내면, SSE 스타일의 청크를 반환합니다.
    """
    body = request.json or {}
    user_input = body.get("input", "")
    # request_id 파트 추가
    client_request_id = body.get("request_id")
    print(f"[DEBUG] /query_stream (POST) called with user_input='{user_input}', request_id='{client_request_id}'")
    
    http_query = {"qry_contents": user_input}
    # request_id 파트 추가
    if client_request_id:
        http_query["request_id"] = client_request_id
    print(f"[DEBUG] Built http_query={http_query}")

    # Obtain request_id from Ray
    # request_id = ray.get(inference_actor.process_query_stream.remote(http_query)) # 단일
    # ----------------------------------------------------------------------------- 다중
    response = inference_handle.process_query_stream.remote(http_query)
    obj_ref = response._to_object_ref_sync()
    request_id = ray.get(obj_ref)
    # ----------------------------------------------------------------------------- 다중
    print(f"[DEBUG] streaming request_id={request_id}")

    # def sse_generator():
    #     print("[DEBUG] sse_generator started: begin pulling partial tokens in a loop")
    #     while True:
    #         partial_text = ray.get(inference_actor.pop_sse_token.remote(request_id)) # 단일
    #         if partial_text is None:
    #             print("[DEBUG] partial_text is None => no more data => break SSE loop")
    #             break
    #         if partial_text == "[[STREAM_DONE]]":
    #             print("[DEBUG] got [[STREAM_DONE]], ending SSE loop")
    #             break
    #         yield f"data: {partial_text}\n\n"
    #     # close_sse_queue 호출
    #     ray.get(inference_actor.close_sse_queue.remote(request_id)) # 단일
    #     print("[DEBUG] SSE closed.")
    
    def sse_generator():
        try:
            while True:
                # Retrieve token from SSEQueueManager
                token = ray.get(sse_manager.get_token.remote(request_id, 120))
                if token is None or token == "[[STREAM_DONE]]":
                    break
                yield f"data: {token}\n\n"
        except Exception as e:
            error_token = json.dumps({"type": "error", "message": str(e)})
            yield f"data: {error_token}\n\n"
        finally:
            # Cleanup: close the SSE queue after streaming is done
            try:
                obj_ref = inference_handle.close_sse_queue.remote(request_id)._to_object_ref_sync()
                ray.get(obj_ref)
            except Exception as ex:
                print(f"[DEBUG] Error closing SSE queue for {request_id}: {str(ex)}")
            print("[DEBUG] SSE closed.")

    return Response(sse_generator(), mimetype="text/event-stream")
# --------------------- Streaming part ----------------------------

# Flask app 실행
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=False)

```


--- ray_setup.py

```python

# ray_setup.py
import ray
from ray import serve

########## Starting Banner ############
from colorama import init, Fore, Style
init(autoreset=True)

BANNER = Fore.GREEN + r"""
'########:'##::::'##:'##:::::::'##::::'##::::::::::'##::: ##::'######::
 ##.....:: ##:::: ##: ##:::::::. ##::'##::::::::::: ###:: ##:'##... ##:
 ##::::::: ##:::: ##: ##::::::::. ##'##:::::::::::: ####: ##: ##:::..::
 ######::: ##:::: ##: ##:::::::::. ###::::::::::::: ## ## ##:. ######::
 ##...:::: ##:::: ##: ##::::::::: ## ##:::::::::::: ##. ####::..... ##:
 ##::::::: ##:::: ##: ##:::::::: ##:. ##::::::::::: ##:. ###:'##::: ##:
 ##:::::::. #######:: ########: ##:::. ##:'#######: ##::. ##:. ######::
..:::::::::.......:::........::..:::::..::.......::..::::..:::......:::
"""

def init_ray():
    print(BANNER)
    # Ray-Dashboard - GPU 상태, 사용 통계 등을 제공하는 모니터링 툴, host 0.0.0.0로 외부 접속을 허용하고, Default 포트인 8265으로 설정
    ray.init(
        include_dashboard=True,
        dashboard_host="0.0.0.0" # External IP accessable
        # dashboard_port=8265
    )
    print("Ray initialized. DashBoard running at http://192.222.54.254:8265") # New Server(2xH100)

```


--- ray_utils.py

```python

# ray_utils.py
import ray  # Ray library
from ray import serve
import json
import asyncio  # async I/O process module
from concurrent.futures import ProcessPoolExecutor  # 스레드 컨트롤
import uuid  # --- NEW OR MODIFIED ---
import time
from typing import Dict, Optional  # --- NEW OR MODIFIED ---
import threading  # To find out the usage of thread
import datetime

from RAG import (
    query_sort,
    execute_rag,
    generate_answer,
    generate_answer_stream,
)  # hypothetically
from utils import (
    load_model,
    load_data,
    process_format_to_response,
    process_to_format,
    error_format,
)


@ray.remote  # From Decorator, Each Actor is allocated 1 GPU
class InferenceActor:
    async def __init__(self, config):
        self.config = config
        # 액터 내부에서 모델 및 토크나이저를 새로 로드 (GPU에 한 번만 로드)
        self.model, self.tokenizer, self.embed_model, self.embed_tokenizer = load_model(
            config
        )
        # 데이터는 캐시 파일을 통해 로드
        self.data = load_data(config.data_path)
        # 비동기 큐와 배치 처리 설정 (마이크로배칭)
        self.request_queue = asyncio.Queue()
        self.max_batch_size = config.ray.max_batch_size  # 최대 배치 수
        self.batch_wait_timeout = config.ray.batch_wait_timeout  # 배치당 처리 시간

        # Actor 내부에서 ProcessPoolExecutor 생성 (직렬화 문제 회피)
        max_workers = int(min(config.ray.num_cpus * 0.8, (26*config.ray.actor_count)-4))
        self.process_pool = ProcessPoolExecutor(max_workers)

        self.queue_manager = ray.get_actor("SSEQueueManager")
        # --- NEW OR MODIFIED ---
        # A dictionary to store SSE queues for streaming requests
        # Key = request_id, Value = an asyncio.Queue of partial token strings
        self.active_sse_queues: Dict[str, asyncio.Queue] = {}

        self.batch_counter = 0  # New counter to track batches

        # Micro-batching만 적용
        # asyncio.create_task(self._batch_processor())

        # In-flight batching까지 추가 적용
        asyncio.create_task(self._in_flight_batch_processor())

    # --------------------------------------------------------
    # EXISTING METHODS FOR NORMAL QUERIES (unchanged)
    # --------------------------------------------------------

    async def process_query(self, http_query):
        """
        Existing synchronous method. Returns final string/dict once done.
        """
        loop = asyncio.get_event_loop()
        future = loop.create_future()
        # There's no SSE queue for normal queries
        sse_queue = None
        await self.request_queue.put((http_query, future, sse_queue))
        # print("self.request_queue : ", self.request_queue)
        return await future

    # -------------------------------------------------------------------------
    # Micro_batch_processor
    # -------------------------------------------------------------------------

    async def _batch_processor(self):
        """
        Continuously processes queued requests in batches (micro-batching).
        We add new logic for streaming partial tokens if a request has an SSE queue.
        """
        while True:
            batch = []
            batch_start_time = time.time()
            # 1) get first request from the queue
            print("=== _batch_processor waiting for request_queue item... ===")
            item = await self.request_queue.get()
            print(
                f"[DEBUG] 첫 요청 도착: {time.strftime('%H:%M:%S')} (현재 배치 크기: 1)"
            )
            batch.append(item)

            print(f"[DEBUG] Received first request at {time.strftime('%H:%M:%S')}")

            # 2) try to fill the batch up to batch_size or until timeout
            try:
                while len(batch) < self.max_batch_size:
                    print("현재 배치 사이즈 : ", len(batch))
                    print("최대 배치 사이즈 : ", self.max_batch_size)
                    item = await asyncio.wait_for(
                        self.request_queue.get(), timeout=self.batch_wait_timeout
                    )

                    batch.append(item)
                    print(
                        f"[DEBUG] 추가 요청 도착: {time.strftime('%H:%M:%S')} (현재 배치 크기: {len(batch)})"
                    )
            except asyncio.TimeoutError:
                elapsed = time.time() - batch_start_time
                print(
                    f"[DEBUG] 타임아웃 도달: {elapsed:.2f}초 후 (최종 배치 크기: {len(batch)})"
                )
                pass

            print(
                f"=== _batch_processor: 배치 사이즈 {len(batch)} 처리 시작 ({time.strftime('%H:%M:%S')}) ==="
            )

            # 각 요청 처리 전후에 로그 추가
            start_proc = time.time()
            await asyncio.gather(
                *(
                    self._process_single_query(req, fut, sse_queue)
                    for (req, fut, sse_queue) in batch
                )
            )
            proc_time = time.time() - start_proc
            print(f"[DEBUG] 해당 배치 처리 완료 (처리시간: {proc_time:.2f}초)")

    # -------------------------------------------------------------------------
    # In-flight BATCH PROCESSOR
    # -------------------------------------------------------------------------

    async def _in_flight_batch_processor(self):
        while True:
            # Wait for the first item (blocking until at least one is available)
            print(
                "=== [In-Flight Batching] Waiting for first item in request_queue... ==="
            )
            first_item = await self.request_queue.get()
            batch = [first_item]
            batch_start_time = time.time()

            print(
                "[In-Flight Batching] Got the first request. Attempting to fill a batch..."
            )

            # Attempt to fill up the batch until we hit max_batch_size or batch_wait_timeout
            while len(batch) < self.max_batch_size:
                try:
                    remain_time = self.batch_wait_timeout - (
                        time.time() - batch_start_time
                    )
                    if remain_time <= 0:
                        print(
                            "[In-Flight Batching] Timed out waiting for more requests; proceeding with current batch."
                        )
                        break
                    item = await asyncio.wait_for(
                        self.request_queue.get(), timeout=remain_time
                    )
                    batch.append(item)
                    print(
                        f"[In-Flight Batching] +1 request => batch size now {len(batch)} <<< {self.max_batch_size}"
                    )
                except asyncio.TimeoutError:
                    print(
                        "[In-Flight Batching] Timeout reached => proceeding with the batch."
                    )
                    break
            self.batch_counter += 1
            print(
                f"[BATCH {self.batch_counter}] In-Flight batch collected with {len(batch)} requests"
            )

            # We have a batch of items: each item is ( http_query_or_stream_dict, future, sse_queue )
            # We'll process them concurrently.
            tasks = []
            for request_tuple in batch:
                request_obj, fut, sse_queue = request_tuple
                tasks.append(self._process_single_query(request_obj, fut, sse_queue))

            # Actually run them all concurrently
            await asyncio.gather(*tasks)

    async def _process_single_query(self, http_query_or_stream_dict, future, sse_queue):
        """
        Process a single query from the micro-batch. If 'sse_queue' is given,
        we do partial-token streaming. Otherwise, normal final result.
        """
        print(
            f"[DEBUG] _process_single_query 시작: {time.strftime('%H:%M:%S')}, 요청 내용: {http_query_or_stream_dict}, 현재 스레드: {threading.current_thread().name}"
        )
        try:
            # --- NEW OR MODIFIED ---
            # Distinguish between normal requests vs streaming requests:
            if (
                isinstance(http_query_or_stream_dict, dict)
                and "request_id" in http_query_or_stream_dict
            ):
                # It's a streaming request
                request_id = http_query_or_stream_dict["request_id"]
                http_query = http_query_or_stream_dict["http_query"]
                is_streaming = True
                print(f"[STREAM] _process_single_query: request_id={request_id}")
            else:
                # It's a normal synchronous request
                request_id = None
                http_query = http_query_or_stream_dict
                is_streaming = False
                print("[SYNC] _process_single_query started...")

            # 1) get user query
            user_input = http_query.get("qry_contents", "")
            # To Calculate the token
            tokens = self.tokenizer(user_input, add_special_tokens=True)["input_ids"]
            print(f"[DEBUG] Processing query: '{user_input}' with {len(tokens)} tokens")

            # 2) optionally reload data if needed
            self.data = load_data(
                self.config.data_path
            )  # if you want always-latest, else skip

            # 3) classify
            print("   ... calling query_sort() ...")
            print(
                f"[DEBUG] query_sort 시작 (offload) - 스레드: {threading.current_thread().name}"
            )

            # 호출부 수정
            params = {
                "user_input": user_input,
                "model": self.model,
                "tokenizer": self.tokenizer,
                "embed_model": self.embed_model,
                "embed_tokenizer": self.embed_tokenizer,
                "data": self.data,
                "config": self.config,
            }
            QU, KE, TA, TI = await query_sort(params)
            print(f"   ... query_sort => QU={QU}, KE={KE}, TA={TA}, TI={TI}")

            # 4) RAG
            if TA == "yes":
                try:
                    docs, docs_list = execute_rag(
                        QU,
                        KE,
                        TA,
                        TI,
                        model=self.model,
                        tokenizer=self.tokenizer,
                        embed_model=self.embed_model,
                        embed_tokenizer=self.embed_tokenizer,
                        data=self.data,
                        config=self.config,
                    )
                    try:
                        retrieval, chart = process_to_format(docs_list, type="SQL")
                    except Exception as e:
                        print("[ERROR] process_to_format (SQL) failed:", str(e))
                        retrieval, chart = [], None

                    # If streaming => partial tokens
                    if is_streaming:
                        print(
                            f"[STREAM] Starting partial generation for request_id={request_id}"
                        )
                        await self._stream_partial_answer(
                            QU, docs, retrieval, chart, request_id, future
                        )
                    else:
                        # normal final result
                        output = await generate_answer(
                            QU,
                            docs,
                            model=self.model,
                            tokenizer=self.tokenizer,
                            config=self.config,
                        )
                        answer = process_to_format([output, chart], type="Answer")
                        outputs = process_format_to_response(retrieval, answer)
                        future.set_result(outputs)

                except Exception as e:
                    outputs = error_format("내부 Excel 에 해당 자료가 없습니다.", 551)
                    future.set_result(outputs)

            else:
                try:
                    print("[SOOWAN] TA is No, before make a retrieval")
                    docs, docs_list = execute_rag(
                        QU,
                        KE,
                        TA,
                        TI,
                        model=self.model,
                        tokenizer=self.tokenizer,
                        embed_model=self.embed_model,
                        embed_tokenizer=self.embed_tokenizer,
                        data=self.data,
                        config=self.config,
                    )
                    retrieval = process_to_format(docs_list, type="Retrieval")
                    print("[SOOWAN] TA is No, and make a retrieval is successed")
                    if is_streaming:
                        print(
                            f"[STREAM] Starting partial generation for request_id={request_id}"
                        )
                        await self._stream_partial_answer(
                            QU, docs, retrieval, None, request_id, future
                        )
                    else:
                        output = await generate_answer(
                            QU,
                            docs,
                            model=self.model,
                            tokenizer=self.tokenizer,
                            config=self.config,
                        )
                        print("process_to_format 이후에 OUTPUT 생성 완료")
                        answer = process_to_format([output], type="Answer")
                        print("process_to_format 이후에 ANSWER까지 생성 완료")
                        outputs = process_format_to_response(retrieval, answer)
                        future.set_result(outputs)

                except Exception as e:
                    outputs = error_format("내부 PPT에 해당 자료가 없습니다.", 552)
                    future.set_result(outputs)

        except Exception as e:
            # If error, set the future
            err_msg = f"처리 중 오류 발생: {str(e)}"
            print("[ERROR]", err_msg)
            future.set_result(error_format(err_msg, 500))

    # ------------------------------------------------------------
    # HELPER FOR STREAMING PARTIAL ANSWERS (Modified to send reference)
    # ------------------------------------------------------------
    async def _stream_partial_answer(
        self, QU, docs, retrieval, chart, request_id, future
    ):
        """
        Instead of returning a final string, we generate partial tokens
        and push them to the SSE queue in real time.
        We'll do a "delta" approach so each chunk is only what's newly added.
        """
        print(
            f"[STREAM] _stream_partial_answer => request_id={request_id}, chart={chart}"
        )

        # 단일
        # queue = self.active_sse_queues.get(request_id)
        # if not queue:
        #     print(f"[STREAM] SSE queue not found => fallback to normal final (request_id={request_id})")
        #     # fallback...
        #     return

        # This will hold the entire text so far. We'll yield only new pieces.
        
        # 먼저, 참조 데이터 전송: type을 "reference"로 명시
        reference_json = json.dumps({
            "type": "reference",
            "status_code": 200,
            "result": "OK",
            "detail": "Reference data",
            "evt_time": datetime.datetime.now().isoformat(),
            "data_list": retrieval
        }, ensure_ascii=False)
        await self.queue_manager.put_token.remote(request_id, reference_json)
        print(f"[STREAM] Sent reference data for request_id={request_id}")
        
        partial_accumulator = ""

        try:
            print(
                f"[STREAM] SSE: calling generate_answer_stream for request_id={request_id}"
            )
            async for partial_text in generate_answer_stream(
                QU, docs, self.model, self.tokenizer, self.config
            ):
                # print(f"[STREAM] Received partial_text: {partial_text}")
                new_text = partial_text[len(partial_accumulator) :]
                partial_accumulator = partial_text
                if not new_text.strip():
                    continue
                    # Wrap answer tokens in a JSON object with type "answer"
                answer_json = json.dumps({
                    "type": "answer",
                    "answer": new_text
                }, ensure_ascii=False)
                # Use the central SSEQueueManager to put tokens
                # print(f"[STREAM] Sending token: {answer_json}")
                await self.queue_manager.put_token.remote(request_id, answer_json)
            final_text = partial_accumulator
            if chart is not None:
                ans = process_to_format([final_text, chart], type="Answer")
                final_res = process_format_to_response(retrieval, ans)
            else:
                ans = process_to_format([final_text], type="Answer")
                final_res = process_format_to_response(retrieval, ans)
            future.set_result(final_res)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            print(
                f"[STREAM] done => placed [[STREAM_DONE]] for request_id={request_id}"
            )
        except Exception as e:
            msg = f"[STREAM] error in partial streaming => {str(e)}"
            future.set_result(error_format(msg, 500))
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")

    # ------------------------------------------------------------
    # NEW METHODS TO SUPPORT SSE
    # ------------------------------------------------------------
    # ----------------------
    # 1) Streaming Entrypoint
    # ----------------------
    async def process_query_stream(self, http_query: dict) -> str:
        """
        Called from /query_stream route.
        Create request_id, SSE queue, push to the micro-batch, return request_id.
        """
        # 사용자로부터 Request_id를 받거나 그렇지 않은 경우, 이를 랜덤으로 생성
        request_id = http_query.get("request_id")
        if not request_id:
            request_id = str(uuid.uuid4())
        await self.queue_manager.create_queue.remote(request_id)
        print(f"[STREAM] process_query_stream => request_id={request_id}, http_query={http_query}")


        loop = asyncio.get_event_loop()
        final_future = loop.create_future()

        sse_queue = asyncio.Queue()
        self.active_sse_queues[request_id] = sse_queue
        print(f"[STREAM] Created SSE queue for request_id={request_id}")

        # We'll push a special item (dict) onto the micro-batch queue
        queued_item = {
            "request_id": request_id,
            "http_query": http_query,
        }

        print(f"[STREAM] Putting item into request_queue for request_id={request_id}")
        await self.request_queue.put((queued_item, final_future, sse_queue))
        print(f"[STREAM] Done putting item in queue => request_id={request_id}")

        return request_id

    # ----------------------
    # 2) SSE token popping
    # ----------------------
    async def pop_sse_token(self, request_id: str) -> Optional[str]:
        """
        The SSE route calls this repeatedly to get partial tokens.
        If no token is available, we block up to 120s, else return None.
        """
        if request_id not in self.active_sse_queues:
            print(
                f"[STREAM] pop_sse_token => no SSE queue found for request_id={request_id}"
            )
            return None

        queue = self.active_sse_queues[request_id]
        try:
            token = await asyncio.wait_for(queue.get(), timeout=120.0)
            # print(f"[STREAM] pop_sse_token => got token from queue: {token}")
            return token
        except asyncio.TimeoutError:
            print(
                f"[STREAM] pop_sse_token => timed out waiting for token, request_id={request_id}"
            )
            return None

    # ----------------------
    # 3) SSE queue cleanup
    # ----------------------
    async def close_sse_queue(self, request_id: str):
        """
        Called by the SSE route after finishing.
        Remove the queue from memory.
        """
        if request_id in self.active_sse_queues:
            print(
                f"[STREAM] close_sse_queue => removing SSE queue for request_id={request_id}"
            )
            del self.active_sse_queues[request_id]
        else:
            print(f"[STREAM] close_sse_queue => no SSE queue found for {request_id}")


# Too using about two actor


# Ray Serve를 통한 배포
@serve.deployment(
    name="inference",
    max_ongoing_requests=50,
    )
class InferenceService:
    def __init__(self, config):
        self.config = config
        self.actor = InferenceActor.options(
            num_gpus=config.ray.num_gpus, num_cpus=config.ray.num_cpus
        ).remote(config)

    async def query(self, http_query: dict):
        result = await self.actor.process_query.remote(http_query)
        return result

    async def process_query_stream(self, http_query: dict) -> str:
        req_id = await self.actor.process_query_stream.remote(http_query)
        return req_id

    async def pop_sse_token(self, req_id: str) -> str:
        token = await self.actor.pop_sse_token.remote(req_id)
        return token

    async def close_sse_queue(self, req_id: str) -> str:
        await self.actor.close_sse_queue.remote(req_id)
        return "closed"


# Ray의 요청을 비동기적으로 관리하기 위해 도입하는 큐-매니저
@ray.remote
class SSEQueueManager:
    def __init__(self):
        self.active_queues = {}
        self.lock = asyncio.Lock()

    async def create_queue(self, request_id):
        async with self.lock:
            self.active_queues[request_id] = asyncio.Queue()
            return True

    async def get_queue(self, request_id):
        return self.active_queues.get(request_id)

    async def get_token(self, request_id, timeout: float):
        queue = self.active_queues.get(request_id)
        if queue:
            try:
                token = await asyncio.wait_for(queue.get(), timeout=timeout)
                return token
            except asyncio.TimeoutError:
                return None
        return None

    async def put_token(self, request_id, token):
        async with self.lock:
            if request_id in self.active_queues:
                await self.active_queues[request_id].put(token)
                return True
            return False

    async def delete_queue(self, request_id):
        async with self.lock:
            if request_id in self.active_queues:
                del self.active_queues[request_id]
                return True
            return False

```


--- utils.py

```python

# utils.py
import json
import numpy as np
import torch
import random
import shutil
from datetime import datetime, timedelta
from transformers import (
    AutoModel,
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    AutoConfig,
)

import os

# Import vLLM utilities
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

# Define the minimum valid file size (e.g., 10MB)
MIN_WEIGHT_SIZE = 10 * 1024 * 1024

# For tracking execution time of functions
from tracking import time_tracker

# Logging
import logging

logging.basicConfig(level=logging.DEBUG)


# -------------------------------------------------
# Function: find_weight_directory
# -------------------------------------------------
# Recursively searches for weight files (safetensors or pytorch_model.bin) in a given base path.
# This method Find the files searching the whole directory
# Because, vLLM not automatically find out the model files.
# -------------------------------------------------
@time_tracker
def find_weight_directory(base_path):
    # ---- Recursively searches for weight files in a given base path ----
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if ".safetensors" in file or "pytorch_model.bin" in file:
                file_path = os.path.join(root, file)
                try:
                    if os.path.getsize(file_path) >= MIN_WEIGHT_SIZE:
                        return root, "safetensors" if ".safetensors" in file else "pt"
                    else:
                        logging.debug(
                            f"파일 {file_path}의 크기가 너무 작음: {os.path.getsize(file_path)} bytes"
                        )
                except Exception as ex:
                    logging.debug(f"파일 크기 확인 실패: {file_path} - {ex}")
    return None, None


# -------------------------------------------------
# Function: load_model
# -------------------------------------------------
@time_tracker
def load_model(config):
    # Loads the embedding model and the main LLM model (using vLLM if specified in the config).
    
    # Get the HF token from the environment variable.
    logging.info("Starting model loading...")
    token = os.getenv("HF_TOKEN_PATH")
    # Check if token is likely a file path.
    if token is not None and not token.startswith("hf_"):
        if os.path.exists(token) and os.path.isfile(token):
            try:
                with open(token, "r") as f:
                    token = f.read().strip()
            except Exception as e:
                print("DEBUG: Exception while reading token file:", e)
                logging.warning("Failed to read token from file: %s", e)
                token = None
        else:
            logging.warning("The HF_TOKEN path does not exist: %s", token)
            token = None
    else:
        print("DEBUG: HF_TOKEN appears to be a token string; using it directly:")

    if token is None or token == "":
        logging.warning("HF_TOKEN is not set. Access to gated models may fail.")
        token = None

    # -------------------------------
    # Load the embedding model and tokenizer.
    # -------------------------------
    print("Loading embedding model")
    try:
        embed_model = AutoModel.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,  # using 'token' parameter
        )
    except Exception as e:
        raise e
    try:
        embed_tokenizer = AutoTokenizer.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,
        )
    except Exception as e:
        raise e
    print(":Embedding tokenizer loaded successfully.")
    embed_model.eval()
    embed_tokenizer.model_max_length = 4096

    # -------------------------------
    # Load the main LLM model via vLLM.
    # -------------------------------
    if config.use_vllm:
        print("vLLM mode enabled. Starting to load main LLM model via vLLM.")
        if config.model.quantization_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
            print("Using 4-bit quantization.")
        elif config.model.quantization_8bit:
            bnb_config = BitsAndBytesConfig(load_in_8bit=True)
            print("Using 8-bit quantization.")
        else:
            bnb_config = None
            print("Using pure option of Model(No quantization)")

        local_model_path = os.path.join(
            config.cache_dir, "models--" + config.model_id.replace("/", "--")
        )
        local_model_path = os.path.abspath(local_model_path)

        config_file = os.path.join(local_model_path, "config.json")
        need_patch = False

        if not os.path.exists(config_file):
            os.makedirs(local_model_path, exist_ok=True)
            try:
                hf_config = AutoConfig.from_pretrained(
                    config.model_id,
                    cache_dir=config.cache_dir,
                    trust_remote_code=True,
                    token=token,
                )
            except Exception as e:
                raise e
            config_dict = hf_config.to_dict()
            if not config_dict.get("architectures"):
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config_dict, f)
        else:
            with open(config_file, "r", encoding="utf-8") as f:
                config_dict = json.load(f)
            if not config_dict.get("architectures"):
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
                with open(config_file, "w", encoding="utf-8") as f:
                    json.dump(config_dict, f)

        weight_dir, weight_format = find_weight_directory(local_model_path)
        if weight_dir is None:
            print("DEBUG: No model weights found. Attempting to download model snapshot.")
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    print(f"DEBUG: Snapshot download attempt {attempt+1}...")
                    # Attempt to download the model snapshot using the Hugging Face hub function.
                    from huggingface_hub import snapshot_download
                    snapshot_download(config.model_id, cache_dir=config.cache_dir, token=token)
                    break  # If download succeeds, break out of the loop.
                except Exception as e:
                    print(f"DEBUG: Snapshot download attempt {attempt+1} failed:", e)
                    if attempt < max_retries - 1:
                        print("DEBUG: Retrying snapshot download...")
                    else:
                        raise RuntimeError(f"Snapshot download failed after {max_retries} attempts: {e}")
            # After download, try to find the weights again.
            weight_dir, weight_format = find_weight_directory(local_model_path)
            if weight_dir is None:
                raise RuntimeError(f"Unable to find model weights even after snapshot download in {local_model_path}.")

        snapshot_config = os.path.join(weight_dir, "config.json")
        if not os.path.exists(snapshot_config):
            shutil.copy(config_file, snapshot_config)
        engine_args = AsyncEngineArgs(
            model=weight_dir,
            tokenizer=config.model_id,
            download_dir=config.cache_dir,
            trust_remote_code=True,
            config_format="hf",
            load_format=weight_format,
        )
        
        vllm_conf = config.get("vllm", {})
        
        engine_args.enable_prefix_caching = True
        engine_args.scheduler_delay_factor = vllm_conf.get("scheduler_delay_factor", 0.1)
        engine_args.enable_chunked_prefill = True
        engine_args.tensor_parallel_size = vllm_conf.get("tensor_parallel_size", 1) # Using Multi-GPU at once.
        engine_args.max_num_seqs = vllm_conf.get("max_num_seqs", 128)
        engine_args.max_num_batched_tokens = vllm_conf.get("max_num_batched_tokens", 8192)
        # engine_args.block_size = vllm_conf.get("block_size", 128)
        engine_args.gpu_memory_utilization = vllm_conf.get("gpu_memory_utilization", 0.9)
        
        if vllm_conf.get("disable_custom_all_reduce", False):
            engine_args.disable_custom_all_reduce = True # For Fixing the Multi GPU problem
        
        engine_args.enable_memory_defrag = True # v1
        
        # print("Final EngineArgs:", engine_args)
        print("EngineArgs setting be finished")

        try:
            # --- 해결책: 현재 스레드가 메인 스레드가 아니면 signal 함수를 임시 패치 ---
            import threading, signal
            if threading.current_thread() is not threading.main_thread():
                original_signal = signal.signal
                signal.signal = lambda s, h: None  # signal 설정 무시
                print("비메인 스레드에서 signal.signal을 monkey-patch 하였습니다.")
            # --- 해결책: ------------------------------------------------------ ---
            engine = AsyncLLMEngine.from_engine_args(engine_args) # Original
            # 엔진 생성 후 원래 signal.signal으로 복원 (필요 시) ----------------- ---
            if threading.current_thread() is not threading.main_thread():
                signal.signal = original_signal
            # --- 해결책: ------------------------------------------------------ ---
            print("DEBUG: vLLM engine successfully created.") # Original
            
        except Exception as e:
            print("DEBUG: Exception during engine creation:", e)
            if "HeaderTooSmall" in str(e):
                print("DEBUG: Falling back to PyTorch weights.")
                fallback_dir = None
                for root, dirs, files in os.walk(local_model_path):
                    for file in files:
                        if (
                            "pytorch_model.bin" in file
                            and os.path.getsize(os.path.join(root, file))
                            >= MIN_WEIGHT_SIZE
                        ):
                            fallback_dir = root
                            break
                    if fallback_dir:
                        break
                if fallback_dir is None:
                    logging.error(
                        "DEBUG: No PyTorch weight file found in", local_model_path
                    )
                    raise e
                engine_args.load_format = "pt"
                engine_args.model = fallback_dir
                print("DEBUG: New EngineArgs for fallback:", engine_args)
                engine = AsyncLLMEngine.from_engine_args(engine_args)
                print("DEBUG: vLLM engine created with PyTorch fallback.")
            else:
                logging.error("DEBUG: Engine creation failed:", e)
                raise e

        engine.is_vllm = True

        print("DEBUG: Loading main LLM tokenizer with token authentication.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
                local_files_only=True  # Force loading from local cache to avoid hub requests
            )
        except Exception as e:
            print("DEBUG: Exception loading main tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        return engine, tokenizer, embed_model, embed_tokenizer

    else:
        print("DEBUG: vLLM is not used. Loading model via standard HF method.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        try:
            model = AutoModelForCausalLM.from_pretrained(
                config.model_id,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                cache_dir=config.cache_dir,
                quantization_config=bnb_config,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading model:", e)
            raise e
        model.eval()
        return model, tokenizer, embed_model, embed_tokenizer


@time_tracker
def load_data(data_path):
    with open(data_path, "r", encoding="utf-8") as json_file:
        data = json.load(json_file)
    file_names = []
    titles = []
    times = []
    vectors = []
    texts = []
    texts_short = []
    texts_vis = []
    tmp = 0
    for file in data:
        for chunk in file["chunks"]:
            file_names.append(file["file_name"])
            vectors.append(np.array(chunk["vector"]))
            titles.append(chunk["title"])
            if chunk["date"] != None:
                times.append(datetime.strptime(chunk["date"], "%Y-%m-%d"))
            else:
                tmp += 1
                times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))
            texts.append(chunk["text"])
            texts_short.append(chunk["text_short"])
            texts_vis.append(chunk["text_vis"])
    vectors = np.array(vectors)
    vectors = torch.from_numpy(vectors).to(torch.float32)
    data_ = {
        "file_names": file_names,
        "titles": titles,
        "times": times,
        "vectors": vectors,
        "texts": texts,
        "texts_short": texts_short,
        "texts_vis": texts_vis,
    }
    print(f"Data Loaded! Full length:{len(titles)}, Time Missing:{tmp}")
    print(f"Time Max:{max(times)}, Time Min:{min(times)}")
    return data_


@time_tracker
def random_seed(seed):
    # Set random seed for Python's built-in random module
    random.seed(seed)

    # Set random seed for NumPy
    np.random.seed(seed)

    # Set random seed for PyTorch
    torch.manual_seed(seed)

    # Ensure the same behavior on different devices (CPU vs GPU)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # If using multi-GPU.

    # Enable deterministic algorithms
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


@time_tracker
def process_to_format(qry_contents, type):
    # 여기서 RAG 시스템을 호출하거나 답변을 생성하도록 구현하세요.
    # 예제 응답 형식
    ### rsp_type : RA(Retrieval All), RT(Retrieval Text), RB(Retrieval taBle), AT(Answer Text), AB(Answer taBle) ###
    # print("[SOOWAN] process_to_format 진입")
    if type == "Retrieval":
        print("[SOOWAN] 타입 : 리트리버")
        tmp_format = {"rsp_type": "R", "rsp_tit": "남성 내부 데이터", "rsp_data": []}
        for i, form in enumerate(qry_contents):
            tmp_format_ = {
                "rsp_tit": f"{i+1}번째 검색데이터: {form['title']} (출처:{form['file_name']})",
                "rsp_data": form["contents"],
            }
            tmp_format["rsp_data"].append(tmp_format_)
        return tmp_format

    elif type == "SQL":
        # print("[SOOWAN] 타입 : SQL")
        tmp_format = {
            "rsp_type": "R",
            "rsp_tit": "남성 내부 데이터",
            "rsp_data": [{"rsp_tit": "SQL Query 결과표", "rsp_data": []}],
        }
        tmp_format_sql = {
            "rsp_type": "TB",
            "rsp_tit": qry_contents[0]["title"],
            "rsp_data": qry_contents[0]["data"],
        }
        tmp_format_chart = {
            "rsp_type": "CT",
            "rsp_tit": qry_contents[1]["title"],
            "rsp_data": {"chart_tp": "BAR", "chart_data": qry_contents[1]["data"]},
        }
        tmp_format["rsp_data"][0]["rsp_data"].append(tmp_format_sql)
        # tmp_format['rsp_data'].append(tmp_format_chart)
        return tmp_format, tmp_format_chart

    elif type == "Answer":
        print("[SOOWAN] 타입 : 대답")
        tmp_format = {"rsp_type": "A", "rsp_tit": "답변", "rsp_data": []}
        for i, form in enumerate(qry_contents):
            if i == 0:
                tmp_format_ = {"rsp_type": "TT", "rsp_data": form}
                tmp_format["rsp_data"].append(tmp_format_)
            elif i == 1:
                tmp_format["rsp_data"].append(form)
            else:
                None

        return tmp_format

    else:
        print("Error! Type Not supported!")
        return None


@time_tracker
def process_format_to_response(*formats):
    # Get multiple formats to tuple

    ans_format = {
        "status_code": 200,
        "result": "OK",
        "detail": "",
        "evt_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
        "data_list": [],
    }

    for format in formats:
        ans_format["data_list"].append(format)

    return ans_format


@time_tracker
def error_format(message, status):
    ans_format = {
        "status_code": status,
        "result": message,
        "detail": "",
        "evt_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
    }
    return json.dumps(ans_format)

```


--- RAG.py

```python

# RAG.py
import torch
import re
import numpy as np
import rank_bm25
import random
import uuid
from datetime import datetime, timedelta
from sql import generate_sql

# Tracking
from tracking import time_tracker

# Import the vLLM to use the AsyncLLMEngine
from vllm.engine.async_llm_engine import AsyncLLMEngine

# In RAG.py (at the top, add an import for prompts)
from prompt_rag import QUERY_SORT_PROMPT, GENERATE_PROMPT_TEMPLATE, STREAM_PROMPT_TEMPLATE

global beep
beep = "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------"

@time_tracker
def execute_rag(QU, KE, TA, TI, **kwargs):
    # print("[SOOWAN]: execute_rag : 진입")
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    embed_model = kwargs.get("embed_model")
    embed_tokenizer = kwargs.get("embed_tokenizer")
    data = kwargs.get("data")
    config = kwargs.get("config")

    if TA == "yes":  # Table 이 필요하면
        # print("[SOOWAN]: execute_rag : 테이블 필요")
        # SQL
        final_sql_query, title, explain, table_json, chart_json = generate_sql(
            QU, model, tokenizer, config
        )

        # docs : 다음 LLM Input 으로 만들것 (String)
        PROMPT = f"""\ 
다음은 SQL 추출에 사용된 쿼리문이야 : {final_sql_query}. \
추가 설명 : {explain}. \
실제 SQL 추출된 데이터 : {str(table_json)}. \
"""
        # docs_list : 사용자들에게 보여줄 정보 (List)
        docs_list = [
            {"title": title, "data": table_json},
            {"title": "시각화 차트", "data": chart_json},
        ]

        return PROMPT, docs_list

    else:
        # print("[SOOWAN]: execute_rag : 테이블 필요없음")
        # RAG
        data = sort_by_time(TI, data)
        docs, docs_list = retrieve(KE, data, config.N, embed_model, embed_tokenizer)
        return docs, docs_list


@time_tracker
async def generate_answer(query, docs, **kwargs):
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    config = kwargs.get("config")
    
    answer = await generate(docs, query, model, tokenizer, config)
    return answer


@time_tracker
async def query_sort(params):
    # params: 딕셔너리로 전달된 값들
    query = params["user_input"]
    model = params["model"]
    tokenizer = params["tokenizer"]
    embed_model = params["embed_model"]
    embed_tokenizer = params["embed_tokenizer"]
    data = params["data"]
    config = params["config"]

    # prompts/prompt_rag.py에서 프롬프트 별도 관리
    PROMPT = QUERY_SORT_PROMPT.format(user_query=query)
    
    # Get Answer from LLM
    print("##### query_sort is starting #####")
    if config.use_vllm:  # use_vllm = True case 
        from vllm import SamplingParams

        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        accepted_request_id = str(uuid.uuid4())
        answer = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
    else:
        input_ids = tokenizer(
            PROMPT, return_tensors="pt", truncation=True, max_length=4024
        ).to("cuda")
        token_count = input_ids["input_ids"].shape[1]
        outputs = model.generate(
            **input_ids,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )
        answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)

    print("[DEBUG query_sort] Generated answer:")
    print(answer)
    
    # Regular expressions for tags
    query_pattern = r"<query.*?>(.*?)<query.*?>"
    keyword_pattern = r"<keyword.*?>(.*?)<keyword.*?>"
    table_pattern = r"<table.*?>(.*?)<table.*?>"
    time_pattern = r"<time.*?>(.*?)<time.*?>"
    
    # [DEBUG-CHANGE]: Check each match before calling group(1)
    m_query = re.search(query_pattern, answer, re.DOTALL)
    m_keyword = re.search(keyword_pattern, answer, re.DOTALL)
    m_table = re.search(table_pattern, answer, re.DOTALL)
    m_time = re.search(time_pattern, answer, re.DOTALL)
    
    if not m_query:
        print("[ERROR query_sort] query_pattern not found in answer:")
        print(answer)
        raise ValueError("Missing <query> tag in generated answer.")
    if not m_keyword:
        print("[ERROR query_sort] keyword_pattern not found in answer:")
        print(answer)
        raise ValueError("Missing <keyword> tag in generated answer.")
    if not m_table:
        print("[ERROR query_sort] table_pattern not found in answer:")
        print(answer)
        raise ValueError("Missing <table> tag in generated answer.")
    if not m_time:
        print("[ERROR query_sort] time_pattern not found in answer:")
        print(answer)
        raise ValueError("Missing <time> tag in generated answer.")

    QU = m_query.group(1)
    KE = m_keyword.group(1)
    TA = m_table.group(1)
    TI = m_time.group(1)

    if TI == "all":
        TI = "1900-01-01:2099-01-01"
    print(beep)
    print(f"구체화 질문: {QU}, 키워드 : {KE}, 테이블 필요 유무: {TA}, 시간: {TI}")
    print(beep)
    return QU, KE, TA, TI


@time_tracker
def sort_by_time(time_bound, data):
    date_format = "%Y-%m-%d"
    target_date_start = datetime.strptime(time_bound.split(":")[0], date_format)
    target_date_end = datetime.strptime(time_bound.split(":")[1], date_format)

    matching_indices = [
        i
        for i, date in enumerate(data["times"])
        if (not isinstance(date, str)) and (target_date_start < date < target_date_end)
    ]

    (
        data["file_names"],
        data["titles"],
        data["times"],
        data["vectors"],
        data["texts"],
        data["texts_short"],
        data["texts_vis"],
    ) = (
        [lst[i] for i in matching_indices]
        for lst in (
            data["file_names"],
            data["titles"],
            data["times"],
            data["vectors"],
            data["texts"],
            data["texts_short"],
            data["texts_vis"],
        )
    )
    return data


@time_tracker
def retrieve(query, data, N, embed_model, embed_tokenizer):
    # print("[SOOWAN] retrieve : 진입")
    # print("[SOOWAN] retrieve : 진입 정보 :", query)
    
    sim_score = cal_sim_score(query, data["vectors"], embed_model, embed_tokenizer)
    # print("[SOOWAN] retrieve : sim_score")
    
    try:
        bm25_score = cal_bm25_score(query, data["texts_short"], embed_tokenizer)
    except Exception as e:
        # print("[SOOWAN] retrieve : BM25 score exception, using zeros", e)
        bm25_score = np.zeros(len(data["texts_short"]))
    # print("[SOOWAN] retrieve : bm25_score")
    
    scaled_sim_score = min_max_scaling(sim_score)
    scaled_bm25_score = min_max_scaling(bm25_score)
    score = scaled_sim_score * 0.4 + scaled_bm25_score * 0.6
    top_k = score[:, 0, 0].argsort()[-N:][::-1]
    documents = ""
    documents_list = []
    for i, index in enumerate(top_k):
        documents += f"{i+1}번째 검색자료 (출처:{data['file_names'][index]}) :\n{data['texts_short'][index]}\n"
        documents_list.append({
            "file_name": data["file_names"][index],
            "title": data["titles"][index],
            "contents": data["texts_vis"][index],
        })
        print("\n" + beep)
    print("-------------자료 검색 성공--------------")
    return documents, documents_list


@time_tracker
def cal_sim_score(query, chunks, embed_model, embed_tokenizer):
    query_V = embed(query, embed_model, embed_tokenizer)
    if len(query_V.shape) == 1:
        query_V = query_V.unsqueeze(0)
    score = []
    for chunk in chunks:
        if len(chunk.shape) == 1:
            chunk = chunk.unsqueeze(0)
        query_norm = query_V / query_V.norm(dim=1)[:, None]
        chunk_norm = chunk / chunk.norm(dim=1)[:, None]
        tmp = torch.mm(query_norm, chunk_norm.transpose(0, 1)) * 100
        score.append(tmp.detach())
    return np.array(score)


@time_tracker
def cal_bm25_score(query, indexes, embed_tokenizer):
    # print("[SOOWAN] cal_bm25_score : 진입")
    try:
        tokenized_corpus = [
            embed_tokenizer(
                text,
                return_token_type_ids=False,
                return_attention_mask=False,
                return_offsets_mapping=False,
            )
            for text in indexes
        ]
        tokenized_corpus = [
            embed_tokenizer.convert_ids_to_tokens(corpus["input_ids"])
            for corpus in tokenized_corpus
        ]
        # print(f"[SOOWAN] cal_bm25_score : Tokenized corpus (first 2 items): {tokenized_corpus[:2]}")
    except Exception as e:
        # print(f"[SOOWAN ERROR BM25] Error tokenizing corpus: {str(e)}")
        return np.zeros(len(indexes))
    if not tokenized_corpus or all(len(tokens) == 0 for tokens in tokenized_corpus):
        # print("[SOOWAN] cal_bm25_score: Empty tokenized corpus, returning zeros.")
        return np.zeros(len(indexes))
    try:
        bm25 = rank_bm25.BM25Okapi(tokenized_corpus)
    except Exception as e:
        # print(f"[SOOWAN ERROR BM25] Error initializing BM25: {str(e)}")
        return np.zeros(len(indexes))
    try:
        tokenized_query = embed_tokenizer(query)
        tokenized_query = embed_tokenizer.convert_ids_to_tokens(tokenized_query["input_ids"])
        # print(f"[SOOWAN] cal_bm25_score : Tokenized query: {tokenized_query}")
    except Exception as e:
        # print(f"[SOOWAN ERROR BM25] Error tokenizing query: {str(e)}")
        return np.zeros(len(indexes))
    try:
        bm25_score = bm25.get_scores(tokenized_query)
        # print(f"[SOOWAN] cal_bm25_score : BM25 score: {bm25_score}")
    except Exception as e:
        # print(f"[SOOWAN ERROR BM25] Error computing BM25 scores: {str(e)}")
        return np.zeros(len(indexes))
    return np.array(bm25_score)


@time_tracker
def embed(query, embed_model, embed_tokenizer):
    inputs = embed_tokenizer(query, padding=True, truncation=True, return_tensors="pt")
    embeddings, _ = embed_model(**inputs, return_dict=False)
    return embeddings[0][0]


@time_tracker
def min_max_scaling(arr):
    arr_min = arr.min()
    arr_max = arr.max()
    if arr_max == arr_min:
        # print("[SOOWAN] min_max_scaling: Zero range detected, returning zeros.")
        return np.zeros_like(arr)
    return (arr - arr_min) / (arr_max - arr_min)


@time_tracker
async def generate(docs, query, model, tokenizer, config):
    PROMPT = GENERATE_PROMPT_TEMPLATE.format(docs=docs, query=query)
    print("Inference steps")
    if config.use_vllm:
        from vllm import SamplingParams
        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        accepted_request_id = str(uuid.uuid4())
        answer = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
    else:
        input_ids = tokenizer(PROMPT, return_tensors="pt", truncation=True, max_length=4024).to("cuda")
        token_count = input_ids["input_ids"].shape[1]
        outputs = model.generate(
            **input_ids,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )
        generated_tokens = outputs[0].shape[0]
        answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)
        print(answer)
        print(">>> decode done, returning answer")
    return answer


@time_tracker
async def collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id):
    import asyncio, concurrent.futures
    outputs = []
    async for output in model.generate(PROMPT, request_id=accepted_request_id, sampling_params=sampling_params):
        outputs.append(output)
    if not outputs:
        raise RuntimeError("No outputs were generated by the model.")
    final_output = next((o for o in outputs if getattr(o, "finished", False)), outputs[-1])
    answer = "".join([getattr(comp, "text", "") for comp in getattr(final_output, "outputs", [])])
    return answer


@time_tracker
async def generate_answer_stream(query, docs, model, tokenizer, config):
    prompt = STREAM_PROMPT_TEMPLATE.format(docs=docs, query=query)
    if config.use_vllm:
        from vllm import SamplingParams
        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        request_id = str(uuid.uuid4())
        async for partial_chunk in collect_vllm_text_stream(prompt, model, sampling_params, request_id):
            # print(f"[STREAM] generate_answer_stream yielded: {partial_chunk}")
            yield partial_chunk
    else:
        import torch
        from transformers import TextIteratorStreamer
        input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4024).to("cuda")
        streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
        generation_kwargs = dict(
            **input_ids,
            streamer=streamer,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        import threading
        t = threading.Thread(target=model.generate, kwargs=generation_kwargs)
        t.start()
        for new_token in streamer:
            yield new_token

@time_tracker
async def collect_vllm_text_stream(prompt, engine: AsyncLLMEngine, sampling_params, request_id) -> str:
    async for request_output in engine.generate(prompt, request_id=request_id, sampling_params=sampling_params):
        if not request_output.outputs:
            continue
        for completion in request_output.outputs:
            # print(f"[STREAM] collect_vllm_text_stream yielding: {completion.text}")
            yield completion.text


if __name__ == "__main__":
    import asyncio
    # engine = AsyncLLMEngine.from_engine_args(engine_args, start_engine_loop=False)
    # if not engine.is_running:
    #     engine.start_background_loop()
    
    async def main():
        status = True
        while status:
            query = input("질문 : ")
            QU, TA, TI = await query_sort({"user_input": query, "model": None, "tokenizer": None, "embed_model": None, "embed_tokenizer": None, "data": None, "config": None})
            print("query_sort result done")
            if TA == "yes":
                print("\n" + beep)
                SQL_results = generate_sql(QU)
                answer = await generate(SQL_results, query)
                print(answer)
                print("\n" + beep)
            else:
                file_names, titles, times, vectors, texts, texts_short = sort_by_time(TI, file_names, titles, times, vectors, texts, texts_short)
                print("\n" + beep)
                docs = retrieve(QU, vectors, texts, texts_short, file_names, N)
                print("\n" + beep)
                answer = await generate(docs, query)
                print(answer)
                print("\n" + beep)
    asyncio.run(main())

```


--- prompt_rag.py

```python

# prompts/prompt_rag.py
from datetime import datetime

# Get today's date (this is computed when the module is loaded)
TODAY = datetime.today()

# Prompt for query sorting
QUERY_SORT_PROMPT = f"""
<bos><start_of_turn>user
너는 질문의 유형을 파악하고 분류하는 역할이야. 질문에 대해 질문자의 의도를 파악하고, 내가 지시하는 대로 답변형태를 맞춰서 해줘. 
query는 질문을 구체화 하는 거야, 그리고 만약 질문에 오타가 있다면 고쳐줘. 
keyword는 질문의 키워드를 뽑는거야. 
table은 질문에 대한 답을 할때 표형식 데이터가 필요한지 여부야, 현재는 매출액 관련 질문만 대응 가능하니 이때만 yes로 답해줘.
time은 질문에 답하기 위해 필요한 데이터의 날짜 범위야(오늘 날짜는 {TODAY.year}년 {TODAY.month}월 {TODAY.day}일). 
시간의 길이는 최소 3개월로 설정해야하고, 날짜는 1일로 설정해. (예시:2024년 10월에 대한 질문은 2024-08-01:2024-11-01) 
또한, '최근'이라는 말이 들어가면 2024-06-01:{TODAY.year}-{TODAY.month}-{TODAY.day}로 설정해줘.

내가 먼저 예시를 줄게

질문: 최근 일본발 베트남착 매출면에서 우리사에 기여도가 높은 화주(고객)은 어떻게 돼?
답변:
<query/>최근 일본발 베트남착 매출면에서 우리사에 기여도가 높은 화주(고객)은 어떻게 돼?<query>
<keyword/>일본발 베트남착 매출 기여도 화주 고객<keyword>
<table/>yes<table>
<time/>2024-08-01:2024-{TODAY.month}-{TODAY.day}<time>

질문: 올해 3월에 중국 시장 전망에 대해 조사했던 내용을 정리해줘
답변:
<query/>2024년 3월 중국시장 전망에 대한 조사내용을 알려주고 정리해줘<query>
<keyword/>2024년 3월 중국시장 전망<keyword>
<table/>no<table>
<time/>2024-02-01:2024-05-01<time>

질문: 부산발 인도네시아착 경쟁사 서비스 및 항차수를 알려줘
답변:
<query/>부산 출발 인도네시아 도착 경쟁사 서비스 및 항차수<query>
<keyword/>부산발 인도네시아착 경쟁사 서비스 항차수<keyword>
<table/>no<table>
<time/>all<time>

질문: 남성해운의 인도 대리점 선정 과정은 어떻게 돼?
답변:
<query/>인도 대리점 선정과정을 보기 좋게 정리해줘<query>
<keyword/>인도 대리점 선정과정<keyword>
<table/>no<table>
<time/>all<time>

### 아래 구분자를 추가하여 실제 사용자 질문을 명확히 구분합니다.
### 새로운 질문: {{user_query}}<end_of_turn>
<start_of_turn>model
답변:
"""

# Template for generating an answer based on internal documents
GENERATE_PROMPT_TEMPLATE = """
<bos><start_of_turn>user
너는 남성해운의 도움을 주는 데이터 분석가야.
주어진 내부 자료에 기반해서 내 질문에 대답해줘. 답변 형식은 보고서처럼 길고 자세하며 논리정연하게 사실만을 가지고 작성해줘.
만약 주어진 자료에 질문에 해당하는 내용이 없으면 "내부 자료에 해당 자료 없음"으로 답변해줘.
또한, 반드시 근거로 사용한 데이터의 출처를 명시해줘.
내부 자료가 표로 들어오면, 그 표를 최대한 말로 풀어서 해석해주고 논리적인 인사이트를 도출해줘.
내부 자료: {docs}
질문: {query}<end_of_turn>
<start_of_turn>model
답변:
"""

# Template for the streaming version of answer generation
STREAM_PROMPT_TEMPLATE = """
<bos><start_of_turn>user
... same instructions ...
내부 자료: {docs}
질문: {query}<end_of_turn>
<start_of_turn>model
답변:
"""

```


--- sql.py

```python

# sql.py
import json
import sqlite3
import re

def generate_sql(query, model, tokenizer, config):
    with open(config.metadata_path, 'r', encoding='utf-8') as file:
        Metadata = json.load(file)
    column_usage = Metadata['column_usage']

    # first_LLM
    outputs_1, filter_conditions, aggregations, orders, sql_query, parsed_columns = first_llm(model, tokenizer, column_usage, query, config)
    print(f'FirstLLM\n필터:{filter_conditions}\n집계:{aggregations}\n정렬:{orders}\nSQL:{sql_query}\n컬럼:{parsed_columns}')
    print(config.beep)

    relevant_metadata = extract_relevant_metadata(parsed_columns, column_usage) # 추출된 컬럼에 해당하는 메타데이터 가져오기
    retrival_metadata = parse_and_augment_filter_conditions(filter_conditions, Metadata)   # Metadata와 매핑하여 구체화된 필터 조건 찾기
    print(f'MetaData\n관련:{relevant_metadata}\n검색:{retrival_metadata}')
    print(config.beep)
    # second_LLM
    final_sql_query, title, explain, outputs_2 = second_llm(model, tokenizer, relevant_metadata, sql_query, query, retrival_metadata, parsed_columns, config)
    print(f'SecondLLM\n제목:{title}\n설명:{explain}\nSQL:{final_sql_query}')
    print(config.beep)
    columns, results = execute_sql_query(final_sql_query, config)   # SQL 쿼리 실행 (데이터 조회)
    print(f'Result\n컬럼:{columns}\n결과:{results}')
    print(config.beep)

    # result -> json
    table_json = create_table_json(columns, results)
    chart_json = create_chart_json(columns, results)
    
    # 결과 출력
    if results:
        print("조회된 컬럼:\n", columns)
        for row in results:
            print(row)
        return final_sql_query, title, explain, table_json, chart_json
    else:
        print("조회 결과가 없습니다.")
        return None
    
def first_llm(model, tokenizer, column_usage, user_query, config):
    PROMPT =\
    f'''
    <bos><start_of_turn>user
    너는 남성 해운 회사의 데이터로 SQL 쿼리를 작성하는 데 도움을 주는 시스템이야. 사용자로부터 받은 질문을 분석하여, 필터 조건, 집계 함수, 정렬 조건, SQL 쿼리 초안, SQL 쿼리에 사용된 모든 컬럼을 추출해줘.
    
    ### 참고 사항:
    1. 다음은 해운 회사 데이터의 메타데이터야. 테이블은 "revenue" 하나뿐이야.: 
    "{column_usage}"
    2. 사용자가 입력한 질문을 분석하여 필요한 컬럼을 식별하고, SQL 쿼리에서 사용할 필터 조건, 집계 함수, 정렬 기준을 제공해줘.
    3. 사용되는 프로그램은 SQLite 야. 이 프로그램에 맞는 언어를 사용해줘 (SQLite 날짜 형식 사용 예시 : strftime('%Y', OUTOBD) AS Year )
    
    ### 사용자가 입력한 질문:
    "{user_query}"
    
    ### 필요한 정보:
    1. 필터 조건 (필요한 경우, 예: <filter/>OUTPOL = '부산', OUTPOD = '일본', OUTBOR = '2024-08-01 이후'<filter/>)
    2. 집계 함수 (필요한 경우, 예: <aggregation/>화주(고객)별 매출액의 합계<aggregation/>)
    3. 정렬 조건 (필요한 경우, 예: <order/>매출액 기준 내림차순<order/>)
    4. SQL 쿼리 초안 (예: <sql_query/>SELECT OUTSHC,SUM(OUTSTL) AS TotalRevenue\n    FROM revenue\n    WHERE OUTPOL = \'한국\' AND OUTPOD = \'베트남\' AND OUTOBD >= \'2023-01-01\'    GROUP BY OUTSHC\n    ORDER BY TotalRevenue DESC;<sql_query/>)
    5. SQL 쿼리에 사용된 모든 컬럼 (예: <columns/>OUTPOL,OUTPOD,OUTBOR,OUTSHC,OUTSTL<columns/>)
    
    ### 출력 형식:
    1. 필터 조건: <filter/><filter/>
    2. 집계 함수:<aggregation/><aggregation/>
    3. 정렬 조건:<order/><order/>
    4. SQL 쿼리 초안 : <sql_query/><sql_query/>
    5. SQL 쿼리에 사용된 모든 컬럼:<columns/><columns/>
    6. 날짜는 YYYY-MM-DD 형식을 사용 (ex: "2023-05-01")
    
    <end_of_turn>
    <start_of_turn>model
    '''

    # Get Answer
    input_ids = tokenizer(PROMPT, return_tensors="pt").to("cuda")
    input_length = input_ids['input_ids'].shape[1]
    outputs = model.generate(**input_ids, max_new_tokens=config.model.max_new_tokens)
    outputs_result = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)

    # Regular expression to extract content between <query/> and <query>
    filter_pattern = r'<filter.*?>(.*?)<filter.*?>'
    aggregation_pattern = r'<aggregation.*?>(.*?)<aggregation.*?>'
    order_pattern = r'<order.*?>(.*?)<order.*?>'
    sql_pattern = r'<sql_query.*?>(.*?)<sql_query.*?>'
    columns_pattern = r'<columns.*?>(.*?)<columns.*?>'
    
    filter_conditions = re.search(filter_pattern, outputs_result, re.DOTALL).group(1)
    aggregations = re.search(aggregation_pattern, outputs_result, re.DOTALL).group(1)
    orders = re.search(order_pattern, outputs_result, re.DOTALL).group(1)
    sql_queries = re.search(sql_pattern, outputs_result, re.DOTALL).group(1)
    parsed_columns = [col.strip() for col in re.search(columns_pattern, outputs_result, re.DOTALL).group(1).split(",")]
    return outputs_result, filter_conditions, aggregations, orders, sql_queries, parsed_columns

# 추출된 컬럼에 해당하는 메타데이터만 가져오는 함수
def extract_relevant_metadata(columns, metadata):
    relevant_metadata = {}
    for column in columns:
        if column in metadata["column_usage"]:
            relevant_metadata[column] = metadata["column_usage"][column]
    return relevant_metadata

# def parse_and_augment_filter_conditions(filter_conditions, Metadata):
#     pattern = r"(\w+)\s*=\s*'([^']+)'"  # ex: OUTPOL = '부산' 과 같은 패턴 추출
#     matches = re.findall(pattern, filter_conditions)
    
#     augmented_filters = []
    
#     for col, val in matches:
#         if col == 'OUTPOL' or col == 'OUTPOD':
#             location_code = Metadata['location_code']
#             mapped_value = search_location_db(val, location_code)
#             if mapped_value != "UNKNOWN":
#                 augmented_filters.append(f"컬럼 {col}에 대한 값 '{val}' -> '{mapped_value}'로 매핑되었습니다.")
#             else:
#                 augmented_filters.append(f"컬럼 {col}의 값 '{val}'에 대한 매핑 정보를 찾을 수 없습니다.")
#     return "\n".join(augmented_filters)

def parse_and_augment_filter_conditions(filter_conditions, Metadata):
    # '컬럼명 = '값'' 또는 '컬럼명 IN ('값1', '값2', ...)' 패턴에 대응하는 정규식
    pattern = r"(\w+)\s*=\s*'([^']+)'|\b(\w+)\s+IN\s+\(([^)]+)\)"
    matches = re.findall(pattern, filter_conditions)
    
    augmented_filters = []
    for match in matches:
        # 매칭 결과에서 'IN' 조건과 '=' 조건을 구분하여 처리
        if match[0]:  # '=' 조건
            col, val = match[0], match[1]
            if col == 'OUTPOL' or col == 'OUTPOD':
                location_code = Metadata['location_code']
                mapped_value = search_location_db(val, location_code)
                if mapped_value != "UNKNOWN":
                    augmented_filters.append(f"컬럼 {col}에 대한 값 '{val}' -> '{mapped_value}'로 매핑되었습니다.")
                else:
                    augmented_filters.append(f"컬럼 {col}의 값 '{val}'에 대한 매핑 정보를 찾을 수 없습니다.")
                    
        elif match[2]:  # 'IN' 조건
            col, val_list = match[2], match[3]
            if col == 'OUTPOL' or col == 'OUTPOD':
                location_code = Metadata['location_code']
                values = [val.strip().strip("'") for val in val_list.split(",")]
                
                mapped_values = []
                for val in values:
                    mapped_value = search_location_db(val, location_code)
                    if mapped_value != "UNKNOWN":
                        mapped_values.append(f"'{val}' -> '{mapped_value}'")
                    else:
                        mapped_values.append(f"'{val}' (매핑 정보 없음)")
                
                augmented_filters.append(f"컬럼 {col}에 대한 값들: {', '.join(mapped_values)}")

    return "\n".join(augmented_filters)

# Location 검색 알고리즘 (매핑 정보 검색)
def search_location_db(location, location_code):
    return location_code.get(location, "Mapping error")

def second_llm(model, tokenizer, relevant_metadata, sql_query, user_query, retrival_metadata, parsed_columns, config):
    PROMPT =\
    f'''
    <bos><start_of_turn>user
    너는 남성 해운 회사의 데이터로 정확한 SQL 쿼리를 작성해주는 시스템이야. 너가 참고해야 할 정보가 있는 경우에는 이를 참고해서 SQL 쿼리 초안을 구체화해서 정확한 SQL 쿼리를 만들어줘. 그리고 이 SQL 쿼리가 어떤 정보를 추출해주는지 짧게 제목을 짓고, 어떻게 사용자의 질문에 답할 수 있는 정보를 추출하는지 설명해줘. 참고해야 할 정보가 없고 SQL 쿼리 초안이 이미 정확하다면, 그대로 출력해줘.

    ### 참고 사항:
    1. 다음은 너가 참고해야 할 정보야:
    "{retrival_metadata}"
    2. SQL 쿼리 초안:
    "{sql_query}"    
    3. 다음은 사용한 데이터의 메타데이터야:
    "{relevant_metadata}"
    4. 다음은 사용자가 입력한 질문이야:
    "{user_query}"    


    ### 필요한 정보:
    1. 정확한 SQL 쿼리 (예: <sql_query/>SELECT OUTSHC,SUM(OUTSTL) AS TotalRevenue\n    FROM revenue\n    WHERE WHERE OUTPOL = \'KRPUS\' AND OUTPOD LIKE \'CN%\' AND OUTOBD >= \'2023-01-01\'\n    GROUP BY OUTSHC\n    ORDER BY TotalRevenue DESC;<sql_query/>)
    2. SQL가 조회하는 데이터 요약 (예: 부산발 중국착 매출 순위 (화주별))
    3. SQL 쿼리 설명

    ### 출력 형식(아래 출력 형식을 꼭 지켜야 해 시작부분에 / 이 들어가고 끝부분에는 없어):
    1. 정확한 SQL 쿼리: <sql_query/>SQL 명령어<sql_query>
    2. SQL가 조회하는 데이터 요약: <title/>데이터 설명문<title>
    3. SQL 쿼리 설명: <explain/>SQL 설명문<explain>
    
    ### 참고자료
    1. 만약 참고자료에 KR% 같은 조건이 있으면 LIKE 를, KRCRD 같은 정확한 정보는 = 를 사용.
    2. 만약 여러개의 LIKE 조건이 있으면 (예시: WHERE OUTPOL LIKE 'KR%' OR OUTPOL LIKE 'CN%' OR OUTPOL LIKE 'JP%') 를 사용.
    3. 날짜는 YYYY-MM-DD 형식을 사용 (ex: "2023-05-01")
    4. LIKE 로 들어간 컬럼들은 다음과 같이 보기좋게 해줘.
    예시 : 
    SELECT 
        CASE 
            WHEN OUTPOL LIKE 'KR%' THEN '한국'
            WHEN OUTPOL LIKE 'JP%' THEN '일본'
            WHEN OUTPOL LIKE 'CN%' THEN '중국'
            ELSE '기타' 
        END AS 국가,
        SUM(OUTSTL) AS TotalRevenue

    <end_of_turn>
    <start_of_turn>model
    '''

    # Get Answer
    input_ids = tokenizer(PROMPT, return_tensors="pt").to("cuda")
    input_length = input_ids['input_ids'].shape[1]
    outputs = model.generate(**input_ids, max_new_tokens=config.model.max_new_tokens)
    outputs_result = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)
    print(f'2번째 LLM Output:{outputs_result}')
    sql_pattern = r'<sql_query.*?>(.*?)<sql_query.*?>'
    title_pattern = r'<title.*?>(.*?)<title.*?>'
    explain_pattern = r'<explain.*?>(.*?)<explain.*?>'
    
    sql_queries = re.search(sql_pattern, outputs_result, re.DOTALL).group(1)
    title = re.search(title_pattern, outputs_result, re.DOTALL).group(1)
    explain = re.search(explain_pattern, outputs_result, re.DOTALL).group(1)
    
    return sql_queries, title, explain, outputs_result

def execute_sql_query(sql_query, config):
    try:
        conn = sqlite3.connect(config.sql_data_path)        # SQLite 데이터베이스에 연결
        cursor = conn.cursor()

        if (config.k is not None) and ("LIMIT" not in sql_query):
            sql_query = sql_query.split(";")[0].strip()
            sql_query += f"\nLIMIT {config.k};"
                 
        cursor.execute(sql_query)        # SQL 쿼리 실행
        
        result = cursor.fetchall()        # 결과 가져오기
        column_names = [description[0] for description in cursor.description]        # 컬럼 이름도 포함하기 위해 description을 사용

        cursor.close()
        conn.close()
        
        return column_names, result

    except sqlite3.Error as e:
        print(f"데이터베이스 오류 발생: {e}")
        return None, None

def create_table_json(columns, results):
    head = "||".join(columns)
    body = "^ ".join("||".join(map(str, row)) for row in results)
    table = {"head": head, "body": body}
    return table

def create_chart_json(columns, results):
    chart = [
        {"label": f"{row[-2]}", "data": [{"x": "매출액", "y": str(row[-1])}]}
        for i, row in enumerate(results)
    ]
    return chart

```


--- tracking.py

```python

# tracking.py
import time
import logging

# Configure logging however you like
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

def time_tracker(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        logging.info(f"Entering {func.__name__}()")
        result = func(*args, **kwargs)
        end_time = time.time()
        elapsed = end_time - start_time
        logging.info(f"Exiting {func.__name__}() -- Elapsed: {elapsed:.2f}s")
        return result
    return wrapper

```


--- templates/chatroom.html

```python

<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8" />
  <title>FLUX_NS 채팅방 테스트 (SSE + Reference)</title>
  <style>
    /* 
      기본적인 리셋 및 바디 스타일 
      - 실제 프로덕션 환경에서는 리셋 CSS 또는 normalize.css 등을 활용 가능
    */
    * {
      box-sizing: border-box;
      margin: 0;
      padding: 0;
    }
    body {
      font-family: "Segoe UI", Tahoma, Geneva, Verdana, sans-serif;
      height: 100vh;
      display: flex;
      background-color: #f0f0f0;
    }

    /* 
      좌측 사이드바 스타일 
      - 채팅방 목록 및 새 채팅방 생성 버튼을 배치
    */
    .sidebar {
      width: 250px;
      background-color: #202123; /* ChatGPT 비슷한 어두운 톤 */
      color: #fff;
      display: flex;
      flex-direction: column;
      align-items: center;
      padding: 20px;
      overflow-y: auto;
    }
    .sidebar h2 {
      margin-bottom: 20px;
      font-size: 1.2rem;
    }
    .sidebar button {
      width: 100%;
      background-color: #3e3f4b;
      color: #fff;
      border: none;
      padding: 10px;
      margin-bottom: 15px;
      cursor: pointer;
      font-size: 0.9rem;
      border-radius: 5px;
    }
    .sidebar button:hover {
      background-color: #565869;
    }
    .chat-room-list {
      list-style: none;
      width: 100%;
    }
    .chat-room-list li {
      margin-bottom: 10px;
    }
    .chat-room-list button {
      width: 100%;
      background-color: #2b2c34;
      color: #fff;
      border: none;
      padding: 8px;
      cursor: pointer;
      font-size: 0.85rem;
      border-radius: 3px;
      text-align: left;
    }
    .chat-room-list button:hover {
      background-color: #444654;
    }

    /* 
      메인 컨테이너(채팅 영역) 스타일 
      - 상단 채팅방 정보, 채팅 내용, 입력 영역
    */
    .main-container {
      flex: 1;
      display: flex;
      flex-direction: column;
      background-color: #f8f8f8;
      overflow: hidden;
    }
    /* 채팅방 상단 영역 (예: 채팅방 이름/ID 표시) */
    .chat-header {
      background-color: #fff;
      padding: 10px 20px;
      border-bottom: 1px solid #ccc;
    }
    .chat-header h2 {
      font-size: 1rem;
      color: #444;
    }

    /* 채팅 메시지들이 표시되는 영역 */
    .chat-box {
      flex: 1;
      overflow-y: auto;
      padding: 20px;
      background-color: #f0f0f0;
    }
    .message-line {
      margin-bottom: 10px;
      line-height: 1.4;
    }
    .message-line strong {
      margin-right: 5px;
    }
    /* 사용자 메시지와 AI(시스템) 메시지 구분 위해 간단한 색상 차이 부여 */
    .message-user {
      color: #333;        /* 사용자: 일반 텍스트 */
    }
    .message-ai {
      color: #1d4ed8;     /* AI: 파란색 계열 */
    }
    .message-system {
      color: #dc2626;     /* 시스템: 붉은 계열 */
    }

    /* 입력 영역 스타일 */
    .input-container {
      background-color: #fff;
      border-top: 1px solid #ccc;
      padding: 10px;
      display: flex;
      gap: 10px;
    }
    .input-container input {
      flex: 1;
      padding: 10px;
      font-size: 1rem;
      border: 1px solid #ccc;
      border-radius: 5px;
    }
    .input-container button {
      padding: 10px 15px;
      font-size: 1rem;
      border: none;
      border-radius: 5px;
      cursor: pointer;
      background-color: #4c82ff;
      color: #fff;
    }
    .input-container button:hover {
      background-color: #365da8;
    }

    /* 참조 데이터 토글 영역 */
    #referenceContainer {
      display: none; /* 처음에는 감춰둠 */
      margin: 10px;
      background-color: #fff;
      padding: 10px;
      border: 1px solid #ccc;
      max-height: 250px;
      overflow-y: auto;
    }
    #toggleRefBtn {
      display: none; /* 참고 데이터가 있을 때만 보임 */
      margin: 0 20px 10px 20px;
      padding: 8px 12px;
      background-color: #eaeaea;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
  </style>
</head>

<body>
  <!-- 왼쪽 사이드바 -->
  <div class="sidebar">
    <h2>FLUX_NS</h2>
    <button onclick="createNewChatFromSidebar()">새 채팅방 생성</button>
    <ul id="chatRoomList" class="chat-room-list"></ul>
  </div>

  <!-- 메인 컨테이너 (채팅 영역) -->
  <div class="main-container">
    <!-- 채팅방 상단 정보 -->
    <div class="chat-header">
      <h2 id="chatRoomId">채팅방 ID: -</h2>
    </div>

    <!-- 토글 버튼 (참고 데이터 보기/숨기기)-->
    <button id="toggleRefBtn" onclick="toggleReferences()">Show References</button>

    <!-- 참조 데이터 표시 영역 -->
    <div id="referenceContainer">
      <!-- 여기에 참고 데이터를 표시 -->
    </div>

    <!-- 채팅 메시지가 표시되는 박스 -->
    <div class="chat-box" id="chatBox"></div>

    <!-- 입력 영역 -->
    <div class="input-container">
      <input type="text" id="userMessage" placeholder="메시지를 입력하세요..." />
      <button onclick="sendMessage()">전송</button>
    </div>
  </div>

  <script>
    /************************************************************************
     * 전역 변수 및 데이터 구조
     ************************************************************************/
    let currentRequestId = null;      // 현재 활성 채팅방의 requestId
    let chatRooms = [];              // [{ requestId, messages: [{sender, text}] }]
    let referenceList = [];          // 이번 쿼리에서 수집한 reference 데이터

    /************************************************************************
     * 페이지 로드 시 자동으로 채팅방 생성 (처음 접속하면 바로 시작하도록)
     ************************************************************************/
    window.addEventListener('load', () => {
      createNewChatRoom(); 
    });

    /************************************************************************
     * 사이드바 및 채팅방 생성
     ************************************************************************/
    function createNewChatRoom() {
      const newRequestId = Date.now().toString() + '-' + Math.floor(Math.random() * 10000);
      chatRooms.push({ requestId: newRequestId, messages: [] });
      setActiveChatRoom(newRequestId);
      renderChatRoomList();
    }

    function createNewChatFromSidebar() {
      createNewChatRoom();
    }

    function renderChatRoomList() {
      const ul = document.getElementById("chatRoomList");
      ul.innerHTML = "";
      chatRooms.forEach(room => {
        const li = document.createElement("li");
        const btn = document.createElement("button");
        btn.innerText = `ChatRoom: ${room.requestId}`;
        btn.onclick = () => setActiveChatRoom(room.requestId);
        li.appendChild(btn);
        ul.appendChild(li);
      });
    }

    function setActiveChatRoom(requestId) {
      currentRequestId = requestId;
      document.getElementById("chatRoomId").innerText = `채팅방 ID: ${requestId}`;

      // 채팅 박스 다시 그리기
      const chatBox = document.getElementById("chatBox");
      chatBox.innerHTML = "";
      const room = chatRooms.find(r => r.requestId === requestId);
      if (room) {
        room.messages.forEach(msg => {
          appendMessageToChatBox(msg.sender, msg.text);
        });
      }

      // 참조 영역은 새 채팅방 선택 시 초기화(접속 전방의 참조는 별개)
      clearReferenceData();
    }

    /************************************************************************
     * 채팅 메시지 표시 헬퍼
     ************************************************************************/
    function appendMessageToChatBox(sender, message) {
      const chatBox = document.getElementById("chatBox");
      const msgDiv = document.createElement("div");
      msgDiv.classList.add("message-line");

      let senderClass = "message-system";
      if (sender === "사용자") senderClass = "message-user";
      else if (sender === "AI") senderClass = "message-ai";

      msgDiv.innerHTML = `<strong class="${senderClass}">${sender}:</strong> ${message}`;
      chatBox.appendChild(msgDiv);
      chatBox.scrollTop = chatBox.scrollHeight;
    }

    // 메시지를 chatRooms에도 저장
    function storeMessageInRoom(requestId, sender, text) {
      const room = chatRooms.find(r => r.requestId === requestId);
      if (room) {
        room.messages.push({ sender, text });
      }
    }

    /************************************************************************
     * 스트리밍 (SSE) 메시지 전송 및 수신
     ************************************************************************/
    async function sendMessage() {
      const messageInput = document.getElementById("userMessage");
      const message = messageInput.value.trim();
      if (!message) return;

      // 사용자 메시지 표시
      appendMessageToChatBox("사용자", message);
      storeMessageInRoom(currentRequestId, "사용자", message);
      messageInput.value = "";

      // 이번 요청마다 참조 리스트 초기화
      clearReferenceData();

      // payload 준비
      const payload = { input: message };
      if (currentRequestId) {
        payload.request_id = currentRequestId;
      }

      try {
        const response = await fetch('/query_stream', {
          method: 'POST',
          headers: { 'Content-Type': 'application/json' },
          body: JSON.stringify(payload)
        });

        if (!response.ok) {
          throw new Error("응답 오류: " + response.status);
        }

        const reader = response.body.getReader();
        const decoder = new TextDecoder("utf-8");

        let sseBuffer = "";
        let partialAnswer = "";

        startAIStreamingMessage();  // AI가 답변 중임을 표시

        while (true) {
          const { value, done } = await reader.read();
          if (done) break;

          const chunk = decoder.decode(value, { stream: true });
          sseBuffer += chunk;

          const lines = sseBuffer.split("\n");
          sseBuffer = lines.pop(); // 남은 라인은 덜 끊긴 조각일 수 있음

          for (let line of lines) {
            line = line.trim();
            if (!line) continue;

            // SSE는 보통 "data: ..." 형태
            if (line.startsWith("data:")) {
              const jsonStr = line.slice(5).trim();
              if (jsonStr === "[[STREAM_DONE]]") {
                finalizeAIMessage(partialAnswer);
                return;
              }
              try {
                const sseData = JSON.parse(jsonStr);
                if (sseData.type === "answer") {
                  partialAnswer += sseData.answer;
                  updateAIStreamingMessage(partialAnswer);
                }
                else if (sseData.type === "reference") {
                  // 참조 데이터 수집
                  addReferenceData(sseData);
                }
                else {
                  // 그 외 타입
                  console.log("기타 SSE 데이터:", sseData);
                }
              } catch (err) {
                // JSON 파싱 실패 -> 그냥 텍스트로 처리
                partialAnswer += jsonStr;
                updateAIStreamingMessage(partialAnswer);
              }
            }
          }
        }
        // while 루프 끝 (done=true)
        if (sseBuffer.trim()) {
          // 남은 sseBuffer 처리할 로직 (필요시 구현)
        }
        finalizeAIMessage(partialAnswer);
      } catch (err) {
        appendMessageToChatBox("시스템", "오류 발생: " + err.message);
        storeMessageInRoom(currentRequestId, "시스템", "오류 발생: " + err.message);
      }
    }

    /************************************************************************
     * AI 스트리밍 표시 (임시 메시지) 
     ************************************************************************/
    let tempAiMsgDiv = null;

    function startAIStreamingMessage() {
      const chatBox = document.getElementById("chatBox");
      tempAiMsgDiv = document.createElement("div");
      tempAiMsgDiv.classList.add("message-line");
      tempAiMsgDiv.innerHTML = `<strong class="message-ai">AI:</strong> <span id="aiTempText"></span>`;
      chatBox.appendChild(tempAiMsgDiv);
      chatBox.scrollTop = chatBox.scrollHeight;
    }

    function updateAIStreamingMessage(text) {
      if (!tempAiMsgDiv) return;
      const span = tempAiMsgDiv.querySelector("#aiTempText");
      if (span) {
        span.textContent = text;
      }
      const chatBox = document.getElementById("chatBox");
      chatBox.scrollTop = chatBox.scrollHeight;
    }

    function finalizeAIMessage(finalText) {
      // 최종 AI 메시지를 chatRooms에 저장
      storeMessageInRoom(currentRequestId, "AI", finalText);
      tempAiMsgDiv = null;

      // 참조 데이터가 하나라도 있으면 Show 버튼 표시
      if (referenceList.length > 0) {
        document.getElementById("toggleRefBtn").style.display = "block";
      }
    }

    /************************************************************************
     * 참조 데이터(Reference) 관련
     ************************************************************************/
    function addReferenceData(refData) {
      // 여기선 단순히 배열에 push
      referenceList.push(refData);
      // 콘솔 찍어볼 수도 있음
      console.log("Reference data received:", refData);
    }

    function clearReferenceData() {
      referenceList = [];
      document.getElementById("toggleRefBtn").style.display = "none";
      document.getElementById("referenceContainer").style.display = "none";
      document.getElementById("referenceContainer").innerHTML = "";
    }

    // 토글 버튼 클릭 시, 참조 정보를 표시하거나 숨기는 함수
    function toggleReferences() {
      const container = document.getElementById("referenceContainer");
      if (container.style.display === "none" || container.style.display === "") {
        // 보이게
        container.style.display = "block";
        // 이미 렌더링되어 있다면 스킵, 없으면 렌더링
        if (!container.innerHTML.trim()) {
          renderReferenceData();
        }
        document.getElementById("toggleRefBtn").textContent = "Hide References";
      } else {
        // 숨기기
        container.style.display = "none";
        document.getElementById("toggleRefBtn").textContent = "Show References";
      }
    }

    // 참조 데이터를 화면에 렌더링
    function renderReferenceData() {
      const container = document.getElementById("referenceContainer");
      container.innerHTML = ""; // 초기화

      referenceList.forEach((refObj, idx) => {
        // 간단히 JSON.stringify 등으로 출력
        // 실제로는 refObj.data_list 등 더 구체적으로 표시 가능
        const div = document.createElement("div");
        div.style.marginBottom = "10px";
        div.innerHTML = `<strong>Reference #${idx + 1}</strong><br>
                         <pre style="white-space: pre-wrap;">${JSON.stringify(refObj, null, 2)}</pre>`;
        container.appendChild(div);
      });
    }
  </script>
</body>
</html>

```


-----------------

# Requirements


Base-Knowledge:
 - 위 파일들은 Gemma2 모델을 활용한 RAG 서비스의 소스 코드입니다.
 - 파일 트리와 각 파일의 내용이 ```python``` 코드 블록 내에 포함되며, 프로젝트의 현재 구조와 상태를 한눈에 파악할 수 있습니다.
 - vLLM과 ray를 활용하여 사용성 및 추론 성능을 개선하였습니다.
 - 에러 발생 시 로깅을 통해 문제를 추적할 수 있도록 설계되었습니다.

My-Requirements:
 1. User requirements.
 2. My requirements.
 3. 추후 소스 코드 개선, 구조 변경, 에러 로그 추가 등 다양한 요구사항을 반영할 수 있는 확장성을 고려합니다.
 4. 전체 코드는 한국어로 주석 및 설명이 포함되어, 이해와 유지보수가 용이하도록 작성됩니다.
