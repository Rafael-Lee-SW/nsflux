# Project Tree of RAG for company

```
├─ .dockerignore
├─ Dockerfile
├─ RAG.py
├─ app.py
├─ config.yaml
├─ data_control.py
├─ prompt_rag.py
├─ ray_setup.py
├─ ray_utils.py
├─ requirements.txt
├─ sql.py
├─ templates/data_manager.html
├─ tracking.py
└─ utils.py
```

--- .dockerignore

```

huggingface
YeoJun

# Ignore git files and folders
.git
.gitignore

# Ignore Python cache files
__pycache__
*.pyc
*.pyo
*.pyd

# Ignore virtual environments or local build folders
venv
env
*.env
.env.*
build/
dist/

# Ignore logs and temporary files
*.log
*.tmp

# If you have any large data or model files that are not needed in build context, ignore them
data/
models/
logs/

```


--- config.yaml

```yaml

# config.yaml
# Server : 2x H100 (80 GB SXM5), 52 CPU cores, 483.2 GB RAM, 6 TB SSD
### Model
model_id : 'google/gemma-2-27b-it'

ray:
  actor_count: 1                  # 총 Actor 개수(same as num_replicas)
  num_gpus: 1                     # 각 Actor(Node)가 점유하고 있는 GPU 갯수
  num_cpus: 24                    # 각 Actor(Node)가 점유하고 있는 CPU 갯수 (1 actor 시에 gpu 48개, 2 actor 시에 gpu 24개 할당)
  max_batch_size: 10              # max_concurrency(actor 최대 동시 처리량, default 1000)로 대체해도 됨
  batch_wait_timeout: 0.05        
  max_ongoing_requests: 100        # ray.serve에서 deployment setting으로 동시 요청 처리 갯수를 의미함(Batch랑 다름)

use_vllm: True # vLLM 사용 여부
vllm:
  enable_prefix_caching: True
  scheduler_delay_factor: 0.1
  enable_chunked_prefill: True
  tensor_parallel_size: 1         # vLLM의 GPU 사용 갯수 (!!!! num_gpus 보다 작아야 함 !!!!)
  max_num_seqs: 192               # v1에 따른 상향
  max_num_batched_tokens: 24576   # v1에 따른 상향
  block_size: 128                 # 미적용
  gpu_memory_utilization: 0.99    # v0: 0.95 / v1: 0.99로 상향
  disable_custom_all_reduce: true
  enable_memory_defrag: True      # v1 신규 기능 활성화

model:
  quantization_4bit : False # Quantize 4-bit
  quantization_8bit : False # Quantize 8-bit
  max_new_tokens : 2048      # 생성할 최대 토큰 수

  do_sample : True # True 일때만 아래가 적용
  temperature : 1.0          # 텍스트 다양성 조정: 높을수록 창의력 향상 (1.0)
  top_k : 30                 # top-k 샘플링: 상위 k개의 후보 토큰 중 하나를 선택 (50)
  top_p : 1.0                # top-p 샘플링: 누적 확률을 기준으로 후보 토큰을 선택 (1.0 보다 낮을수록 창의력 증가)
  repetition_penalty : 1.0   # 같은 단어를 반복해서 출력하지 않도록 패널티를 부여 (1.0 보다 클수록 페널티 증가)
embed_model_id : 'BM-K/KoSimCSE-roberta-multitask'
# cache_dir : "D:/huggingface" # Windows Local
# cache_dir : "/media/user/7340afbb-e4ce-4a38-8210-c6362e85eae7/RAG/RAG_application/huggingface" # Local
cache_dir : "/workspace/huggingface"  # Docker

### Data
data_path : 'data/1104_NS_DB_old.json' # VectorDB Path
metadata_path : 'data/Metadata.json' # Metadata.json Path
sql_data_path : 'data/poc.db'        # SQLite 데이터베이스 Path

### Retrieve
N : 5 # Retrieve top N chunks

### Others
beep : '-------------------------------------------------------------------------------------------------------------------------------------------------------------------------'
seed : 4734                     # Radom Seed
k : 15                        # SQL Max Rows (None=MAX)

```


--- Dockerfile

```dockerfile

# 베이스 이미지 선택
FROM globeai/flux_ns:1.24

# 작업 디렉토리 설정
WORKDIR /workspace

# requirements.txt만 먼저 복사해서 종속성 설치 (캐시 활용)
COPY requirements.txt .

# pip 캐시 사용 안 함으로 설치 (임시 파일 최소화)
RUN pip install --no-cache-dir -r requirements.txt

# Solve the C compier
RUN apt-get update && apt-get install build-essential -y

# 현재 디렉토리의 모든 파일을 컨테이너의 /app 폴더로 복사
COPY . /workspace

# Flask 앱이 실행될 포트를 열어둠
EXPOSE 5000

# Ray Dashboard 포트 (8265)와 vLLM 관련 포트 필요 시 추가
EXPOSE 8265
# Expose port for the vLLM
EXPOSE 8000

# Flask 앱 실행 명령어
CMD ["python", "app.py"]

```


--- requirements.txt

```txt

accelerate==1.3.0
aiohappyeyeballs==2.4.4
aiohttp==3.11.12
aiohttp-cors==0.7.0
aiosignal==1.3.2
annotated-types==0.7.0
attrs==25.1.0
bitsandbytes==0.45.1
blinker==1.9.0
cachetools==5.5.1
certifi==2025.1.31
charset-normalizer==3.4.1
click==8.1.8
colorama==0.4.6
colorful==0.5.6
distlib==0.3.9
filelock==3.17.0
Flask==3.1.0
Flask[async]==3.1.0
fastapi[standard]==0.112.0
Jinja2==3.1.5
uvicorn==0.34.0
gunicorn==23.0.0
frozenlist==1.5.0
fsspec==2025.2.0
google-api-core==2.24.1
google-auth==2.38.0
googleapis-common-protos==1.66.0
grpcio==1.70.0
huggingface-hub==0.28.1
idna==3.10
itsdangerous==2.2.0
Jinja2==3.1.5
jsonschema==4.23.0
jsonschema-specifications==2024.10.1
langchain==0.3.19
MarkupSafe==3.0.2
mpmath==1.3.0
msgpack==1.1.0
multidict==6.1.0
networkx==3.4.2
numpy==1.26.4
opencensus==0.11.4
opencensus-context==0.1.3
optree==0.14.0
packaging==24.2
pillow==11.1.0
platformdirs==4.3.6
prometheus_client==0.21.1
propcache==0.2.1
proto-plus==1.26.0
protobuf==5.29.3
psutil==6.1.1
py-spy==0.4.0
pyasn1==0.6.1
pyasn1_modules==0.4.1
pydantic==2.10.6
pydantic_core==2.27.2
python-box==7.3.2
python-dotenv==1.0.1
PyYAML==6.0.2
rank-bm25==0.2.2
ray[serve]==2.42.0
referencing==0.36.2
regex==2024.11.6
requests==2.32.3
rpds-py==0.22.3
rsa==4.9
safetensors==0.5.2
setuptools==75.8.0
six==1.17.0
smart-open==7.1.0
sympy==1.13.1
the==0.1.5
tokenizers==0.21.0
torch==2.5.1
torchvision==0.20.1
tqdm==4.67.1
transformers==4.48.2
typing_extensions==4.12.2
urllib3==2.3.0
virtualenv==20.29.1
vllm==0.7.2
Werkzeug==3.1.3
wrapt==1.17.2
yarl==1.18.3
python-pptx==1.0.2
pandas==2.2.3
plotly==6.0.0
pypdf2==3.0.1
umap-learn==0.5.3

```


--- app.py

```python

# app.py
import os
# Setting environment variable
# os.environ["TRANSFORMERS_CACHE"] = "/workspace/huggingface"
os.environ["HF_HOME"] = "/workspace/huggingface"
# os.environ["PYTORCH_CUDA_ALLOC_CONF"] = "expandable_segments:True"
# For the Huggingface Token setting
os.environ["HF_TOKEN_PATH"] = "/root/.cache/huggingface/token"
# Change to GNU to using OpenMP. Because this is more friendly with CUDA(NVIDIA),
# and Some library(Pytorch, Numpy, vLLM etc) use the OpenMP so that set the GNU is better.
# OpenMP: Open-Multi-Processing API
os.environ["MKL_THREADING_LAYER"] = "GNU"
# Increase download timeout (in seconds)
os.environ["HF_HUB_DOWNLOAD_TIMEOUT"] = "60"
# Use the vLLM as v1 version
os.environ["VLLM_USE_V1"] = "1"
os.environ["VLLM_STANDBY_MEM"] = "0"
os.environ["VLLM_METRICS_LEVEL"] = "1"
os.environ["VLLM_PROFILE_MEMORY"]= "1"
# GPU 단독 사용(박상제 연구원님이랑 분기점)
os.environ["CUDA_VISIBLE_DEVICES"] = "1"  # GPU1 사용

from flask import (
    Flask,
    request,
    Response,
    render_template,
    jsonify,
    g,
    stream_with_context,
)
import json
import yaml
from box import Box
from utils import random_seed, error_format
from datetime import datetime

# Import the Ray modules
from ray_setup import init_ray
from ray import serve
from ray_utils import InferenceActor
from ray_utils import InferenceService, SSEQueueManager

# ------ checking process of the thread level
import logging

# 로깅 설정: 요청 처리 시간과 현재 스레드 이름을 기록
logging.basicConfig(
    level=logging.DEBUG,
    format='%(asctime)s %(levelname)s [%(threadName)s] %(message)s'
)

import ray
import uuid
import asyncio

# Configuration
with open("./config.yaml", "r") as f:
    config_yaml = yaml.load(f, Loader=yaml.FullLoader)
    config = Box(config_yaml)
random_seed(config.seed)

########## Ray Dashboard 8265 port ##########
init_ray()  # Initialize the Ray
sse_manager = SSEQueueManager.options(name="SSEQueueManager").remote()
serve.start(detached=True)

#### Ray-Actor 다중 ####
inference_service = InferenceService.options(num_replicas=config.ray.actor_count).bind(config)
serve.run(inference_service)
inference_handle = serve.get_deployment_handle("inference", app_name="default")

#### Ray-Actor 단독 ####
# inference_actor = InferenceActor.options(num_cpus=config.ray.num_cpus, num_gpus=config.ray.num_gpus).remote(config)

########## FLASK APP setting ##########
app = Flask(__name__)
content_type = "application/json; charset=utf-8"


# 기본 페이지를 불러오는 라우트
@app.route("/")
def index():
    return render_template("index.html")  # index.html을 렌더링

# Test 페이지를 불러오는 라우트
@app.route("/test")
def test_page():
    return render_template("index_test.html")

# chatroomPage 페이지를 불러오는 라우트
@app.route("/chat")
def chat_page():
    return render_template("chatroom.html")

# data 관리
from data_control import data_control_bp
app.register_blueprint(data_control_bp, url_prefix="/data")

# Query Endpoint (Non-streaming)
@app.route("/query", methods=["POST"])
async def query():
    try:
        
        # Log when the query is received
        receive_time = datetime.now().isoformat()
        print(f"[APP] Received /query request at {receive_time}")
        
        # Optionally, attach the client time if desired:
        http_query = request.json  # 클라이언트로부터 JSON 요청 수신
        
        http_query["server_receive_time"] = receive_time
        
        # Ray Serve 배포된 서비스를 통해 추론 요청 (자동으로 로드밸런싱됨)
        # result = await inference_actor.process_query.remote(http_query) # 단일
        result = await inference_handle.query.remote(http_query) # 다중
        if isinstance(result, dict):
            result = json.dumps(result, ensure_ascii=False)
        # print("APP.py - 결과: ", result)
        return Response(result, content_type=content_type)
    except Exception as e:
        error_resp = error_format(f"서버 처리 중 오류 발생: {str(e)}", 500)
        return Response(error_resp, content_type=content_type)

# --------------------- Streaming part ----------------------------

# Streaming Endpoint (POST 방식 SSE) → 동기식 뷰 함수로 변경
@app.route("/query_stream", methods=["POST"])
def query_stream():
    """
    POST 방식 SSE 스트리밍 엔드포인트.
    클라이언트가 {"input": "..."} 형태의 JSON을 보내면, SSE 스타일의 청크를 반환합니다.
    """
    body = request.json or {}
    user_input = body.get("input", "")
    # request_id 파트 추가
    client_request_id = body.get("request_id")
    print(f"[DEBUG] /query_stream (POST) called with user_input='{user_input}', request_id='{client_request_id}'")
    
    http_query = {"qry_contents": user_input}
    # request_id 파트 추가
    if client_request_id:
        http_query["request_id"] = client_request_id
    print(f"[DEBUG] Built http_query={http_query}")

    # Obtain request_id from Ray
    # request_id = ray.get(inference_actor.process_query_stream.remote(http_query)) # 단일
    # ----------------------------------------------------------------------------- 다중
    response = inference_handle.process_query_stream.remote(http_query)
    obj_ref = response._to_object_ref_sync()
    request_id = ray.get(obj_ref)
    # ----------------------------------------------------------------------------- 다중
    print(f"[DEBUG] streaming request_id={request_id}")

    # def sse_generator():
    #     print("[DEBUG] sse_generator started: begin pulling partial tokens in a loop")
    #     while True:
    #         partial_text = ray.get(inference_actor.pop_sse_token.remote(request_id)) # 단일
    #         if partial_text is None:
    #             print("[DEBUG] partial_text is None => no more data => break SSE loop")
    #             break
    #         if partial_text == "[[STREAM_DONE]]":
    #             print("[DEBUG] got [[STREAM_DONE]], ending SSE loop")
    #             break
    #         yield f"data: {partial_text}\n\n"
    #     # close_sse_queue 호출
    #     ray.get(inference_actor.close_sse_queue.remote(request_id)) # 단일
    #     print("[DEBUG] SSE closed.")
    
    def sse_generator():
        try:
            while True:
                # Retrieve token from SSEQueueManager
                token = ray.get(sse_manager.get_token.remote(request_id, 120))
                if token is None or token == "[[STREAM_DONE]]":
                    break
                yield f"data: {token}\n\n"
        except Exception as e:
            error_token = json.dumps({"type": "error", "message": str(e)})
            yield f"data: {error_token}\n\n"
        finally:
            # Cleanup: close the SSE queue after streaming is done
            try:
                obj_ref = inference_handle.close_sse_queue.remote(request_id)._to_object_ref_sync()
                ray.get(obj_ref)
            except Exception as ex:
                print(f"[DEBUG] Error closing SSE queue for {request_id}: {str(ex)}")
            print("[DEBUG] SSE closed.")

    return Response(sse_generator(), mimetype="text/event-stream")
# --------------------- Streaming part ----------------------------

# Flask app 실행
if __name__ == "__main__":
    app.run(host="0.0.0.0", port=5000, debug=False)

```


--- ray_setup.py

```python

# ray_setup.py
import ray
from ray import serve

########## Starting Banner ############
from colorama import init, Fore, Style
init(autoreset=True)

BANNER = Fore.GREEN + r"""
'########:'##::::'##:'##:::::::'##::::'##::::::::::'##::: ##::'######::
 ##.....:: ##:::: ##: ##:::::::. ##::'##::::::::::: ###:: ##:'##... ##:
 ##::::::: ##:::: ##: ##::::::::. ##'##:::::::::::: ####: ##: ##:::..::
 ######::: ##:::: ##: ##:::::::::. ###::::::::::::: ## ## ##:. ######::
 ##...:::: ##:::: ##: ##::::::::: ## ##:::::::::::: ##. ####::..... ##:
 ##::::::: ##:::: ##: ##:::::::: ##:. ##::::::::::: ##:. ###:'##::: ##:
 ##:::::::. #######:: ########: ##:::. ##:'#######: ##::. ##:. ######::
..:::::::::.......:::........::..:::::..::.......::..::::..:::......:::
"""

def init_ray():
    print(BANNER)
    # Ray-Dashboard - GPU 상태, 사용 통계 등을 제공하는 모니터링 툴, host 0.0.0.0로 외부 접속을 허용하고, Default 포트인 8265으로 설정
    ray.init(
        include_dashboard=True,
        dashboard_host="0.0.0.0" # External IP accessable
        # dashboard_port=8265
    )
    print("Ray initialized. DashBoard running at http://192.222.54.254:8265") # New Server(2xH100)

```


--- ray_utils.py

```python

# ray_utils.py
import ray  # Ray library
from ray import serve
import json
import asyncio  # async I/O process module
from concurrent.futures import ProcessPoolExecutor  # 스레드 컨트롤
import uuid  # --- NEW OR MODIFIED ---
import time
from typing import Dict, Optional  # --- NEW OR MODIFIED ---
import threading  # To find out the usage of thread
import datetime

from RAG import (
    query_sort,
    execute_rag,
    generate_answer,
    generate_answer_stream,
)  # hypothetically
from utils import (
    load_model,
    load_data,
    process_format_to_response,
    process_to_format,
    error_format,
)

# 랭체인 도입
from langchain.memory import ConversationBufferMemory

@ray.remote  # From Decorator, Each Actor is allocated 1 GPU
class InferenceActor:
    async def __init__(self, config):
        self.config = config
        # 액터 내부에서 모델 및 토크나이저를 새로 로드 (GPU에 한 번만 로드)
        self.model, self.tokenizer, self.embed_model, self.embed_tokenizer = load_model(
            config
        )
        # 데이터는 캐시 파일을 통해 로드
        self.data = load_data(config.data_path)
        # 비동기 큐와 배치 처리 설정 (마이크로배칭)
        self.request_queue = asyncio.Queue()
        self.max_batch_size = config.ray.max_batch_size  # 최대 배치 수
        self.batch_wait_timeout = config.ray.batch_wait_timeout  # 배치당 처리 시간

        # Actor 내부에서 ProcessPoolExecutor 생성 (직렬화 문제 회피)
        max_workers = int(min(config.ray.num_cpus * 0.8, (26*config.ray.actor_count)-4))
        self.process_pool = ProcessPoolExecutor(max_workers)

        self.queue_manager = ray.get_actor("SSEQueueManager")
        # --- NEW OR MODIFIED ---
        # A dictionary to store SSE queues for streaming requests
        # Key = request_id, Value = an asyncio.Queue of partial token strings
        self.active_sse_queues: Dict[str, asyncio.Queue] = {}

        self.batch_counter = 0  # New counter to track batches

        # Micro-batching만 적용
        # asyncio.create_task(self._batch_processor())

        # ---------------------------
        # LangChain Memory 맵 (랭체인)
        # key: request_id, value: ConversationBufferMemory()
        # ---------------------------
        self.memory_map = {}

        # In-flight batching까지 추가 적용
        asyncio.create_task(self._in_flight_batch_processor())


    def get_memory_for_session(self, request_id: str) -> ConversationBufferMemory:
        """
        세션별 Memory를 안전하게 가져오는 헬퍼 메서드.
        만약 memory_map에 request_id가 없으면 새로 생성해 저장 후 반환.
        """
        if request_id not in self.memory_map:
            print(f"[DEBUG] Creating new ConversationBufferMemory for session={request_id}")
            self.memory_map[request_id] = ConversationBufferMemory(return_messages=True)
        return self.memory_map[request_id]


    # --------------------------------------------------------
    # EXISTING METHODS FOR NORMAL QUERIES (unchanged)
    # --------------------------------------------------------

    async def process_query(self, http_query):
        """
        Existing synchronous method. Returns final string/dict once done.
        """
        loop = asyncio.get_event_loop()
        future = loop.create_future()
        # There's no SSE queue for normal queries
        sse_queue = None
        await self.request_queue.put((http_query, future, sse_queue))
        # print("self.request_queue : ", self.request_queue)
        return await future

    # -------------------------------------------------------------------------
    # Micro_batch_processor
    # -------------------------------------------------------------------------

    async def _batch_processor(self):
        """
        Continuously processes queued requests in batches (micro-batching).
        We add new logic for streaming partial tokens if a request has an SSE queue.
        """
        while True:
            batch = []
            batch_start_time = time.time()
            # 1) get first request from the queue
            print("=== _batch_processor waiting for request_queue item... ===")
            item = await self.request_queue.get()
            print(
                f"[DEBUG] 첫 요청 도착: {time.strftime('%H:%M:%S')} (현재 배치 크기: 1)"
            )
            batch.append(item)

            print(f"[DEBUG] Received first request at {time.strftime('%H:%M:%S')}")

            # 2) try to fill the batch up to batch_size or until timeout
            try:
                while len(batch) < self.max_batch_size:
                    print("현재 배치 사이즈 : ", len(batch))
                    print("최대 배치 사이즈 : ", self.max_batch_size)
                    item = await asyncio.wait_for(
                        self.request_queue.get(), timeout=self.batch_wait_timeout
                    )

                    batch.append(item)
                    print(
                        f"[DEBUG] 추가 요청 도착: {time.strftime('%H:%M:%S')} (현재 배치 크기: {len(batch)})"
                    )
            except asyncio.TimeoutError:
                elapsed = time.time() - batch_start_time
                print(
                    f"[DEBUG] 타임아웃 도달: {elapsed:.2f}초 후 (최종 배치 크기: {len(batch)})"
                )
                pass

            print(
                f"=== _batch_processor: 배치 사이즈 {len(batch)} 처리 시작 ({time.strftime('%H:%M:%S')}) ==="
            )

            # 각 요청 처리 전후에 로그 추가
            start_proc = time.time()
            await asyncio.gather(
                *(
                    self._process_single_query(req, fut, sse_queue)
                    for (req, fut, sse_queue) in batch
                )
            )
            proc_time = time.time() - start_proc
            print(f"[DEBUG] 해당 배치 처리 완료 (처리시간: {proc_time:.2f}초)")

    # -------------------------------------------------------------------------
    # In-flight BATCH PROCESSOR
    # -------------------------------------------------------------------------

    async def _in_flight_batch_processor(self):
        while True:
            # Wait for the first item (blocking until at least one is available)
            print(
                "=== [In-Flight Batching] Waiting for first item in request_queue... ==="
            )
            first_item = await self.request_queue.get()
            batch = [first_item]
            batch_start_time = time.time()

            print(
                "[In-Flight Batching] Got the first request. Attempting to fill a batch..."
            )

            # Attempt to fill up the batch until we hit max_batch_size or batch_wait_timeout
            while len(batch) < self.max_batch_size:
                try:
                    remain_time = self.batch_wait_timeout - (
                        time.time() - batch_start_time
                    )
                    if remain_time <= 0:
                        print(
                            "[In-Flight Batching] Timed out waiting for more requests; proceeding with current batch."
                        )
                        break
                    item = await asyncio.wait_for(
                        self.request_queue.get(), timeout=remain_time
                    )
                    batch.append(item)
                    print(
                        f"[In-Flight Batching] +1 request => batch size now {len(batch)} <<< {self.max_batch_size}"
                    )
                except asyncio.TimeoutError:
                    print(
                        "[In-Flight Batching] Timeout reached => proceeding with the batch."
                    )
                    break
            self.batch_counter += 1
            print(
                f"[BATCH {self.batch_counter}] In-Flight batch collected with {len(batch)} requests"
            )

            # We have a batch of items: each item is ( http_query_or_stream_dict, future, sse_queue )
            # We'll process them concurrently.
            tasks = []
            for request_tuple in batch:
                request_obj, fut, sse_queue = request_tuple
                tasks.append(self._process_single_query(request_obj, fut, sse_queue))

            # Actually run them all concurrently
            await asyncio.gather(*tasks)

    async def _process_single_query(self, http_query_or_stream_dict, future, sse_queue):
        """
        Process a single query from the micro-batch. If 'sse_queue' is given,
        we do partial-token streaming. Otherwise, normal final result.
        """
        print(
            f"[DEBUG] _process_single_query 시작: {time.strftime('%H:%M:%S')}, 요청 내용: {http_query_or_stream_dict}, 현재 스레드: {threading.current_thread().name}"
        )
        try:
            # 1) request_id 구분
            if (
                isinstance(http_query_or_stream_dict, dict)
                and "request_id" in http_query_or_stream_dict
            ):
                # It's a streaming request
                request_id = http_query_or_stream_dict["request_id"]
                http_query = http_query_or_stream_dict["http_query"]
                is_streaming = True
                print(f"[STREAM] _process_single_query: request_id={request_id}")
            else:
                # It's a normal synchronous request
                request_id = None
                http_query = http_query_or_stream_dict
                is_streaming = False
                print("[SYNC] _process_single_query started...")
                
            # 2) Memory 객체 가져오기 (없으면 새로 생성)
            if request_id not in self.memory_map:
                self.memory_map[request_id] = ConversationBufferMemory(return_messages=True)
            
            memory = self.memory_map[request_id]

            # 3) 유저가 현재 입력한 쿼리 가져오기
            user_input = http_query.get("qry_contents", "")
            
            # 4) LangChain Memory에서 이전 대화 이력(history) 추출
            past_context = memory.load_memory_variables({})["history"]
            
            # # To Calculate the token
            # tokens = self.tokenizer(user_input, add_special_tokens=True)["input_ids"]
            # print(f"[DEBUG] Processing query: '{user_input}' with {len(tokens)} tokens")

            # 5) 필요하다면 데이터를 다시 로드(1.16version 유지)
            self.data = load_data(
                self.config.data_path
            )  # if you want always-latest, else skip

            # 6) 현재 사용중인 Thread 확인
            print("   ... calling query_sort() ...")
            # print(
            #     f"[DEBUG] query_sort 시작 (offload) - 스레드: {threading.current_thread().name}"
            # )
            # 7) “대화 이력 + 현재 사용자 질문”을 Prompt에 합쳐서 RAG 수행
            #    방법 1) query_sort() 전에 past_context를 참조해 query를 확장
            #    방법 2) generate_answer()에서 Prompt 앞부분에 붙임
            # 여기서는 예시로 “query_sort”에 past_context를 넘겨
            # 호출부 수정
            params = {
                "user_input": f"{past_context}\n사용자 질문: {user_input}",
                "model": self.model,
                "tokenizer": self.tokenizer,
                "embed_model": self.embed_model,
                "embed_tokenizer": self.embed_tokenizer,
                "data": self.data,
                "config": self.config,
            }
            QU, KE, TA, TI = await query_sort(params)
            print(f"   ... query_sort => QU={QU}, KE={KE}, TA={TA}, TI={TI}")

            # 4) RAG
            if TA == "yes":
                try:
                    docs, docs_list = execute_rag(
                        QU,
                        KE,
                        TA,
                        TI,
                        model=self.model,
                        tokenizer=self.tokenizer,
                        embed_model=self.embed_model,
                        embed_tokenizer=self.embed_tokenizer,
                        data=self.data,
                        config=self.config,
                    )
                    try:
                        retrieval, chart = process_to_format(docs_list, type="SQL")
                    except Exception as e:
                        print("[ERROR] process_to_format (SQL) failed:", str(e))
                        retrieval, chart = [], None

                    # If streaming => partial tokens
                    if is_streaming:
                        print(
                            f"[STREAM] Starting partial generation for request_id={request_id}"
                        )
                        await self._stream_partial_answer(
                            QU, docs, retrieval, chart, request_id, future, user_input
                        )
                    else:
                        # normal final result
                        output = await generate_answer(
                            QU,
                            docs,
                            model=self.model,
                            tokenizer=self.tokenizer,
                            config=self.config,
                        )
                        answer = process_to_format([output, chart], type="Answer")
                        outputs = process_format_to_response(retrieval, answer)
                        memory.save_context({"input": user_input}, {"output": output}) # 랭체인
                        future.set_result(outputs)

                except Exception as e:
                    outputs = error_format("내부 Excel 에 해당 자료가 없습니다.", 551)
                    future.set_result(outputs)

            else:
                try:
                    print("[SOOWAN] TA is No, before make a retrieval")
                    docs, docs_list = execute_rag(
                        QU,
                        KE,
                        TA,
                        TI,
                        model=self.model,
                        tokenizer=self.tokenizer,
                        embed_model=self.embed_model,
                        embed_tokenizer=self.embed_tokenizer,
                        data=self.data,
                        config=self.config,
                    )
                    retrieval = process_to_format(docs_list, type="Retrieval")
                    print("[SOOWAN] TA is No, and make a retrieval is successed")
                    if is_streaming:
                        print(
                            f"[STREAM] Starting partial generation for request_id={request_id}"
                        )
                        await self._stream_partial_answer(
                            QU, docs, retrieval, None, request_id, future, user_input
                        )
                    else:
                        output = await generate_answer(
                            QU,
                            docs,
                            model=self.model,
                            tokenizer=self.tokenizer,
                            config=self.config,
                        )
                        print("process_to_format 이후에 OUTPUT 생성 완료")
                        answer = process_to_format([output], type="Answer")
                        print("process_to_format 이후에 ANSWER까지 생성 완료")
                        outputs = process_format_to_response(retrieval, answer)
                        memory.save_context({"input": user_input}, {"output": output}) # 랭체인
                        future.set_result(outputs)

                except Exception as e:
                    outputs = error_format("내부 PPT에 해당 자료가 없습니다.", 552)
                    future.set_result(outputs)

        except Exception as e:
            # If error, set the future
            err_msg = f"처리 중 오류 발생: {str(e)}"
            print("[ERROR]", err_msg)
            future.set_result(error_format(err_msg, 500))

    # ------------------------------------------------------------
    # HELPER FOR STREAMING PARTIAL ANSWERS (Modified to send reference)
    # ------------------------------------------------------------
    async def _stream_partial_answer(
        self, QU, docs, retrieval, chart, request_id, future, user_input
    ):
        """
        Instead of returning a final string, we generate partial tokens
        and push them to the SSE queue in real time.
        We'll do a "delta" approach so each chunk is only what's newly added.
        """
        print(
            f"[STREAM] _stream_partial_answer => request_id={request_id}, chart={chart}"
        )

        # 단일
        # queue = self.active_sse_queues.get(request_id)
        # if not queue:
        #     print(f"[STREAM] SSE queue not found => fallback to normal final (request_id={request_id})")
        #     # fallback...
        #     return

        # This will hold the entire text so far. We'll yield only new pieces.
        
        # 먼저, 참조 데이터 전송: type을 "reference"로 명시
        reference_json = json.dumps({
            "type": "reference",
            "status_code": 200,
            "result": "OK",
            "detail": "Reference data",
            "evt_time": datetime.datetime.now().isoformat(),
            "data_list": retrieval
        }, ensure_ascii=False)
        await self.queue_manager.put_token.remote(request_id, reference_json)
        print(f"[STREAM] Sent reference data for request_id={request_id}")
        
        # 1) 메모리 가져오기 (없으면 생성)
        try:
            memory = self.get_memory_for_session(request_id)
        except Exception as e:
            msg = f"[STREAM] Error retrieving memory for {request_id}: {str(e)}"
            print(msg)
            # 에러 응답을 SSE로 전송하고 종료
            error_token = json.dumps({"type":"error","message":msg}, ensure_ascii=False)
            await self.queue_manager.put_token.remote(request_id, error_token)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            future.set_result(error_format(msg, 500))
            return
        
        # 2) 과거 대화 이력 로드
        try:
            past_context = memory.load_memory_variables({})["history"]
        except KeyError:
            # 만약 "history" 키가 없으면 빈 문자열로 처리
            print(f"[STREAM] No 'history' in memory for {request_id}, using empty.")
            past_context = ""
        except Exception as e:
            msg = f"[STREAM] load_memory_variables error for {request_id}: {str(e)}"
            print(msg)
            error_token = json.dumps({"type":"error","message":msg}, ensure_ascii=False)
            await self.queue_manager.put_token.remote(request_id, error_token)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            future.set_result(error_format(msg, 500))
            return

        # 3) 최종 프롬프트 구성
        final_query = f"{past_context}\n\n[사용자 질문]\n{QU}"
        print(f"[STREAM] final_query = \n{final_query}")
        
        partial_accumulator = ""

        try:
            print(
                f"[STREAM] SSE: calling generate_answer_stream for request_id={request_id}"
            )
            async for partial_text in generate_answer_stream(
                final_query, docs, self.model, self.tokenizer, self.config
            ):
                # print(f"[STREAM] Received partial_text: {partial_text}")
                new_text = partial_text[len(partial_accumulator) :]
                partial_accumulator = partial_text
                if not new_text.strip():
                    continue
                    # Wrap answer tokens in a JSON object with type "answer"
                answer_json = json.dumps({
                    "type": "answer",
                    "answer": new_text
                }, ensure_ascii=False)
                # Use the central SSEQueueManager to put tokens
                # print(f"[STREAM] Sending token: {answer_json}")
                await self.queue_manager.put_token.remote(request_id, answer_json)
            final_text = partial_accumulator
            # 이제 memory에 저장 (이미 request_id를 알고 있다고 가정) # 랭체인
            try:
                memory.save_context({"input": user_input}, {"output": final_text})
            except Exception as e:
                msg = f"[STREAM] memory.save_context failed: {str(e)}"
                print(msg)

                
            if chart is not None:
                ans = process_to_format([final_text, chart], type="Answer")
                final_res = process_format_to_response(retrieval, ans)
            else:
                ans = process_to_format([final_text], type="Answer")
                final_res = process_format_to_response(retrieval, ans)
            future.set_result(final_res)
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")
            print(
                f"[STREAM] done => placed [[STREAM_DONE]] for request_id={request_id}"
            )
        except Exception as e:
            msg = f"[STREAM] error in partial streaming => {str(e)}"
            future.set_result(error_format(msg, 500))
            await self.queue_manager.put_token.remote(request_id, "[[STREAM_DONE]]")

    # ------------------------------------------------------------
    # NEW METHODS TO SUPPORT SSE
    # ------------------------------------------------------------
    # ----------------------
    # 1) Streaming Entrypoint
    # ----------------------
    async def process_query_stream(self, http_query: dict) -> str:
        """
        Called from /query_stream route.
        Create request_id, SSE queue, push to the micro-batch, return request_id.
        """
        # 사용자로부터 Request_id를 받거나 그렇지 않은 경우, 이를 랜덤으로 생성
        request_id = http_query.get("request_id")
        if not request_id:
            request_id = str(uuid.uuid4())
        await self.queue_manager.create_queue.remote(request_id)
        print(f"[STREAM] process_query_stream => request_id={request_id}, http_query={http_query}")


        loop = asyncio.get_event_loop()
        final_future = loop.create_future()

        sse_queue = asyncio.Queue()
        self.active_sse_queues[request_id] = sse_queue
        print(f"[STREAM] Created SSE queue for request_id={request_id}")

        # We'll push a special item (dict) onto the micro-batch queue
        queued_item = {
            "request_id": request_id,
            "http_query": http_query,
        }

        print(f"[STREAM] Putting item into request_queue for request_id={request_id}")
        await self.request_queue.put((queued_item, final_future, sse_queue))
        print(f"[STREAM] Done putting item in queue => request_id={request_id}")

        return request_id

    # ----------------------
    # 2) SSE token popping
    # ----------------------
    async def pop_sse_token(self, request_id: str) -> Optional[str]:
        """
        The SSE route calls this repeatedly to get partial tokens.
        If no token is available, we block up to 120s, else return None.
        """
        if request_id not in self.active_sse_queues:
            print(
                f"[STREAM] pop_sse_token => no SSE queue found for request_id={request_id}"
            )
            return None

        queue = self.active_sse_queues[request_id]
        try:
            token = await asyncio.wait_for(queue.get(), timeout=120.0)
            # print(f"[STREAM] pop_sse_token => got token from queue: {token}")
            return token
        except asyncio.TimeoutError:
            print(
                f"[STREAM] pop_sse_token => timed out waiting for token, request_id={request_id}"
            )
            return None

    # ----------------------
    # 3) SSE queue cleanup
    # ----------------------
    async def close_sse_queue(self, request_id: str):
        """
        Called by the SSE route after finishing.
        Remove the queue from memory.
        """
        if request_id in self.active_sse_queues:
            print(
                f"[STREAM] close_sse_queue => removing SSE queue for request_id={request_id}"
            )
            del self.active_sse_queues[request_id]
        else:
            print(f"[STREAM] close_sse_queue => no SSE queue found for {request_id}")


# Too using about two actor


# Ray Serve를 통한 배포
@serve.deployment(
    name="inference",
    max_ongoing_requests=50,
    )
class InferenceService:
    def __init__(self, config):
        self.config = config
        self.actor = InferenceActor.options(
            num_gpus=config.ray.num_gpus, num_cpus=config.ray.num_cpus
        ).remote(config)

    async def query(self, http_query: dict):
        result = await self.actor.process_query.remote(http_query)
        return result

    async def process_query_stream(self, http_query: dict) -> str:
        req_id = await self.actor.process_query_stream.remote(http_query)
        return req_id

    async def pop_sse_token(self, req_id: str) -> str:
        token = await self.actor.pop_sse_token.remote(req_id)
        return token

    async def close_sse_queue(self, req_id: str) -> str:
        await self.actor.close_sse_queue.remote(req_id)
        return "closed"


# Ray의 요청을 비동기적으로 관리하기 위해 도입하는 큐-매니저
@ray.remote
class SSEQueueManager:
    def __init__(self):
        self.active_queues = {}
        self.lock = asyncio.Lock()

    async def create_queue(self, request_id):
        async with self.lock:
            self.active_queues[request_id] = asyncio.Queue()
            return True

    async def get_queue(self, request_id):
        return self.active_queues.get(request_id)

    async def get_token(self, request_id, timeout: float):
        queue = self.active_queues.get(request_id)
        if queue:
            try:
                token = await asyncio.wait_for(queue.get(), timeout=timeout)
                return token
            except asyncio.TimeoutError:
                return None
        return None

    async def put_token(self, request_id, token):
        async with self.lock:
            if request_id in self.active_queues:
                await self.active_queues[request_id].put(token)
                return True
            return False

    async def delete_queue(self, request_id):
        async with self.lock:
            if request_id in self.active_queues:
                del self.active_queues[request_id]
                return True
            return False

```


--- utils.py

```python

# utils.py
import json
import numpy as np
import torch
import random
import shutil
from datetime import datetime, timedelta
from transformers import (
    AutoModel,
    AutoTokenizer,
    AutoModelForCausalLM,
    BitsAndBytesConfig,
    AutoConfig,
)

import os

# 전역 캐시 변수 - 데이터의 변화를 감지하기 위한
_cached_data = None
_cached_data_mtime = 0

# Import vLLM utilities
from vllm.engine.arg_utils import AsyncEngineArgs
from vllm.engine.async_llm_engine import AsyncLLMEngine

# Define the minimum valid file size (e.g., 10MB)
MIN_WEIGHT_SIZE = 10 * 1024 * 1024

# For tracking execution time of functions
from tracking import time_tracker

# Logging
import logging

logging.basicConfig(level=logging.DEBUG)


# -------------------------------------------------
# Function: find_weight_directory
# -------------------------------------------------
# Recursively searches for weight files (safetensors or pytorch_model.bin) in a given base path.
# This method Find the files searching the whole directory
# Because, vLLM not automatically find out the model files.
# -------------------------------------------------
@time_tracker
def find_weight_directory(base_path):
    # ---- Recursively searches for weight files in a given base path ----
    for root, dirs, files in os.walk(base_path):
        for file in files:
            if ".safetensors" in file or "pytorch_model.bin" in file:
                file_path = os.path.join(root, file)
                try:
                    if os.path.getsize(file_path) >= MIN_WEIGHT_SIZE:
                        return root, "safetensors" if ".safetensors" in file else "pt"
                    else:
                        logging.debug(
                            f"파일 {file_path}의 크기가 너무 작음: {os.path.getsize(file_path)} bytes"
                        )
                except Exception as ex:
                    logging.debug(f"파일 크기 확인 실패: {file_path} - {ex}")
    return None, None


# -------------------------------------------------
# Function: load_model
# -------------------------------------------------
@time_tracker
def load_model(config):
    # Loads the embedding model and the main LLM model (using vLLM if specified in the config).
    
    # Get the HF token from the environment variable.
    logging.info("Starting model loading...")
    token = os.getenv("HF_TOKEN_PATH")
    # Check if token is likely a file path.
    if token is not None and not token.startswith("hf_"):
        if os.path.exists(token) and os.path.isfile(token):
            try:
                with open(token, "r") as f:
                    token = f.read().strip()
            except Exception as e:
                print("DEBUG: Exception while reading token file:", e)
                logging.warning("Failed to read token from file: %s", e)
                token = None
        else:
            logging.warning("The HF_TOKEN path does not exist: %s", token)
            token = None
    else:
        print("DEBUG: HF_TOKEN appears to be a token string; using it directly:")

    if token is None or token == "":
        logging.warning("HF_TOKEN is not set. Access to gated models may fail.")
        token = None

    # -------------------------------
    # Load the embedding model and tokenizer.
    # -------------------------------
    print("Loading embedding model")
    try:
        embed_model = AutoModel.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,  # using 'token' parameter
        )
    except Exception as e:
        raise e
    try:
        embed_tokenizer = AutoTokenizer.from_pretrained(
            config.embed_model_id,
            cache_dir=config.cache_dir,
            trust_remote_code=True,
            token=token,
        )
    except Exception as e:
        raise e
    print(":Embedding tokenizer loaded successfully.")
    embed_model.eval()
    embed_tokenizer.model_max_length = 4096

    # -------------------------------
    # Load the main LLM model via vLLM.
    # -------------------------------
    if config.use_vllm:
        print("vLLM mode enabled. Starting to load main LLM model via vLLM.")
        if config.model.quantization_4bit:
            bnb_config = BitsAndBytesConfig(
                load_in_4bit=True,
                bnb_4bit_compute_dtype=torch.bfloat16,
                bnb_4bit_use_double_quant=True,
                bnb_4bit_quant_type="nf4",
            )
            print("Using 4-bit quantization.")
        elif config.model.quantization_8bit:
            bnb_config = BitsAndBytesConfig(load_in_8bit=True)
            print("Using 8-bit quantization.")
        else:
            bnb_config = None
            print("Using pure option of Model(No quantization)")

        local_model_path = os.path.join(
            config.cache_dir, "models--" + config.model_id.replace("/", "--")
        )
        local_model_path = os.path.abspath(local_model_path)

        config_file = os.path.join(local_model_path, "config.json")
        need_patch = False

        if not os.path.exists(config_file):
            os.makedirs(local_model_path, exist_ok=True)
            try:
                hf_config = AutoConfig.from_pretrained(
                    config.model_id,
                    cache_dir=config.cache_dir,
                    trust_remote_code=True,
                    token=token,
                )
            except Exception as e:
                raise e
            config_dict = hf_config.to_dict()
            if not config_dict.get("architectures"):
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
            with open(config_file, "w", encoding="utf-8") as f:
                json.dump(config_dict, f)
        else:
            with open(config_file, "r", encoding="utf-8") as f:
                config_dict = json.load(f)
            if not config_dict.get("architectures"):
                config_dict["architectures"] = ["Gemma2ForCausalLM"]
                with open(config_file, "w", encoding="utf-8") as f:
                    json.dump(config_dict, f)

        weight_dir, weight_format = find_weight_directory(local_model_path)
        if weight_dir is None:
            print("DEBUG: No model weights found. Attempting to download model snapshot.")
            max_retries = 3
            for attempt in range(max_retries):
                try:
                    print(f"DEBUG: Snapshot download attempt {attempt+1}...")
                    # Attempt to download the model snapshot using the Hugging Face hub function.
                    from huggingface_hub import snapshot_download
                    snapshot_download(config.model_id, cache_dir=config.cache_dir, token=token)
                    break  # If download succeeds, break out of the loop.
                except Exception as e:
                    print(f"DEBUG: Snapshot download attempt {attempt+1} failed:", e)
                    if attempt < max_retries - 1:
                        print("DEBUG: Retrying snapshot download...")
                    else:
                        raise RuntimeError(f"Snapshot download failed after {max_retries} attempts: {e}")
            # After download, try to find the weights again.
            weight_dir, weight_format = find_weight_directory(local_model_path)
            if weight_dir is None:
                raise RuntimeError(f"Unable to find model weights even after snapshot download in {local_model_path}.")

        snapshot_config = os.path.join(weight_dir, "config.json")
        if not os.path.exists(snapshot_config):
            shutil.copy(config_file, snapshot_config)
        engine_args = AsyncEngineArgs(
            model=weight_dir,
            tokenizer=config.model_id,
            download_dir=config.cache_dir,
            trust_remote_code=True,
            config_format="hf",
            load_format=weight_format,
        )
        
        vllm_conf = config.get("vllm", {})
        
        engine_args.enable_prefix_caching = True
        engine_args.scheduler_delay_factor = vllm_conf.get("scheduler_delay_factor", 0.1)
        engine_args.enable_chunked_prefill = True
        engine_args.tensor_parallel_size = vllm_conf.get("tensor_parallel_size", 1) # Using Multi-GPU at once.
        # engine_args.max_num_seqs = vllm_conf.get("max_num_seqs", 128)
        engine_args.max_num_batched_tokens = vllm_conf.get("max_num_batched_tokens", 8192)
        # engine_args.block_size = vllm_conf.get("block_size", 128)
        engine_args.gpu_memory_utilization = vllm_conf.get("gpu_memory_utilization")
        
        if vllm_conf.get("disable_custom_all_reduce", False):
            engine_args.disable_custom_all_reduce = True # For Fixing the Multi GPU problem
        
        # engine_args.enable_memory_defrag = True # v1 새로운 기능
        
        # print("Final EngineArgs:", engine_args)
        print("EngineArgs setting be finished")

        try:
            # --- v1 구동 해결책: 현재 스레드가 메인 스레드가 아니면 signal 함수를 임시 패치 ---
            import threading, signal
            if threading.current_thread() is not threading.main_thread():
                original_signal = signal.signal
                signal.signal = lambda s, h: None  # signal 설정 무시
                print("비메인 스레드에서 signal.signal을 monkey-patch 하였습니다.")
            # --- v1 구동 해결책: ------------------------------------------------------ ---
            engine = AsyncLLMEngine.from_engine_args(engine_args) # Original
            # v1 구동 해결책: 엔진 생성 후 원래 signal.signal으로 복원 (필요 시) ----------------- ---
            if threading.current_thread() is not threading.main_thread():
                signal.signal = original_signal
            # --- v1 구동 해결책: ------------------------------------------------------ ---
            print("DEBUG: vLLM engine successfully created.") # Original
            
        except Exception as e:
            print("DEBUG: Exception during engine creation:", e)
            if "HeaderTooSmall" in str(e):
                print("DEBUG: Falling back to PyTorch weights.")
                fallback_dir = None
                for root, dirs, files in os.walk(local_model_path):
                    for file in files:
                        if (
                            "pytorch_model.bin" in file
                            and os.path.getsize(os.path.join(root, file))
                            >= MIN_WEIGHT_SIZE
                        ):
                            fallback_dir = root
                            break
                    if fallback_dir:
                        break
                if fallback_dir is None:
                    logging.error(
                        "DEBUG: No PyTorch weight file found in", local_model_path
                    )
                    raise e
                engine_args.load_format = "pt"
                engine_args.model = fallback_dir
                print("DEBUG: New EngineArgs for fallback:", engine_args)
                engine = AsyncLLMEngine.from_engine_args(engine_args)
                print("DEBUG: vLLM engine created with PyTorch fallback.")
            else:
                logging.error("DEBUG: Engine creation failed:", e)
                raise e

        engine.is_vllm = True

        print("DEBUG: Loading main LLM tokenizer with token authentication.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
                local_files_only=True  # Force loading from local cache to avoid hub requests
            )
        except Exception as e:
            print("DEBUG: Exception loading main tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        return engine, tokenizer, embed_model, embed_tokenizer

    else:
        print("DEBUG: vLLM is not used. Loading model via standard HF method.")
        try:
            tokenizer = AutoTokenizer.from_pretrained(
                config.model_id,
                cache_dir=config.cache_dir,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading tokenizer:", e)
            raise e
        tokenizer.model_max_length = 4024
        try:
            model = AutoModelForCausalLM.from_pretrained(
                config.model_id,
                device_map="auto",
                torch_dtype=torch.bfloat16,
                cache_dir=config.cache_dir,
                quantization_config=bnb_config,
                trust_remote_code=True,
                token=token,
            )
        except Exception as e:
            print("DEBUG: Exception loading model:", e)
            raise e
        model.eval()
        return model, tokenizer, embed_model, embed_tokenizer

# @time_tracker
# def load_data(data_path):
#     global _cached_data, _cached_data_mtime
#     try:
#         current_mtime = os.path.getmtime(data_path)
#     except Exception as e:
#         print("파일 수정 시간 확인 실패:", e)
#         return None

#     # 캐시가 비어있거나 파일 수정 시간이 변경된 경우 데이터 재로드
#     if _cached_data is None or current_mtime != _cached_data_mtime:
#         with open(data_path, "r", encoding="utf-8") as json_file:
#             data = json.load(json_file)
#         # 데이터 전처리 (예: 리스트 변환 및 numpy, torch 변환)
#         file_names = []
#         titles = []
#         times = []
#         vectors = []
#         texts = []
#         texts_short = []
#         texts_vis = []
#         missing_time = 0
#         for file in data:
#             for chunk in file["chunks"]:
#                 file_names.append(file["file_name"])
#                 vectors.append(np.array(chunk["vector"]))
#                 titles.append(chunk["title"])
#                 if chunk["date"]:
#                     times.append(datetime.strptime(chunk["date"], "%Y-%m-%d"))
#                 else:
#                     missing_time += 1
#                     times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))
#                 texts.append(chunk["text"])
#                 texts_short.append(chunk["text_short"])
#                 texts_vis.append(chunk["text_vis"])
#         vectors = np.array(vectors)
#         vectors = torch.from_numpy(vectors).to(torch.float32)
#         _cached_data = {
#             "file_names": file_names,
#             "titles": titles,
#             "times": times,
#             "vectors": vectors,
#             "texts": texts,
#             "texts_short": texts_short,
#             "texts_vis": texts_vis,
#         }
#         _cached_data_mtime = current_mtime
#         print(f"Data loaded! Length: {len(titles)}, Missing times: {missing_time}")
#     else:
#         print("Using cached data")
#     return _cached_data

@time_tracker
def load_data(data_path):
    global _cached_data, _cached_data_mtime
    try:
        current_mtime = os.path.getmtime(data_path)
    except Exception as e:
        print("파일 수정 시간 확인 실패:", e)
        return None

    # 캐시가 비어있거나 파일 수정 시간이 변경된 경우 데이터 재로드
    if _cached_data is None or current_mtime != _cached_data_mtime:
        with open(data_path, "r", encoding="utf-8") as json_file:
            data = json.load(json_file)

        # --- 디버그 함수: 벡터 포맷 검사 ---
        debug_vector_format(data)

        # 데이터 전처리 (예: 리스트 변환 및 numpy, torch 변환)
        file_names = []
        titles = []
        times = []
        vectors = []
        texts = []
        texts_short = []
        texts_vis = []
        missing_time = 0

        for file_obj in data:
            for chunk in file_obj["chunks"]:
                file_names.append(file_obj["file_name"])
                
                # 여기서도 np.array()로 변환하며 에러가 있으면 except 처리
                try:
                    arr = np.array(chunk["vector"])
                    vectors.append(arr)
                except Exception as e:
                    logging.warning(f"[load_data] 벡터 변환 오류: {e} → 빈 벡터로 대체")
                    vectors.append(np.zeros((1, 768), dtype=np.float32))  # 임의로 1x768 형식
                
                titles.append(chunk["title"])
                
                # 날짜 파싱
                if chunk["date"]:
                    try:
                        times.append(datetime.strptime(chunk["date"], "%Y-%m-%d"))
                    except ValueError:
                        logging.warning(f"잘못된 날짜 형식: {chunk['date']} → 기본 날짜로 대체")
                        times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))
                        missing_time += 1
                else:
                    missing_time += 1
                    times.append(datetime.strptime("2023-10-31", "%Y-%m-%d"))

                texts.append(chunk["text"])
                texts_short.append(chunk["text_short"])
                texts_vis.append(chunk["text_vis"])

        # 실제 텐서로 변환
        try:
            vectors = np.array(vectors)
            vectors = torch.from_numpy(vectors).to(torch.float32)
        except Exception as e:
            logging.error(f"[load_data] 최종 벡터 텐서 변환 오류: {str(e)}")
            # 필요 시 추가 처리

        _cached_data = {
            "file_names": file_names,
            "titles": titles,
            "times": times,
            "vectors": vectors,
            "texts": texts,
            "texts_short": texts_short,
            "texts_vis": texts_vis,
        }
        _cached_data_mtime = current_mtime
        print(f"Data loaded! Length: {len(titles)}, Missing times: {missing_time}")
    else:
        print("Using cached data")

    return _cached_data


def debug_vector_format(data):
    """
    data(List[Dict]): load_data에서 JSON으로 로드된 객체.
    각 file_obj에 대해 chunks 리스트를 순회하며 vector 형식을 디버깅 출력.
    """
    print("\n[DEBUG] ===== 벡터 형식 검사 시작 =====")
    for f_i, file_obj in enumerate(data):
        file_name = file_obj.get("file_name", f"Unknown_{f_i}")
        chunks = file_obj.get("chunks", [])
        for c_i, chunk in enumerate(chunks):
            vector_data = chunk.get("vector", None)
            if vector_data is None:
                # print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector 없음(None)")
                continue
            # 자료형, 길이, shape 등 확인
            vector_type = type(vector_data)
            # shape을 안전하게 얻기 위해 np.array 변환 시도
            try:
                arr = np.array(vector_data)
                shape = arr.shape
                # print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector_type={vector_type}, shape={shape}")
            except Exception as e:
                print(f"[DEBUG] file={file_name}, chunk_index={c_i} → vector 변환 실패: {str(e)}")
    print("[DEBUG] ===== 벡터 형식 검사 종료 =====\n")


@time_tracker
def random_seed(seed):
    # Set random seed for Python's built-in random module
    random.seed(seed)

    # Set random seed for NumPy
    np.random.seed(seed)

    # Set random seed for PyTorch
    torch.manual_seed(seed)

    # Ensure the same behavior on different devices (CPU vs GPU)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # If using multi-GPU.

    # Enable deterministic algorithms
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False


@time_tracker
def process_to_format(qry_contents, type):
    # 여기서 RAG 시스템을 호출하거나 답변을 생성하도록 구현하세요.
    # 예제 응답 형식
    ### rsp_type : RA(Retrieval All), RT(Retrieval Text), RB(Retrieval taBle), AT(Answer Text), AB(Answer taBle) ###
    # print("[SOOWAN] process_to_format 진입")
    if type == "Retrieval":
        print("[SOOWAN] 타입 : 리트리버")
        tmp_format = {"rsp_type": "R", "rsp_tit": "남성 내부 데이터", "rsp_data": []}
        for i, form in enumerate(qry_contents):
            tmp_format_ = {
                "rsp_tit": f"{i+1}번째 검색데이터: {form['title']} (출처:{form['file_name']})",
                "rsp_data": form["contents"],
            }
            tmp_format["rsp_data"].append(tmp_format_)
        return tmp_format

    elif type == "SQL":
        # print("[SOOWAN] 타입 : SQL")
        tmp_format = {
            "rsp_type": "R",
            "rsp_tit": "남성 내부 데이터",
            "rsp_data": [{"rsp_tit": "SQL Query 결과표", "rsp_data": []}],
        }
        tmp_format_sql = {
            "rsp_type": "TB",
            "rsp_tit": qry_contents[0]["title"],
            "rsp_data": qry_contents[0]["data"],
        }
        tmp_format_chart = {
            "rsp_type": "CT",
            "rsp_tit": qry_contents[1]["title"],
            "rsp_data": {"chart_tp": "BAR", "chart_data": qry_contents[1]["data"]},
        }
        tmp_format["rsp_data"][0]["rsp_data"].append(tmp_format_sql)
        # tmp_format['rsp_data'].append(tmp_format_chart)
        return tmp_format, tmp_format_chart

    elif type == "Answer":
        print("[SOOWAN] 타입 : 대답")
        tmp_format = {"rsp_type": "A", "rsp_tit": "답변", "rsp_data": []}
        for i, form in enumerate(qry_contents):
            if i == 0:
                tmp_format_ = {"rsp_type": "TT", "rsp_data": form}
                tmp_format["rsp_data"].append(tmp_format_)
            elif i == 1:
                tmp_format["rsp_data"].append(form)
            else:
                None

        return tmp_format

    else:
        print("Error! Type Not supported!")
        return None


@time_tracker
def process_format_to_response(*formats):
    # Get multiple formats to tuple

    ans_format = {
        "status_code": 200,
        "result": "OK",
        "detail": "",
        "evt_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
        "data_list": [],
    }

    for format in formats:
        ans_format["data_list"].append(format)

    return ans_format


@time_tracker
def error_format(message, status):
    ans_format = {
        "status_code": status,
        "result": message,
        "detail": "",
        "evt_time": datetime.now().strftime("%Y-%m-%d %H:%M:%S.%f"),
    }
    return json.dumps(ans_format)

# ---------------------- 벡터화 -----------------------

import yaml
from box import Box
# Configuration
with open("./config.yaml", "r") as f:
    config_yaml = yaml.load(f, Loader=yaml.FullLoader)
    config = Box(config_yaml)

# 임베딩 모델 및 토크나이저 (청크 벡터화를 위해 별도 로드)
embedding_model = AutoModel.from_pretrained(config.embed_model_id, cache_dir=config.cache_dir)
embedding_tokenizer = AutoTokenizer.from_pretrained(config.embed_model_id, cache_dir=config.cache_dir)
embedding_model.eval()

# -------------------- 벡터화 함수 --------------------
@time_tracker
def vectorize_content(content):
    try:
        inputs = embedding_tokenizer(content, padding=True, truncation=True, return_tensors="pt")
        with torch.no_grad():
            outputs = embedding_model(**inputs, return_dict=False)
        # 첫 토큰의 임베딩을 사용 (1D 벡터)
        vector = outputs[0][:, 0, :].squeeze(0).tolist()
        
        # 벡터 일관성 확인
        expected_dim = 768  # 임베딩 모델 차원에 맞게 조정
        
        # 리스트가 아닌 경우 변환 시도
        if not isinstance(vector, list):
            print(f"경고: 벡터가 리스트가 아님, 타입: {type(vector)}")
            try:
                vector = list(vector)
            except Exception as e:
                print("오류: 벡터를 리스트로 변환 실패:", e)
                vector = [0.0] * expected_dim  # 기본 벡터 제공
        
        # 벡터 차원 확인 및 조정
        if len(vector) != expected_dim:
            print(f"경고: 벡터 차원 불일치. 예상: {expected_dim}, 실제: {len(vector)}")
            if len(vector) < expected_dim:
                # 부족한 차원은 0으로 패딩
                vector.extend([0.0] * (expected_dim - len(vector)))
            else:
                # 초과 차원은 자르기
                vector = vector[:expected_dim]
        
        # 기존 파일 형식과 일치하도록 항상 2차원 배열 형식으로 반환 ([[...] 형태])
        if vector and not isinstance(vector[0], list):
            return [vector]
        return vector
    except Exception as e:
        print(f"vectorize_content 함수 오류: {str(e)}")
        # 오류 시 기본 벡터 반환 (2차원 형식)
        return [[0.0] * 768]

# -------------------- 텍스트 출력 필드 정규화 함수 --------------------
def normalize_text_vis(text_vis):
    """
    text_vis가 이미 올바른 리스트-딕셔너리 구조이면 그대로 반환하고,
    그렇지 않은 경우 기본 구조로 감싸서 반환합니다.
    """
    if isinstance(text_vis, list) and len(text_vis) > 0 and isinstance(text_vis[0], dict):
        # 필요한 키가 존재하는지 확인
        if all(k in text_vis[0] for k in ("rsp_type", "rsp_tit", "rsp_data")):
            return text_vis
    if isinstance(text_vis, str):
        return [{
            "rsp_type": "TT",
            "rsp_tit": "",
            "rsp_data": text_vis
        }]
    return [{
        "rsp_type": "TT",
        "rsp_tit": "",
        "rsp_data": str(text_vis)
    }]

# -------------------- 데이터셋 진단 및 수정 도구 --------------------
# 데이터셋 진단 및 복구 함수 (utils.py 또는 별도 파일에 추가)
def diagnose_and_fix_dataset(data_path, output_path=None):
    """
    데이터셋의 벡터 차원 문제를 진단하고 수정합니다.
    """
    try:
        print(f"데이터셋 진단 중: {data_path}")
        with open(data_path, "r", encoding="utf-8") as f:
            data = json.load(f)
        
        print(f"데이터셋 내 파일 수: {len(data)}")
        dimensions = {}
        fixed_count = 0
        problem_count = 0
        
        # 1단계: 가장 흔한 차원 찾기
        for file_idx, file in enumerate(data):
            file_name = file.get("file_name", f"Unknown-{file_idx}")
            for chunk_idx, chunk in enumerate(file.get("chunks", [])):
                if "vector" in chunk and chunk["vector"]:
                    vector = chunk["vector"]
                    try:
                        if isinstance(vector, list):
                            dim = len(vector)
                            dimensions[dim] = dimensions.get(dim, 0) + 1
                        else:
                            print(f"벡터가 리스트가 아님: {file_name}, 청크 {chunk_idx}")
                            problem_count += 1
                    except Exception as e:
                        print(f"벡터 길이 확인 실패: {file_name}, 청크 {chunk_idx} - {str(e)}")
                        problem_count += 1
        
        if dimensions:
            # 가장 흔한 차원 찾기
            expected_dim = max(dimensions.items(), key=lambda x: x[1])[0]
            print(f"가장 흔한 벡터 차원: {expected_dim} (총 {dimensions[expected_dim]}개 발견)")
            print(f"발견된 모든 차원: {dimensions}")
        else:
            print("데이터셋에서 유효한 벡터를 찾을 수 없습니다!")
            return False
        
        # 2단계: 잘못된 차원의 벡터 수정
        for file_idx, file in enumerate(data):
            file_name = file.get("file_name", f"Unknown-{file_idx}")
            for chunk_idx, chunk in enumerate(file.get("chunks", [])):
                if "vector" in chunk and chunk["vector"]:
                    vector = chunk["vector"]
                    try:
                        if not isinstance(vector, list):
                            print(f"리스트가 아닌 벡터 수정 시도: {file_name}, 청크 {chunk_idx}")
                            try:
                                vector = list(vector)
                                chunk["vector"] = vector
                                fixed_count += 1
                            except:
                                # 변환 실패 시 빈 벡터 생성
                                chunk["vector"] = [0.0] * expected_dim
                                fixed_count += 1
                                print(f"리스트 변환 실패, 기본 벡터 사용")
                        
                        dim = len(vector)
                        if dim != expected_dim:
                            print(f"벡터 차원 수정: {file_name}, 청크 {chunk_idx} (차원: {dim})")
                            if dim < expected_dim:
                                # 0으로 패딩
                                chunk["vector"] = vector + [0.0] * (expected_dim - dim)
                            else:
                                # 자르기
                                chunk["vector"] = vector[:expected_dim]
                            fixed_count += 1
                    except Exception as e:
                        print(f"벡터 처리 중 오류: {file_name}, 청크 {chunk_idx} - {str(e)}")
                        problem_count += 1
        
        print(f"고정된 벡터 수: {fixed_count}, 문제 벡터 수: {problem_count}")
        
        # 수정된 데이터셋 저장
        if output_path is None:
            output_path = data_path
        
        # 덮어쓰기 전 백업 생성
        if output_path == data_path:
            backup_path = f"{data_path}.bak"
            print(f"백업 생성: {backup_path}")
            with open(backup_path, "w", encoding="utf-8") as f:
                json.dump(data, f, ensure_ascii=False, indent=2)
        
        print(f"수정된 데이터셋 저장: {output_path}")
        with open(output_path, "w", encoding="utf-8") as f:
            json.dump(data, f, ensure_ascii=False, indent=2)
        
        return True
    
    except Exception as e:
        print(f"데이터셋 진단 중 오류: {str(e)}")
        return False

```


--- RAG.py

```python

# RAG.py
import torch
import re
import numpy as np
import rank_bm25
import random
import uuid
import logging
from datetime import datetime, timedelta
from sql import generate_sql

# Tracking
from tracking import time_tracker

# Import the vLLM to use the AsyncLLMEngine
from vllm.engine.async_llm_engine import AsyncLLMEngine

# In RAG.py (at the top, add an import for prompts)
from prompt_rag import QUERY_SORT_PROMPT, GENERATE_PROMPT_TEMPLATE, STREAM_PROMPT_TEMPLATE

global beep
beep = "-------------------------------------------------------------------------------------------------------------------------------------------------------------------------"

@time_tracker
def execute_rag(QU, KE, TA, TI, **kwargs):
    print("[SOOWAN]: execute_rag : 진입")
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    embed_model = kwargs.get("embed_model")
    embed_tokenizer = kwargs.get("embed_tokenizer")
    data = kwargs.get("data")
    config = kwargs.get("config")

    if TA == "yes":  # Table 이 필요하면
        print("[SOOWAN]: execute_rag : 테이블 필요")
        # SQL
        final_sql_query, title, explain, table_json, chart_json = generate_sql(
            QU, model, tokenizer, config
        )

        # docs : 다음 LLM Input 으로 만들것 (String)
        PROMPT = f"""\ 
다음은 SQL 추출에 사용된 쿼리문이야 : {final_sql_query}. \
추가 설명 : {explain}. \
실제 SQL 추출된 데이터 : {str(table_json)}. \
"""
        # docs_list : 사용자들에게 보여줄 정보 (List)
        docs_list = [
            {"title": title, "data": table_json},
            {"title": "시각화 차트", "data": chart_json},
        ]

        return PROMPT, docs_list

    else:
        print("[SOOWAN]: execute_rag : 테이블 필요없음")
        # 적응형 시간 필터링으로 RAG 실행
        filtered_data = expand_time_range_if_needed(TI, data, min_docs=50)
        
        # 디버깅을 위해 문서 수 로깅
        print(f"[RETRIEVE] 검색에 사용되는 문서 수: {len(filtered_data.get('vectors', []))}")
        
        docs, docs_list = retrieve(KE, filtered_data, config.N, embed_model, embed_tokenizer)
        return docs, docs_list


@time_tracker
async def generate_answer(query, docs, **kwargs):
    model = kwargs.get("model")
    tokenizer = kwargs.get("tokenizer")
    config = kwargs.get("config")
    
    answer = await generate(docs, query, model, tokenizer, config)
    return answer


@time_tracker
async def query_sort(params):
    # params: 딕셔너리로 전달된 값들
    query = params["user_input"]
    model = params["model"]
    tokenizer = params["tokenizer"]
    embed_model = params["embed_model"]
    embed_tokenizer = params["embed_tokenizer"]
    data = params["data"]
    config = params["config"]

    # prompts/prompt_rag.py에서 프롬프트 별도 관리
    PROMPT = QUERY_SORT_PROMPT.format(user_query=query)
    
    # Get Answer from LLM
    print("##### query_sort is starting #####")
    if config.use_vllm:  # use_vllm = True case 
        from vllm import SamplingParams

        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        accepted_request_id = str(uuid.uuid4())
        answer = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
    else:
        input_ids = tokenizer(
            PROMPT, return_tensors="pt", truncation=True, max_length=4024
        ).to("cuda")
        token_count = input_ids["input_ids"].shape[1]
        outputs = model.generate(
            **input_ids,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )
        answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)

    print("[DEBUG query_sort] Generated answer:")
    print(answer)
    
    # Regular expressions for tags
    query_pattern = r"<query.*?>(.*?)<query.*?>"
    keyword_pattern = r"<keyword.*?>(.*?)<keyword.*?>"
    table_pattern = r"<table.*?>(.*?)<table.*?>"
    time_pattern = r"<time.*?>(.*?)<time.*?>"
    
    # [DEBUG-CHANGE]: Check each match before calling group(1)
    m_query = re.search(query_pattern, answer, re.DOTALL)
    m_keyword = re.search(keyword_pattern, answer, re.DOTALL)
    m_table = re.search(table_pattern, answer, re.DOTALL)
    m_time = re.search(time_pattern, answer, re.DOTALL)
    
    if not m_query:
        print("[ERROR query_sort] query_pattern not found in answer:")
        print(answer)
        raise ValueError("Missing <query> tag in generated answer.")
    if not m_keyword:
        print("[ERROR query_sort] keyword_pattern not found in answer:")
        print(answer)
        raise ValueError("Missing <keyword> tag in generated answer.")
    if not m_table:
        print("[ERROR query_sort] table_pattern not found in answer:")
        print(answer)
        raise ValueError("Missing <table> tag in generated answer.")
    if not m_time:
        print("[ERROR query_sort] time_pattern not found in answer:")
        print(answer)
        raise ValueError("Missing <time> tag in generated answer.")

    QU = m_query.group(1)
    KE = m_keyword.group(1)
    TA = m_table.group(1)
    TI = m_time.group(1)

    if TI == "all":
        TI = "1900-01-01:2099-01-01"
    print(beep)
    print(f"구체화 질문: {QU}, 키워드 : {KE}, 테이블 필요 유무: {TA}, 시간: {TI}")
    print(beep)
    return QU, KE, TA, TI


# @time_tracker
# def sort_by_time(time_bound, data):
#     date_format = "%Y-%m-%d"
#     target_date_start = datetime.strptime(time_bound.split(":")[0], date_format)
#     target_date_end = datetime.strptime(time_bound.split(":")[1], date_format)

#     matching_indices = [
#         i
#         for i, date in enumerate(data["times"])
#         if (not isinstance(date, str)) and (target_date_start < date < target_date_end)
#     ]

#     (
#         data["file_names"],
#         data["titles"],
#         data["times"],
#         data["vectors"],
#         data["texts"],
#         data["texts_short"],
#         data["texts_vis"],
#     ) = (
#         [lst[i] for i in matching_indices]
#         for lst in (
#             data["file_names"],
#             data["titles"],
#             data["times"],
#             data["vectors"],
#             data["texts"],
#             data["texts_short"],
#             data["texts_vis"],
#         )
#     )
#     return data


# @time_tracker
# def retrieve(query, data, N, embed_model, embed_tokenizer):
#     print("[SOOWAN] retrieve : 진입")
#     print("[SOOWAN] retrieve : 진입 정보 :", query)
    
#     sim_score = cal_sim_score(query, data["vectors"], embed_model, embed_tokenizer)
#     print("[SOOWAN] retrieve : sim_score :", sim_score)
    
#     try:
#         bm25_score = cal_bm25_score(query, data["texts_short"], embed_tokenizer)
#     except Exception as e:
#         print("[SOOWAN] retrieve : BM25 score exception, using zeros", e)
#         bm25_score = np.zeros(len(data["texts_short"]))
#     print("[SOOWAN] retrieve : bm25_score")
    
#     scaled_sim_score = min_max_scaling(sim_score)
#     scaled_bm25_score = min_max_scaling(bm25_score)
#     score = scaled_sim_score * 0.4 + scaled_bm25_score * 0.6
#     top_k = score[:, 0, 0].argsort()[-N:][::-1]
#     documents = ""
#     documents_list = []
#     for i, index in enumerate(top_k):
#         documents += f"{i+1}번째 검색자료 (출처:{data['file_names'][index]}) :\n{data['texts_short'][index]}\n"
#         documents_list.append({
#             "file_name": data["file_names"][index],
#             "title": data["titles"][index],
#             "contents": data["texts_vis"][index],
#         })
#         print("\n" + beep)
#     print("-------------자료 검색 성공--------------")
#     return documents, documents_list

@time_tracker
def sort_by_time(time_bound, data):
    """
    원본 데이터는 유지하고 필터링된 복사본을 반환하는 함수
    """
    # 원본 문서 수 로깅
    original_count = len(data["times"])
    print(f"[시간 필터 전] 문서 수: {original_count}")
    
    # "all" 시간 범위 특별 처리
    if time_bound == "all" or time_bound == "1900-01-01:2099-01-01":
        print(f"[시간 필터] 전체 기간 사용 - 모든 문서 포함")
        return data  # 원본 데이터 그대로 반환
    
    # 시간 범위 파싱
    date_format = "%Y-%m-%d"
    target_date_start = datetime.strptime(time_bound.split(":")[0], date_format)
    target_date_end = datetime.strptime(time_bound.split(":")[1], date_format)
    
    # 시간 범위에 맞는 문서 인덱스 찾기
    matching_indices = [
        i
        for i, date in enumerate(data["times"])
        if (not isinstance(date, str)) and (target_date_start < date < target_date_end)
    ]
    
    filtered_count = len(matching_indices)
    print(f"[시간 필터 후] 문서 수: {filtered_count}, 기간: {time_bound}")
    
    # 너무 적은 문서가 남은 경우 경고 로그
    if filtered_count < 50 and filtered_count < original_count * 0.1:
        print(f"[경고] 시간 필터로 인해 문서가 크게 줄었습니다: {original_count} → {filtered_count}")
    
    # 필터링된 데이터를 새로운 딕셔너리에 복사
    filtered_data = {}
    filtered_data["file_names"] = [data["file_names"][i] for i in matching_indices]
    filtered_data["titles"] = [data["titles"][i] for i in matching_indices]
    filtered_data["times"] = [data["times"][i] for i in matching_indices]
    
    # 벡터 타입에 따른 다른 처리
    if isinstance(data["vectors"], torch.Tensor):
        filtered_data["vectors"] = data["vectors"][matching_indices]
    else:
        filtered_data["vectors"] = [data["vectors"][i] for i in matching_indices]
    
    filtered_data["texts"] = [data["texts"][i] for i in matching_indices]
    filtered_data["texts_short"] = [data["texts_short"][i] for i in matching_indices]
    filtered_data["texts_vis"] = [data["texts_vis"][i] for i in matching_indices]
    
    return filtered_data

@time_tracker
def retrieve(query, data, N, embed_model, embed_tokenizer):
    print("[SOOWAN] retrieve : 진입")
    logging.info(f"Retrieval for query: '{query}'")
    logging.info(f"Available documents: {len(data['vectors'])}")
    
    try:
        sim_score = cal_sim_score(query, data["vectors"], embed_model, embed_tokenizer)
        logging.info(f"Similarity score shape: {sim_score.shape}")
        
        bm25_score = cal_bm25_score(query, data["texts_short"], embed_tokenizer)
        logging.info(f"BM25 score shape: {bm25_score.shape}")
        
        scaled_sim_score = min_max_scaling(sim_score)
        scaled_bm25_score = min_max_scaling(bm25_score)
        
        # Combined score (0.4 semantic + 0.6 lexical)
        score = scaled_sim_score * 0.4 + scaled_bm25_score * 0.6
        top_k = score[:, 0, 0].argsort()[-N:][::-1]
        
        # Log top results for debugging
        logging.info(f"Top {N} document indices: {top_k}")
        logging.info(f"Top {N} document scores: {[score[:, 0, 0][i] for i in top_k]}")
        logging.info(f"Top document titles: {[data['titles'][i] for i in top_k]}")
        
        documents = ""
        documents_list = []
        for i, index in enumerate(top_k):
            documents += f"{i+1}번째 검색자료 (출처:{data['file_names'][index]}) :\n{data['texts_short'][index]}\n"
            documents_list.append({
                "file_name": data["file_names"][index],
                "title": data["titles"][index],
                "contents": data["texts_vis"][index],
            })
            print("\n" + beep)
        print("-------------자료 검색 성공--------------")
        return documents, documents_list
        
        # Continue with document assembly...
    except Exception as e:
        logging.error(f"Retrieval error: {str(e)}", exc_info=True)
        return "", []

@time_tracker
def expand_time_range_if_needed(time_bound, data, min_docs=50):
    """
    시간 필터링 결과가 너무 적은 경우 자동으로 시간 범위를 확장하는 함수
    """
    # "all" 시간 범위는 그대로 사용
    if time_bound == "all" or time_bound == "1900-01-01:2099-01-01":
        print(f"[시간 범위] 전체 기간 사용")
        return data
    
    # 원래 시간 범위로 먼저 시도
    filtered_data = sort_by_time(time_bound, data)
    filtered_count = len(filtered_data.get("times", []))
    
    # 필터링된 문서 수가 충분하면 바로 반환
    if filtered_count >= min_docs:
        print(f"[시간 범위] 원래 범위로 충분한 문서 확보: {filtered_count}개")
        return filtered_data
    
    # 시간 범위 확장 시도
    print(f"[시간 범위 확장] 원래 범위는 {filtered_count}개 문서만 제공 (최소 필요: {min_docs}개)")
    
    # 원래 날짜 파싱
    date_format = "%Y-%m-%d"
    try:
        start_date = datetime.strptime(time_bound.split(":")[0], date_format)
        end_date = datetime.strptime(time_bound.split(":")[1], date_format)
    except Exception as e:
        print(f"[시간 범위 오류] 날짜 형식 오류: {time_bound}, 오류: {e}")
        return data  # 오류 시 원본 데이터 반환
    
    # 점진적으로 더 넓은 범위 시도
    expansions = [
        (3, "3개월"),
        (6, "6개월"),
        (12, "1년"),
        (24, "2년"),
        (60, "5년")
    ]
    
    for months, label in expansions:
        # 양방향으로 균등하게 확장
        new_start = start_date - timedelta(days=30*months//2)
        new_end = end_date + timedelta(days=30*months//2)
        
        new_range = f"{new_start.strftime(date_format)}:{new_end.strftime(date_format)}"
        print(f"[시간 범위 확장] {label} 확장 시도: {new_range}")
        
        expanded_data = sort_by_time(new_range, data)
        expanded_count = len(expanded_data.get("times", []))
        
        if expanded_count >= min_docs:
            print(f"[시간 범위 확장] {label} 확장으로 {expanded_count}개 문서 확보")
            return expanded_data
    
    # 모든 확장이 실패하면 전체 데이터셋 사용
    print(f"[시간 범위 확장] 모든 확장 시도 실패, 전체 데이터셋 사용")
    return data

@time_tracker
def cal_sim_score(query, chunks, embed_model, embed_tokenizer):
    print("[SOOWAN] cal_sim_score : 진입 / query : ", query)
    query_V = embed(query, embed_model, embed_tokenizer)
    print("[SOOWAN] cal_sim_score : query_V 생산 완료")
    if len(query_V.shape) == 1:
        query_V = query_V.unsqueeze(0)
        print("[SOOWAN] cal_sim_score : query_V.shape == 1")
    score = []
    for chunk in chunks:
        if len(chunk.shape) == 1:
            chunk = chunk.unsqueeze(0)
        query_norm = query_V / query_V.norm(dim=1)[:, None]
        chunk_norm = chunk / chunk.norm(dim=1)[:, None]
        tmp = torch.mm(query_norm, chunk_norm.transpose(0, 1)) * 100
        score.append(tmp.detach())
    return np.array(score)


# @time_tracker
# def cal_bm25_score(query, indexes, embed_tokenizer):
#     print("[SOOWAN] cal_bm25_score : 진입")
#     try:
#         tokenized_corpus = [
#             embed_tokenizer(
#                 text,
#                 return_token_type_ids=False,
#                 return_attention_mask=False,
#                 return_offsets_mapping=False,
#             )
#             for text in indexes
#         ]
#         tokenized_corpus = [
#             embed_tokenizer.convert_ids_to_tokens(corpus["input_ids"])
#             for corpus in tokenized_corpus
#         ]
#         print(f"[SOOWAN] cal_bm25_score : Tokenized corpus (first 2 items): {tokenized_corpus[:2]}")
#     except Exception as e:
#         print(f"[SOOWAN ERROR BM25] Error tokenizing corpus: {str(e)}")
#         return np.zeros(len(indexes))
#     if not tokenized_corpus or all(len(tokens) == 0 for tokens in tokenized_corpus):
#         print("[SOOWAN] cal_bm25_score: Empty tokenized corpus, returning zeros.")
#         return np.zeros(len(indexes))
#     try:
#         bm25 = rank_bm25.BM25Okapi(tokenized_corpus)
#     except Exception as e:
#         print(f"[SOOWAN ERROR BM25] Error initializing BM25: {str(e)}")
#         return np.zeros(len(indexes))
#     try:
#         tokenized_query = embed_tokenizer(query)
#         tokenized_query = embed_tokenizer.convert_ids_to_tokens(tokenized_query["input_ids"])
#         print(f"[SOOWAN] cal_bm25_score : Tokenized query: {tokenized_query}")
#     except Exception as e:
#         print(f"[SOOWAN ERROR BM25] Error tokenizing query: {str(e)}")
#         return np.zeros(len(indexes))
#     try:
#         bm25_score = bm25.get_scores(tokenized_query)
#         print(f"[SOOWAN] cal_bm25_score : BM25 score: {bm25_score}")
#     except Exception as e:
#         print(f"[SOOWAN ERROR BM25] Error computing BM25 scores: {str(e)}")
#         return np.zeros(len(indexes))
#     return np.array(bm25_score)
@time_tracker
def cal_bm25_score(query, indexes, embed_tokenizer):
    logging.info(f"Starting BM25 calculation for query: {query}")
    logging.info(f"Document count: {len(indexes)}")
    
    if not indexes:
        logging.warning("Empty document list provided to BM25")
        return np.zeros(0)
        
    # Process documents individually to isolate failures
    tokenized_corpus = []
    for i, text in enumerate(indexes):
        try:
            tokens = embed_tokenizer(text, return_token_type_ids=False,
                                    return_attention_mask=False,
                                    return_offsets_mapping=False)
            tokens = embed_tokenizer.convert_ids_to_tokens(tokens["input_ids"])
            if len(tokens) == 0:
                logging.warning(f"Document {i} tokenized to empty list")
                tokens = ["<empty>"]  # Placeholder to avoid BM25 errors
            tokenized_corpus.append(tokens)
        except Exception as e:
            logging.error(f"Failed to tokenize document {i}: {str(e)}")
            tokenized_corpus.append(["<error>"])  # Placeholder
    
    try:
        bm25 = rank_bm25.BM25Okapi(tokenized_corpus)
        tokenized_query = embed_tokenizer.convert_ids_to_tokens(
            embed_tokenizer(query)["input_ids"]
        )
        scores = bm25.get_scores(tokenized_query)
        
        # Check for valid scores
        if np.isnan(scores).any() or np.isinf(scores).any():
            logging.warning("BM25 produced NaN/Inf scores - replacing with zeros")
            scores = np.nan_to_num(scores)
            
        logging.info(f"BM25 scores: min={scores.min():.4f}, max={scores.max():.4f}, mean={scores.mean():.4f}")
        return scores
    except Exception as e:
        logging.error(f"BM25 scoring failed: {str(e)}")
        return np.zeros(len(indexes))



@time_tracker
def embed(query, embed_model, embed_tokenizer):
    print("[SOOWAN] embed: 진입")
    inputs = embed_tokenizer(query, padding=True, truncation=True, return_tensors="pt")
    embeddings, _ = embed_model(**inputs, return_dict=False)
    print("[SOOWAN] embed: 완료")
    return embeddings[0][0]


@time_tracker
def min_max_scaling(arr):
    arr_min = arr.min()
    arr_max = arr.max()
    if arr_max == arr_min:
        print("[SOOWAN] min_max_scaling: Zero range detected, returning zeros.")
        return np.zeros_like(arr)
    return (arr - arr_min) / (arr_max - arr_min)


@time_tracker
async def generate(docs, query, model, tokenizer, config):
    PROMPT = GENERATE_PROMPT_TEMPLATE.format(docs=docs, query=query)
    print("Inference steps")
    if config.use_vllm:
        from vllm import SamplingParams
        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        accepted_request_id = str(uuid.uuid4())
        answer = await collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id)
    else:
        input_ids = tokenizer(PROMPT, return_tensors="pt", truncation=True, max_length=4024).to("cuda")
        token_count = input_ids["input_ids"].shape[1]
        outputs = model.generate(
            **input_ids,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
            eos_token_id=tokenizer.eos_token_id,
            pad_token_id=tokenizer.eos_token_id,
        )
        generated_tokens = outputs[0].shape[0]
        answer = tokenizer.decode(outputs[0][token_count:], skip_special_tokens=True)
        print(answer)
        print(">>> decode done, returning answer")
    return answer


@time_tracker
async def collect_vllm_text(PROMPT, model, sampling_params, accepted_request_id):
    import asyncio, concurrent.futures
    outputs = []
    async for output in model.generate(PROMPT, request_id=accepted_request_id, sampling_params=sampling_params):
        outputs.append(output)
    if not outputs:
        raise RuntimeError("No outputs were generated by the model.")
    final_output = next((o for o in outputs if getattr(o, "finished", False)), outputs[-1])
    answer = "".join([getattr(comp, "text", "") for comp in getattr(final_output, "outputs", [])])
    return answer


@time_tracker
async def generate_answer_stream(query, docs, model, tokenizer, config):
    prompt = STREAM_PROMPT_TEMPLATE.format(docs=docs, query=query)
    if config.use_vllm:
        from vllm import SamplingParams
        sampling_params = SamplingParams(
            max_tokens=config.model.max_new_tokens,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        request_id = str(uuid.uuid4())
        async for partial_chunk in collect_vllm_text_stream(prompt, model, sampling_params, request_id):
            # print(f"[STREAM] generate_answer_stream yielded: {partial_chunk}")
            yield partial_chunk
    else:
        import torch
        from transformers import TextIteratorStreamer
        input_ids = tokenizer(prompt, return_tensors="pt", truncation=True, max_length=4024).to("cuda")
        streamer = TextIteratorStreamer(tokenizer, skip_special_tokens=True)
        generation_kwargs = dict(
            **input_ids,
            streamer=streamer,
            max_new_tokens=config.model.max_new_tokens,
            do_sample=config.model.do_sample,
            temperature=config.model.temperature,
            top_k=config.model.top_k,
            top_p=config.model.top_p,
            repetition_penalty=config.model.repetition_penalty,
        )
        import threading
        t = threading.Thread(target=model.generate, kwargs=generation_kwargs)
        t.start()
        for new_token in streamer:
            yield new_token

@time_tracker
async def collect_vllm_text_stream(prompt, engine: AsyncLLMEngine, sampling_params, request_id) -> str:
    async for request_output in engine.generate(prompt, request_id=request_id, sampling_params=sampling_params):
        if not request_output.outputs:
            continue
        for completion in request_output.outputs:
            # print(f"[STREAM] collect_vllm_text_stream yielding: {completion.text}")
            yield completion.text


if __name__ == "__main__":
    import asyncio
    # engine = AsyncLLMEngine.from_engine_args(engine_args, start_engine_loop=False)
    # if not engine.is_running:
    #     engine.start_background_loop()
    
    async def main():
        status = True
        while status:
            query = input("질문 : ")
            QU, TA, TI = await query_sort({"user_input": query, "model": None, "tokenizer": None, "embed_model": None, "embed_tokenizer": None, "data": None, "config": None})
            print("query_sort result done")
            if TA == "yes":
                print("\n" + beep)
                SQL_results = generate_sql(QU)
                answer = await generate(SQL_results, query)
                print(answer)
                print("\n" + beep)
            else:
                file_names, titles, times, vectors, texts, texts_short = sort_by_time(TI, file_names, titles, times, vectors, texts, texts_short)
                print("\n" + beep)
                docs = retrieve(QU, vectors, texts, texts_short, file_names, N)
                print("\n" + beep)
                answer = await generate(docs, query)
                print(answer)
                print("\n" + beep)
    asyncio.run(main())

```


--- prompt_rag.py

```python

# prompts/prompt_rag.py
from datetime import datetime

# Get today's date (this is computed when the module is loaded)
TODAY = datetime.today()

# Prompt for query sorting
QUERY_SORT_PROMPT = f"""
<bos><start_of_turn>user
너는 질문의 유형을 파악하고 분류하는 역할이야. 질문에 대해 질문자의 의도를 파악하고, 내가 지시하는 대로 답변형태를 맞춰서 해줘. 
query는 질문을 구체화 하는 거야, 그리고 만약 질문에 오타가 있다면 고쳐줘. 
keyword는 질문의 키워드를 뽑는거야. 
table은 질문에 대한 답을 할때 표형식 데이터가 필요한지 여부야, 현재는 매출액 관련 질문만 대응 가능하니 이때만 yes로 답해줘.
time은 질문에 답하기 위해 필요한 데이터의 날짜 범위야(오늘 날짜는 {TODAY.year}년 {TODAY.month}월 {TODAY.day}일). 
시간의 길이는 최소 3개월로 설정해야하고, 날짜는 1일로 설정해. (예시:2024년 10월에 대한 질문은 2024-08-01:2024-11-01) 
또한, '최근'이라는 말이 들어가면 2024-06-01:{TODAY.year}-{TODAY.month}-{TODAY.day}로 설정해줘.

내가 먼저 예시를 줄게

질문: 최근 일본발 베트남착 매출면에서 우리사에 기여도가 높은 화주(고객)은 어떻게 돼?
답변:
<query/>최근 일본발 베트남착 매출면에서 우리사에 기여도가 높은 화주(고객)은 어떻게 돼?<query>
<keyword/>일본발 베트남착 매출 기여도 화주 고객<keyword>
<table/>yes<table>
<time/>2024-08-01:2024-{TODAY.month}-{TODAY.day}<time>

질문: 올해 3월에 중국 시장 전망에 대해 조사했던 내용을 정리해줘
답변:
<query/>2024년 3월 중국시장 전망에 대한 조사내용을 알려주고 정리해줘<query>
<keyword/>2024년 3월 중국시장 전망<keyword>
<table/>no<table>
<time/>2024-02-01:2024-05-01<time>

질문: 부산발 인도네시아착 경쟁사 서비스 및 항차수를 알려줘
답변:
<query/>부산 출발 인도네시아 도착 경쟁사 서비스 및 항차수<query>
<keyword/>부산발 인도네시아착 경쟁사 서비스 항차수<keyword>
<table/>no<table>
<time/>all<time>

질문: 남성해운의 인도 대리점 선정 과정은 어떻게 돼?
답변:
<query/>인도 대리점 선정과정을 보기 좋게 정리해줘<query>
<keyword/>인도 대리점 선정과정<keyword>
<table/>no<table>
<time/>all<time>

### 아래 구분자를 추가하여 실제 사용자 질문을 명확히 구분합니다.
### 새로운 질문: {{user_query}}<end_of_turn>
<start_of_turn>model
답변:
"""

# Template for generating an answer based on internal documents
GENERATE_PROMPT_TEMPLATE = """
<bos><start_of_turn>user
너는 남성해운의 도움을 주는 데이터 분석가야.
주어진 내부 자료에 기반해서 내 질문에 대답해줘. 답변 형식은 보고서처럼 길고 자세하며 논리정연하게 사실만을 가지고 작성해줘.
만약 주어진 자료에 질문에 해당하는 내용이 없으면 "내부 자료에 해당 자료 없음"으로 답변해줘.
또한, 반드시 근거로 사용한 데이터의 출처를 명시해줘.
내부 자료가 표로 들어오면, 그 표를 최대한 말로 풀어서 해석해주고 논리적인 인사이트를 도출해줘.
답변 형식은 markdown 형식으로 작성해줘.
내부 자료: {docs}
질문: {query}<end_of_turn>
<start_of_turn>model
답변:
"""

# Template for the streaming version of answer generation
STREAM_PROMPT_TEMPLATE = """
<bos><start_of_turn>user
너는 남성해운의 도움을 주는 데이터 분석가야.
주어진 내부 자료에 기반해서 내 질문에 대답해줘. 답변 형식은 보고서처럼 길고 자세하며 논리정연하게 사실만을 가지고 작성해줘.
만약 주어진 자료에 질문에 해당하는 내용이 없으면 "내부 자료에 해당 자료 없음"으로 답변해줘.
또한, 반드시 근거로 사용한 데이터의 출처를 명시해줘.
내부 자료가 표로 들어오면, 그 표를 최대한 말로 풀어서 해석해주고 논리적인 인사이트를 도출해줘.
답변 형식은 markdown 형식으로 작성해줘.
내부 자료: {docs}
질문: {query}<end_of_turn>
<start_of_turn>model
답변:
"""

```


--- sql.py

```python

# sql.py
import json
import sqlite3
import re

def generate_sql(query, model, tokenizer, config):
    with open(config.metadata_path, 'r', encoding='utf-8') as file:
        Metadata = json.load(file)
    column_usage = Metadata['column_usage']

    # first_LLM
    outputs_1, filter_conditions, aggregations, orders, sql_query, parsed_columns = first_llm(model, tokenizer, column_usage, query, config)
    print(f'FirstLLM\n필터:{filter_conditions}\n집계:{aggregations}\n정렬:{orders}\nSQL:{sql_query}\n컬럼:{parsed_columns}')
    print(config.beep)

    relevant_metadata = extract_relevant_metadata(parsed_columns, column_usage) # 추출된 컬럼에 해당하는 메타데이터 가져오기
    retrival_metadata = parse_and_augment_filter_conditions(filter_conditions, Metadata)   # Metadata와 매핑하여 구체화된 필터 조건 찾기
    print(f'MetaData\n관련:{relevant_metadata}\n검색:{retrival_metadata}')
    print(config.beep)
    # second_LLM
    final_sql_query, title, explain, outputs_2 = second_llm(model, tokenizer, relevant_metadata, sql_query, query, retrival_metadata, parsed_columns, config)
    print(f'SecondLLM\n제목:{title}\n설명:{explain}\nSQL:{final_sql_query}')
    print(config.beep)
    columns, results = execute_sql_query(final_sql_query, config)   # SQL 쿼리 실행 (데이터 조회)
    print(f'Result\n컬럼:{columns}\n결과:{results}')
    print(config.beep)

    # result -> json
    table_json = create_table_json(columns, results)
    chart_json = create_chart_json(columns, results)
    
    # 결과 출력
    if results:
        print("조회된 컬럼:\n", columns)
        for row in results:
            print(row)
        return final_sql_query, title, explain, table_json, chart_json
    else:
        print("조회 결과가 없습니다.")
        return None
    
def first_llm(model, tokenizer, column_usage, user_query, config):
    PROMPT =\
    f'''
    <bos><start_of_turn>user
    너는 남성 해운 회사의 데이터로 SQL 쿼리를 작성하는 데 도움을 주는 시스템이야. 사용자로부터 받은 질문을 분석하여, 필터 조건, 집계 함수, 정렬 조건, SQL 쿼리 초안, SQL 쿼리에 사용된 모든 컬럼을 추출해줘.
    
    ### 참고 사항:
    1. 다음은 해운 회사 데이터의 메타데이터야. 테이블은 "revenue" 하나뿐이야.: 
    "{column_usage}"
    2. 사용자가 입력한 질문을 분석하여 필요한 컬럼을 식별하고, SQL 쿼리에서 사용할 필터 조건, 집계 함수, 정렬 기준을 제공해줘.
    3. 사용되는 프로그램은 SQLite 야. 이 프로그램에 맞는 언어를 사용해줘 (SQLite 날짜 형식 사용 예시 : strftime('%Y', OUTOBD) AS Year )
    
    ### 사용자가 입력한 질문:
    "{user_query}"
    
    ### 필요한 정보:
    1. 필터 조건 (필요한 경우, 예: <filter/>OUTPOL = '부산', OUTPOD = '일본', OUTBOR = '2024-08-01 이후'<filter/>)
    2. 집계 함수 (필요한 경우, 예: <aggregation/>화주(고객)별 매출액의 합계<aggregation/>)
    3. 정렬 조건 (필요한 경우, 예: <order/>매출액 기준 내림차순<order/>)
    4. SQL 쿼리 초안 (예: <sql_query/>SELECT OUTSHC,SUM(OUTSTL) AS TotalRevenue\n    FROM revenue\n    WHERE OUTPOL = \'한국\' AND OUTPOD = \'베트남\' AND OUTOBD >= \'2023-01-01\'    GROUP BY OUTSHC\n    ORDER BY TotalRevenue DESC;<sql_query/>)
    5. SQL 쿼리에 사용된 모든 컬럼 (예: <columns/>OUTPOL,OUTPOD,OUTBOR,OUTSHC,OUTSTL<columns/>)
    
    ### 출력 형식:
    1. 필터 조건: <filter/><filter/>
    2. 집계 함수:<aggregation/><aggregation/>
    3. 정렬 조건:<order/><order/>
    4. SQL 쿼리 초안 : <sql_query/><sql_query/>
    5. SQL 쿼리에 사용된 모든 컬럼:<columns/><columns/>
    6. 날짜는 YYYY-MM-DD 형식을 사용 (ex: "2023-05-01")
    
    <end_of_turn>
    <start_of_turn>model
    '''

    # Get Answer
    input_ids = tokenizer(PROMPT, return_tensors="pt").to("cuda")
    input_length = input_ids['input_ids'].shape[1]
    outputs = model.generate(**input_ids, max_new_tokens=config.model.max_new_tokens)
    outputs_result = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)

    # Regular expression to extract content between <query/> and <query>
    filter_pattern = r'<filter.*?>(.*?)<filter.*?>'
    aggregation_pattern = r'<aggregation.*?>(.*?)<aggregation.*?>'
    order_pattern = r'<order.*?>(.*?)<order.*?>'
    sql_pattern = r'<sql_query.*?>(.*?)<sql_query.*?>'
    columns_pattern = r'<columns.*?>(.*?)<columns.*?>'
    
    filter_conditions = re.search(filter_pattern, outputs_result, re.DOTALL).group(1)
    aggregations = re.search(aggregation_pattern, outputs_result, re.DOTALL).group(1)
    orders = re.search(order_pattern, outputs_result, re.DOTALL).group(1)
    sql_queries = re.search(sql_pattern, outputs_result, re.DOTALL).group(1)
    parsed_columns = [col.strip() for col in re.search(columns_pattern, outputs_result, re.DOTALL).group(1).split(",")]
    return outputs_result, filter_conditions, aggregations, orders, sql_queries, parsed_columns

# 추출된 컬럼에 해당하는 메타데이터만 가져오는 함수
def extract_relevant_metadata(columns, metadata):
    relevant_metadata = {}
    for column in columns:
        if column in metadata["column_usage"]:
            relevant_metadata[column] = metadata["column_usage"][column]
    return relevant_metadata

# def parse_and_augment_filter_conditions(filter_conditions, Metadata):
#     pattern = r"(\w+)\s*=\s*'([^']+)'"  # ex: OUTPOL = '부산' 과 같은 패턴 추출
#     matches = re.findall(pattern, filter_conditions)
    
#     augmented_filters = []
    
#     for col, val in matches:
#         if col == 'OUTPOL' or col == 'OUTPOD':
#             location_code = Metadata['location_code']
#             mapped_value = search_location_db(val, location_code)
#             if mapped_value != "UNKNOWN":
#                 augmented_filters.append(f"컬럼 {col}에 대한 값 '{val}' -> '{mapped_value}'로 매핑되었습니다.")
#             else:
#                 augmented_filters.append(f"컬럼 {col}의 값 '{val}'에 대한 매핑 정보를 찾을 수 없습니다.")
#     return "\n".join(augmented_filters)

def parse_and_augment_filter_conditions(filter_conditions, Metadata):
    # '컬럼명 = '값'' 또는 '컬럼명 IN ('값1', '값2', ...)' 패턴에 대응하는 정규식
    pattern = r"(\w+)\s*=\s*'([^']+)'|\b(\w+)\s+IN\s+\(([^)]+)\)"
    matches = re.findall(pattern, filter_conditions)
    
    augmented_filters = []
    for match in matches:
        # 매칭 결과에서 'IN' 조건과 '=' 조건을 구분하여 처리
        if match[0]:  # '=' 조건
            col, val = match[0], match[1]
            if col == 'OUTPOL' or col == 'OUTPOD':
                location_code = Metadata['location_code']
                mapped_value = search_location_db(val, location_code)
                if mapped_value != "UNKNOWN":
                    augmented_filters.append(f"컬럼 {col}에 대한 값 '{val}' -> '{mapped_value}'로 매핑되었습니다.")
                else:
                    augmented_filters.append(f"컬럼 {col}의 값 '{val}'에 대한 매핑 정보를 찾을 수 없습니다.")
                    
        elif match[2]:  # 'IN' 조건
            col, val_list = match[2], match[3]
            if col == 'OUTPOL' or col == 'OUTPOD':
                location_code = Metadata['location_code']
                values = [val.strip().strip("'") for val in val_list.split(",")]
                
                mapped_values = []
                for val in values:
                    mapped_value = search_location_db(val, location_code)
                    if mapped_value != "UNKNOWN":
                        mapped_values.append(f"'{val}' -> '{mapped_value}'")
                    else:
                        mapped_values.append(f"'{val}' (매핑 정보 없음)")
                
                augmented_filters.append(f"컬럼 {col}에 대한 값들: {', '.join(mapped_values)}")

    return "\n".join(augmented_filters)

# Location 검색 알고리즘 (매핑 정보 검색)
def search_location_db(location, location_code):
    return location_code.get(location, "Mapping error")

def second_llm(model, tokenizer, relevant_metadata, sql_query, user_query, retrival_metadata, parsed_columns, config):
    PROMPT =\
    f'''
    <bos><start_of_turn>user
    너는 남성 해운 회사의 데이터로 정확한 SQL 쿼리를 작성해주는 시스템이야. 너가 참고해야 할 정보가 있는 경우에는 이를 참고해서 SQL 쿼리 초안을 구체화해서 정확한 SQL 쿼리를 만들어줘. 그리고 이 SQL 쿼리가 어떤 정보를 추출해주는지 짧게 제목을 짓고, 어떻게 사용자의 질문에 답할 수 있는 정보를 추출하는지 설명해줘. 참고해야 할 정보가 없고 SQL 쿼리 초안이 이미 정확하다면, 그대로 출력해줘.

    ### 참고 사항:
    1. 다음은 너가 참고해야 할 정보야:
    "{retrival_metadata}"
    2. SQL 쿼리 초안:
    "{sql_query}"    
    3. 다음은 사용한 데이터의 메타데이터야:
    "{relevant_metadata}"
    4. 다음은 사용자가 입력한 질문이야:
    "{user_query}"    


    ### 필요한 정보:
    1. 정확한 SQL 쿼리 (예: <sql_query/>SELECT OUTSHC,SUM(OUTSTL) AS TotalRevenue\n    FROM revenue\n    WHERE WHERE OUTPOL = \'KRPUS\' AND OUTPOD LIKE \'CN%\' AND OUTOBD >= \'2023-01-01\'\n    GROUP BY OUTSHC\n    ORDER BY TotalRevenue DESC;<sql_query/>)
    2. SQL가 조회하는 데이터 요약 (예: 부산발 중국착 매출 순위 (화주별))
    3. SQL 쿼리 설명

    ### 출력 형식(아래 출력 형식을 꼭 지켜야 해 시작부분에 / 이 들어가고 끝부분에는 없어):
    1. 정확한 SQL 쿼리: <sql_query/>SQL 명령어<sql_query>
    2. SQL가 조회하는 데이터 요약: <title/>데이터 설명문<title>
    3. SQL 쿼리 설명: <explain/>SQL 설명문<explain>
    
    ### 참고자료
    1. 만약 참고자료에 KR% 같은 조건이 있으면 LIKE 를, KRCRD 같은 정확한 정보는 = 를 사용.
    2. 만약 여러개의 LIKE 조건이 있으면 (예시: WHERE OUTPOL LIKE 'KR%' OR OUTPOL LIKE 'CN%' OR OUTPOL LIKE 'JP%') 를 사용.
    3. 날짜는 YYYY-MM-DD 형식을 사용 (ex: "2023-05-01")
    4. LIKE 로 들어간 컬럼들은 다음과 같이 보기좋게 해줘.
    예시 : 
    SELECT 
        CASE 
            WHEN OUTPOL LIKE 'KR%' THEN '한국'
            WHEN OUTPOL LIKE 'JP%' THEN '일본'
            WHEN OUTPOL LIKE 'CN%' THEN '중국'
            ELSE '기타' 
        END AS 국가,
        SUM(OUTSTL) AS TotalRevenue

    <end_of_turn>
    <start_of_turn>model
    '''

    # Get Answer
    input_ids = tokenizer(PROMPT, return_tensors="pt").to("cuda")
    input_length = input_ids['input_ids'].shape[1]
    outputs = model.generate(**input_ids, max_new_tokens=config.model.max_new_tokens)
    outputs_result = tokenizer.decode(outputs[0][input_length:], skip_special_tokens=True)
    print(f'2번째 LLM Output:{outputs_result}')
    sql_pattern = r'<sql_query.*?>(.*?)<sql_query.*?>'
    title_pattern = r'<title.*?>(.*?)<title.*?>'
    explain_pattern = r'<explain.*?>(.*?)<explain.*?>'
    
    sql_queries = re.search(sql_pattern, outputs_result, re.DOTALL).group(1)
    title = re.search(title_pattern, outputs_result, re.DOTALL).group(1)
    explain = re.search(explain_pattern, outputs_result, re.DOTALL).group(1)
    
    return sql_queries, title, explain, outputs_result

def execute_sql_query(sql_query, config):
    try:
        conn = sqlite3.connect(config.sql_data_path)        # SQLite 데이터베이스에 연결
        cursor = conn.cursor()

        if (config.k is not None) and ("LIMIT" not in sql_query):
            sql_query = sql_query.split(";")[0].strip()
            sql_query += f"\nLIMIT {config.k};"
                 
        cursor.execute(sql_query)        # SQL 쿼리 실행
        
        result = cursor.fetchall()        # 결과 가져오기
        column_names = [description[0] for description in cursor.description]        # 컬럼 이름도 포함하기 위해 description을 사용

        cursor.close()
        conn.close()
        
        return column_names, result

    except sqlite3.Error as e:
        print(f"데이터베이스 오류 발생: {e}")
        return None, None

def create_table_json(columns, results):
    head = "||".join(columns)
    body = "^ ".join("||".join(map(str, row)) for row in results)
    table = {"head": head, "body": body}
    return table

def create_chart_json(columns, results):
    chart = [
        {"label": f"{row[-2]}", "data": [{"x": "매출액", "y": str(row[-1])}]}
        for i, row in enumerate(results)
    ]
    return chart

```


--- tracking.py

```python

# tracking.py
import time
import logging

# Configure logging however you like
logging.basicConfig(level=logging.INFO, format='%(asctime)s %(levelname)s %(message)s')

def time_tracker(func):
    def wrapper(*args, **kwargs):
        start_time = time.time()
        logging.info(f"Entering {func.__name__}()")
        result = func(*args, **kwargs)
        end_time = time.time()
        elapsed = end_time - start_time
        logging.info(f"Exiting {func.__name__}() -- Elapsed: {elapsed:.2f}s")
        return result
    return wrapper

```


--- data_control.py

```python

# data_control.py
import os
import json
import datetime
import torch
import numpy as np
from umap import UMAP
from flask import Blueprint, request, jsonify, render_template
from transformers import AutoModel, AutoTokenizer
from utils import vectorize_content, normalize_text_vis  # Assumes you have defined vectorize_content in utils.py

# For PPTX extraction
from pptx import Presentation

# For PDF extraction
import PyPDF2

# For visualization we use Plotly
import plotly.express as px
import pandas as pd

DATA_PATH = "data/1104_NS_DB_old.json"

# 임베딩 모델 로드 (필요한 경우 캐싱 고려)
embedding_model = AutoModel.from_pretrained("BM-K/KoSimCSE-roberta-multitask")
embedding_tokenizer = AutoTokenizer.from_pretrained("BM-K/KoSimCSE-roberta-multitask")
embedding_model.eval()

data_control_bp = Blueprint("data_manager", __name__, template_folder="templates")

# --- Helper functions for new file types ---

def extract_text_from_pptx(file_path):
    prs = Presentation(file_path)
    texts = []
    for slide in prs.slides:
        for shape in slide.shapes:
            if hasattr(shape, "text") and shape.text:
                texts.append(shape.text)
    return "\n".join(texts)

def extract_text_from_pdf(file_path):
    pdf_text = ""
    with open(file_path, "rb") as f:
        reader = PyPDF2.PdfReader(f)
        for page in reader.pages:
            page_text = page.extract_text()
            if page_text:
                pdf_text += page_text + "\n"
    return pdf_text

@data_control_bp.route("/manager")
def data_control_page():
    return render_template("data_manager.html")

# --- 다중 파일 업로드 지원 (수정됨) ---
@data_control_bp.route("/upload", methods=["POST"])
def data_upload():
    if "dataFile" not in request.files:
        return jsonify({"message": "파일이 업로드되지 않았습니다."}), 400
    files = request.files.getlist("dataFile")
    if not files:
        return jsonify({"message": "업로드할 파일이 없습니다."}), 400

    # 기존 데이터 로드
    if os.path.exists(DATA_PATH):
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            existing_data = json.load(f)
    else:
        existing_data = []
        
    # 모든 파일에 걸쳐 최대 chunk_id 찾기
    max_chunk_id = 0
    for entry in existing_data:
        for chunk in entry.get("chunks", []):
            max_chunk_id = max(max_chunk_id, chunk.get("chunk_id", 0))
    
    print(f"기존 데이터의 최대 chunk_id: {max_chunk_id}")
    
    messages = []
    for file in files:
        if file.filename == "":
            messages.append("파일 이름이 없습니다.")
            continue
        ext = os.path.splitext(file.filename)[1].lower()
        try:
            new_chunk = None
            # TXT 파일 처리
            if ext == ".txt":
                content = file.read().decode("utf-8")
                vector = vectorize_content(content)
                text_vis = content
                new_chunk = {
                    "chunk_id": None,  # 이후에 할당
                    "title": os.path.splitext(file.filename)[0],
                    "date": datetime.datetime.now().strftime("%Y-%m-%d"),
                    "type": "text",
                    "text": content,
                    "text_short": content[:200],
                    "vector": vector,
                    "text_vis": normalize_text_vis(text_vis)
                }
            # JSON 파일 처리
            elif ext == ".json":
                new_entry = json.load(file)
                if not isinstance(new_entry, list):
                    new_entry = [new_entry]
                # JSON 파일의 각 청크에 대해 벡터 처리
                for entry in new_entry:
                    for chunk in entry.get("chunks", []):
                        if not chunk.get("vector"):
                            text = chunk.get("text", "")
                            chunk["vector"] = vectorize_content(text) if text else [[0.0] * 768]
                        else:
                            vector = chunk["vector"]
                            expected_dim = 768
                            if not isinstance(vector, list):
                                text = chunk.get("text", "")
                                chunk["vector"] = vectorize_content(text) if text else [[0.0]*expected_dim]
                            elif len(vector) != expected_dim and isinstance(vector[0], (int, float)):
                                chunk["vector"] = vector[:expected_dim] if len(vector) >= expected_dim else vector + [0.0]*(expected_dim - len(vector))
                        chunk["text_vis"] = normalize_text_vis(chunk.get("text_vis", ""))
                # JSON 파일은 전체 엔트리 추가
                for entry in new_entry:
                    for chunk in entry.get("chunks", []):
                        max_chunk_id += 1
                        chunk["chunk_id"] = max_chunk_id
                    existing_data.append(entry)
                messages.append(f"{file.filename}: 업로드 및 벡터화 성공.")
                continue  # 다음 파일로 넘어감
            # PPTX 파일 처리
            elif ext == ".pptx":
                temp_path = os.path.join("temp", file.filename)
                os.makedirs("temp", exist_ok=True)
                file.save(temp_path)
                content = extract_text_from_pptx(temp_path)
                os.remove(temp_path)
                vector = vectorize_content(content)
                text_vis = content
                new_chunk = {
                    "chunk_id": None,
                    "title": os.path.splitext(file.filename)[0],
                    "date": datetime.datetime.now().strftime("%Y-%m-%d"),
                    "type": "pptx",
                    "text": content,
                    "text_short": content[:200],
                    "vector": vector,
                    "text_vis": normalize_text_vis(text_vis)
                }
            # PDF 파일 처리
            elif ext == ".pdf":
                temp_path = os.path.join("temp", file.filename)
                os.makedirs("temp", exist_ok=True)
                file.save(temp_path)
                content = extract_text_from_pdf(temp_path)
                os.remove(temp_path)
                vector = vectorize_content(content)
                text_vis = content
                new_chunk = {
                    "chunk_id": None,
                    "title": os.path.splitext(file.filename)[0],
                    "date": datetime.datetime.now().strftime("%Y-%m-%d"),
                    "type": "pdf",
                    "text": content,
                    "text_short": content[:200],
                    "vector": vector,
                    "text_vis": normalize_text_vis(text_vis)
                }
            else:
                messages.append(f"{file.filename}: 지원되지 않는 파일 형식입니다.")
                continue

            # 단일 파일(텍스트, pptx, pdf)의 경우
            if new_chunk is not None:
                existing_entry = next((entry for entry in existing_data if entry.get("file_name") == file.filename), None)
                if existing_entry:
                    max_chunk_id += 1
                    new_chunk["chunk_id"] = max_chunk_id
                    existing_entry.setdefault("chunks", []).append(new_chunk)
                else:
                    max_chunk_id += 1
                    new_chunk["chunk_id"] = max_chunk_id
                    new_entry = {
                        "file_name": file.filename,
                        "chunks": [new_chunk]
                    }
                    existing_data.append(new_entry)
                messages.append(f"{file.filename}: 업로드 및 벡터화 성공.")
        except Exception as e:
            messages.append(f"{file.filename}: 업로드 실패: {str(e)}")
    # 업데이트된 데이터 저장
    with open(DATA_PATH, "w", encoding="utf-8") as f:
        json.dump(existing_data, f, ensure_ascii=False, indent=2)
    return jsonify({"message": "\n".join(messages)})

# --- 페이지네이션 및 인덱스 포함 데이터 목록 (수정됨) ---
@data_control_bp.route("/list", methods=["GET"])
def data_list():
    try:
        if os.path.exists(DATA_PATH):
            with open(DATA_PATH, "r", encoding="utf-8") as f:
                data_entries = json.load(f)
        else:
            data_entries = []
        summary = []
        for idx, entry in enumerate(data_entries):
            if "chunks" in entry and entry["chunks"]:
                chunk = entry["chunks"][0]
                summary.append({
                    "index": idx,
                    "file_name": entry.get("file_name", ""),
                    "title": chunk.get("title", ""),
                    "date": chunk.get("date", "")
                })
            else:
                summary.append({
                    "index": idx,
                    "file_name": entry.get("file_name", ""),
                    "title": "",
                    "date": ""
                })
        # 페이지네이션: 기본 페이지 1, 한 페이지당 30개
        page = request.args.get("page", 1, type=int)
        per_page = 30
        total = len(summary)
        start = (page - 1) * per_page
        end = start + per_page
        paginated = summary[start:end]
        return jsonify({
            "page": page,
            "per_page": per_page,
            "total": total,
            "data": paginated
        })
    except Exception as e:
        return jsonify({"message": f"데이터 목록 불러오기 실패: {str(e)}"}), 500

# --- 상세보기 엔드포인트 추가 ---
@data_control_bp.route("/detail/<int:index>", methods=["GET"])
def data_detail(index):
    try:
        if os.path.exists(DATA_PATH):
            with open(DATA_PATH, "r", encoding="utf-8") as f:
                data_entries = json.load(f)
        else:
            return jsonify({"message": "데이터 파일이 존재하지 않습니다."}), 404
        if index < 0 or index >= len(data_entries):
            return jsonify({"message": "유효하지 않은 인덱스입니다."}), 400
        return jsonify(data_entries[index])
    except Exception as e:
        return jsonify({"message": f"데이터 상세 보기 실패: {str(e)}"}), 500

@data_control_bp.route("/delete", methods=["POST"])
def data_delete():
    try:
        req = request.get_json()
        index = req.get("index")
        if index is None:
            return jsonify({"message": "삭제할 인덱스가 제공되지 않았습니다."}), 400
        if os.path.exists(DATA_PATH):
            with open(DATA_PATH, "r", encoding="utf-8") as f:
                data_entries = json.load(f)
        else:
            return jsonify({"message": "데이터 파일이 존재하지 않습니다."}), 404
        if not (0 <= index < len(data_entries)):
            return jsonify({"message": "유효하지 않은 인덱스입니다."}), 400
        del data_entries[index]
        with open(DATA_PATH, "w", encoding="utf-8") as f:
            json.dump(data_entries, f, ensure_ascii=False, indent=2)
        return jsonify({"message": "데이터 삭제가 완료되었습니다."})
    except Exception as e:
        return jsonify({"message": f"데이터 삭제 실패: {str(e)}"}), 500

# --- 검색 및 UMAP 관련 엔드포인트는 기존 그대로 유지 ---
@data_control_bp.route("/search", methods=["GET"])
def search_data():
    query = request.args.get("q", "").lower()
    if os.path.exists(DATA_PATH):
        with open(DATA_PATH, "r", encoding="utf-8") as f:
            data_entries = json.load(f)
    else:
        data_entries = []
    results = []
    for idx, entry in enumerate(data_entries):
        file_name = entry.get("file_name", "").lower()
        title = ""
        if "chunks" in entry and entry["chunks"]:
            title = entry["chunks"][0].get("title", "").lower()
        if query in file_name or query in title:
            results.append({
                "index": idx,
                "file_name": entry.get("file_name", ""),
                "title": title,
            })
    return jsonify(results)

@data_control_bp.route("/umap_visualization")
def umap_visualization_page():
    return render_template("umap_visualization.html")

@data_control_bp.route("/api/umap_data", methods=["GET"])
def get_umap_data():
    try:
        if os.path.exists(DATA_PATH):
            with open(DATA_PATH, "r", encoding="utf-8") as f:
                data_entries = json.load(f)
        else:
            return jsonify({"error": "데이터 파일을 찾을 수 없습니다."}), 404
        
        vectors = []
        metadata = []
        
        for entry in data_entries:
            for chunk in entry.get("chunks", []):
                if "vector" in chunk and chunk["vector"]:
                    vectors.append(chunk["vector"])
                    metadata.append({
                        "id": f"{entry.get('file_name', 'unknown')}_{chunk.get('chunk_id', '0')}",
                        "file_name": entry.get("file_name", "Unknown"),
                        "title": chunk.get("title", ""),
                        "date": chunk.get("date", ""),
                        "text_short": chunk.get("text_short", "")[:100] + "..."
                    })
        
        if not vectors:
            return jsonify({"error": "벡터 데이터가 없습니다."}), 404
        
        vectors_array = np.array(vectors)
        vectors_array = np.squeeze(vectors_array)
        
        from umap import UMAP
        n_neighbors = min(15, len(vectors) - 1)
        umap_model = UMAP(n_components=2, 
                            n_neighbors=n_neighbors, 
                            min_dist=0.1, 
                            metric='cosine', 
                            random_state=42)
        embedding = umap_model.fit_transform(vectors_array)
        
        nodes = []
        for i, (x, y) in enumerate(embedding):
            nodes.append({
                "id": metadata[i]["id"],
                "x": float(x),
                "y": float(y),
                "file_name": metadata[i]["file_name"],
                "title": metadata[i]["title"],
                "date": metadata[i]["date"],
                "text_short": metadata[i]["text_short"]
            })
        
        edges = []
        if len(vectors) > 1:
            from sklearn.metrics.pairwise import cosine_similarity
            similarity = cosine_similarity(vectors_array)
            threshold = 0.7
            for i in range(len(vectors)):
                for j in range(i+1, len(vectors)):
                    if similarity[i, j] > threshold:
                        edges.append({
                            "source": metadata[i]["id"],
                            "target": metadata[j]["id"],
                            "value": float(similarity[i, j])
                        })
        
        return jsonify({
            "nodes": nodes,
            "edges": edges
        })
        
    except Exception as e:
        import traceback
        traceback.print_exc()
        return jsonify({"error": f"UMAP 시각화 처리 중 오류 발생: {str(e)}"}), 500

```


--- templates/data_manager.html

```html

<!DOCTYPE html>
<html lang="ko">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>데이터 관리 및 대규모 UMAP 벡터 시각화</title>
  <!-- d3.js for pagination and legends in the data management section -->
  <script src="https://d3js.org/d3.v7.min.js"></script>
  <style>
    /* Shared styles */
    body {
      font-family: 'Inter', sans-serif;
      margin: 0;
      padding: 0;
      background-color: #f9fafb;
    }
    .container {
      width: 100%;
      max-width: 1400px;
      margin: 0 auto;
      padding: 20px;
    }
    h1, h2 {
      color: #111827;
    }
    .controls {
      margin-bottom: 15px;
      display: flex;
      gap: 10px;
      align-items: center;
    }
    button {
      padding: 8px 16px;
      background-color: #7c3aed;
      color: white;
      border: none;
      border-radius: 4px;
      cursor: pointer;
    }
    button:hover {
      background-color: #6d28d9;
    }
    /* Data management table styles */
    #dataListContainer {
      max-height: 300px;
      overflow-y: auto;
      border: 1px solid #ddd;
      margin-bottom: 20px;
    }
    table {
      width: 100%;
      border-collapse: collapse;
      margin-top: 15px;
    }
    table, th, td {
      border: 1px solid #ccc;
    }
    th, td {
      padding: 8px;
      text-align: center;
    }
    tr:hover {
      background-color: #f1f5f9;
      cursor: pointer;
    }
    /* PIXI.js visualization styles */
    #pixiSection {
      position: relative;
      height: 700px;
      margin-top: 40px;
      background-color: #ffffff;
      border-radius: 8px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.1);
      overflow: hidden;
    }
    /* Container for PIXI canvas */
    #pixiContainer {
      width: 100%;
      height: 100%;
    }
    /* Loading overlay for PIXI visualization */
    #pixiLoadingOverlay {
      position: absolute;
      top: 0;
      left: 0;
      width: 100%;
      height: 100%;
      background: rgba(255, 255, 255, 0.8);
      display: flex;
      align-items: center;
      justify-content: center;
      z-index: 10;
      font-size: 24px;
      color: #333;
    }
    /* Legend styles for D3 in data management section */
    .legend {
      position: absolute;
      right: 20px;
      top: 20px;
      background-color: rgba(255, 255, 255, 0.9);
      padding: 10px;
      border-radius: 4px;
      box-shadow: 0 1px 3px rgba(0,0,0,0.1);
    }
    .legend-item {
      display: flex;
      align-items: center;
      margin-bottom: 5px;
    }
    .legend-color {
      width: 15px;
      height: 15px;
      margin-right: 8px;
      border-radius: 3px;
    }
  </style>
</head>
<body>
  <div class="container">
    <!-- Data Management Section -->
    <h1>데이터 관리</h1>
    
    <h2>데이터 업로드</h2>
    <!-- 다중 파일 업로드 -->
    <form id="uploadForm" enctype="multipart/form-data">
      <input type="file" id="dataFile" name="dataFile" accept=".txt,.json,.pptx,.pdf" multiple required>
      <button type="submit">업로드 및 벡터화</button>
    </form>

    <h2>데이터 검색</h2>
    <div class="controls">
      <input type="text" id="searchInput" placeholder="검색어를 입력하세요">
      <button onclick="performSearch()">검색</button>
    </div>
    <div id="searchResults"></div>

    <h2>데이터 목록</h2>
    <!-- 데이터 목록은 항상 보이도록 -->
    <div id="dataListContainer">
      <table id="dataTable">
        <thead>
          <tr>
            <th>인덱스</th>
            <th>파일 이름</th>
            <th>제목</th>
            <th>수정일자</th>
            <th>액션</th>
          </tr>
        </thead>
        <tbody></tbody>
      </table>
      <div id="pagination"></div>
    </div>

    <!-- Optimized Visualization Section using PIXI.js -->
    <h1>대규모 UMAP 벡터 시각화 (Optimized with PIXI.js)</h1>
    <div id="pixiSection">
      <div id="pixiContainer"></div>
      <div id="pixiLoadingOverlay">데이터 로딩 중...</div>
    </div>
  </div>

  <!-- Data Management Scripts -->
  <script>
    let currentPage = 1;
    const perPage = 30;

    async function loadDataList(page = 1) {
      currentPage = page;
      const response = await fetch(`/data/list?page=${page}`);
      const result = await response.json();
      const data = result.data;
      const tbody = document.getElementById('dataTable').getElementsByTagName('tbody')[0];
      tbody.innerHTML = '';
      data.forEach(item => {
        const row = document.createElement('tr');
        // 클릭 시 상세보기
        row.onclick = () => { showDetail(item.index); };
        row.innerHTML = `
          <td>${item.index}</td>
          <td>${item.file_name}</td>
          <td>${item.title}</td>
          <td>${item.date}</td>
          <td><button onclick="deleteData(event, ${item.index})">삭제</button></td>
        `;
        tbody.appendChild(row);
      });
      renderPagination(result.total, page);
    }

    function renderPagination(total, current) {
      const paginationDiv = document.getElementById('pagination');
      paginationDiv.innerHTML = '';
      const totalPages = Math.ceil(total / perPage);
      for (let i = 1; i <= totalPages; i++) {
        const btn = document.createElement('button');
        btn.textContent = i;
        if (i === current) {
          btn.disabled = true;
        }
        btn.onclick = () => { loadDataList(i); };
        paginationDiv.appendChild(btn);
      }
    }

    async function deleteData(event, index) {
      event.stopPropagation();
      if (!confirm('이 데이터를 삭제하시겠습니까?')) return;
      const response = await fetch('/data/delete', {
        method: 'POST',
        headers: { 'Content-Type': 'application/json' },
        body: JSON.stringify({ index: index })
      });
      const result = await response.json();
      alert(result.message);
      loadDataList(currentPage);
      loadPixiVisualization();
    }

    async function showDetail(index) {
      try {
        const response = await fetch(`/data/detail/${index}`);
        const data = await response.json();
        alert(JSON.stringify(data, null, 2));
      } catch (error) {
        console.error('Error fetching detail:', error);
        alert('상세 정보를 불러오는 중 오류 발생');
      }
    }

    document.getElementById('uploadForm').addEventListener('submit', async function(e) {
      e.preventDefault();
      const fileInput = document.getElementById('dataFile');
      const formData = new FormData();
      for (let i = 0; i < fileInput.files.length; i++) {
        formData.append('dataFile', fileInput.files[i]);
      }
      const response = await fetch('/data/upload', {
        method: 'POST',
        body: formData
      });
      const result = await response.json();
      alert(result.message);
      loadDataList(currentPage);
      loadPixiVisualization();
    });

    async function performSearch() {
      const query = document.getElementById('searchInput').value.trim();
      if (!query) return alert('검색어를 입력하세요.');
      try {
        const response = await fetch(`/data/search?q=${encodeURIComponent(query)}`);
        const results = await response.json();
        displaySearchResults(results);
      } catch (error) {
        console.error('검색 오류:', error);
        alert('검색 중 오류가 발생했습니다.');
      }
    }

    function displaySearchResults(results) {
      const container = document.getElementById('searchResults');
      if (results.length === 0) {
        container.innerHTML = '<p>검색 결과가 없습니다.</p>';
        return;
      }
      let html = '<ul>';
      results.forEach(item => {
        html += `<li onclick="showDetail(${item.index})">[${item.index}] 파일: ${item.file_name}, 제목: ${item.title}</li>`;
      });
      html += '</ul>';
      container.innerHTML = html;
    }

    // Initial load of data list
    document.addEventListener('DOMContentLoaded', () => {
      loadDataList();
      loadPixiVisualization();
    });
  </script>

  <!-- PIXI.js Library -->
  <script src="https://pixijs.download/release/pixi.min.js"></script>
  <!-- Optimized Visualization using PIXI.js -->
  <script>
    // Create PIXI Application
    const pixiApp = new PIXI.Application({
      width: window.innerWidth,
      height: 700,  // Fixed height for visualization section
      backgroundColor: 0xffffff,
      antialias: true,
      resolution: window.devicePixelRatio || 1,
    });
    // Append PIXI view to pixiContainer
    const pixiContainerDiv = document.getElementById('pixiContainer');
    pixiContainerDiv.appendChild(pixiApp.view);

    // Create a PIXI container for the visualization content
    const vizContainer = new PIXI.Container();
    pixiApp.stage.addChild(vizContainer);

    // Load visualization data from API endpoint
    async function loadVisualizationData() {
      const response = await fetch('/data/api/umap_data');
      if (!response.ok) {
        throw new Error("데이터를 불러오는데 실패했습니다.");
      }
      const data = await response.json();
      return data;
    }

    // Downsample nodes to reduce load for very large datasets
    function downsample(nodes, maxPoints) {
      if (nodes.length <= maxPoints) return nodes;
      const sampled = [];
      const step = Math.floor(nodes.length / maxPoints);
      for (let i = 0; i < nodes.length; i += step) {
        sampled.push(nodes[i]);
      }
      return sampled;
    }

    // Draw the visualization with PIXI Graphics
    function drawVisualization(data) {
      vizContainer.removeChildren();
      const maxNodes = 10000; // Adjust as needed
      const nodes = downsample(data.nodes, maxNodes);
      // Filter edges to those connecting sampled nodes
      const nodeIds = new Set(nodes.map(n => n.id));
      const edges = data.edges.filter(edge => nodeIds.has(edge.source) && nodeIds.has(edge.target));

      // Draw edges
      const edgeGraphics = new PIXI.Graphics();
      edgeGraphics.lineStyle(1, 0x999999, 0.3);
      for (const edge of edges) {
        const source = data.nodes.find(n => n.id === edge.source);
        const target = data.nodes.find(n => n.id === edge.target);
        if (source && target) {
          edgeGraphics.moveTo(source.x, source.y);
          edgeGraphics.lineTo(target.x, target.y);
        }
      }
      vizContainer.addChild(edgeGraphics);

      // Draw nodes as circles
      for (const node of nodes) {
        const circle = new PIXI.Graphics();
        circle.beginFill(0x1f77b4);
        circle.drawCircle(0, 0, node.r || 5);
        circle.endFill();
        circle.x = node.x;
        circle.y = node.y;
        vizContainer.addChild(circle);
      }
    }

    // Set up panning and zooming interactions for PIXI visualization
    let isDragging = false;
    let dragStart = { x: 0, y: 0 };
    let containerStart = { x: vizContainer.x, y: vizContainer.y };

    pixiApp.view.addEventListener('mousedown', (event) => {
      isDragging = true;
      dragStart = { x: event.clientX, y: event.clientY };
      containerStart = { x: vizContainer.x, y: vizContainer.y };
    });
    pixiApp.view.addEventListener('mousemove', (event) => {
      if (isDragging) {
        const dx = event.clientX - dragStart.x;
        const dy = event.clientY - dragStart.y;
        vizContainer.x = containerStart.x + dx;
        vizContainer.y = containerStart.y + dy;
      }
    });
    pixiApp.view.addEventListener('mouseup', () => { isDragging = false; });
    pixiApp.view.addEventListener('mouseleave', () => { isDragging = false; });
    pixiApp.view.addEventListener('wheel', (event) => {
      event.preventDefault();
      const scaleFactor = event.deltaY > 0 ? 0.95 : 1.05;
      vizContainer.scale.x *= scaleFactor;
      vizContainer.scale.y *= scaleFactor;
    });

    // Main function to load data and draw visualization
    async function loadPixiVisualization() {
      document.getElementById('pixiLoadingOverlay').style.display = 'flex';
      try {
        const data = await loadVisualizationData();
        // Optionally, you could apply scaling to coordinates here if needed.
        drawVisualization(data);
      } catch (error) {
        console.error(error);
        document.getElementById('pixiLoadingOverlay').innerText = "데이터 로딩 오류";
      } finally {
        document.getElementById('pixiLoadingOverlay').style.display = 'none';
      }
    }

    // Handle window resize for PIXI application
    window.addEventListener('resize', () => {
      pixiApp.renderer.resize(window.innerWidth, 700);
    });
  </script>
</body>
</html>

```


-----------------

# Requirements


Base-Knowledge:
 - 위 파일들은 LLM 모델을 활용한 사내 RAG 서비스의 소스 코드입니다.
 - 파일 트리와 각 파일의 내용이 코드 블록 내에 포함되어, 프로젝트의 현재 구조와 상태를 한눈에 파악할 수 있습니다.
 - vLLM과 ray를 활용하여 사용성 및 추론 성능을 개선하였습니다.
 - Langchain을 활용하여 reqeust_id별로 대화를 저장하고 활용할 수 있습니다.
 - 에러 발생 시 로깅을 통해 문제를 추적할 수 있도록 설계되었습니다.

My-Requirements:
 1. 추후 소스 코드 개선, 구조 변경, 에러 로그 추가 등 다양한 요구사항을 반영할 수 있는 확장성을 고려합니다.
 2. 전체 코드는 한국어로 주석 및 설명이 포함되어, 이해와 유지보수가 용이하도록 작성됩니다.
 3. User requirements.
 4. My requirements.
